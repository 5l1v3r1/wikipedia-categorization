{"parse":{"title":"Gaussian adaptation","pageid":9210345,"revid":793268977,"text":{"*":"<div class=\"mw-parser-output\"><table class=\"plainlinks metadata ambox ambox-content ambox-multiple_issues compact-ambox\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/40px-Ambox_important.svg.png\" width=\"40\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x\" data-file-width=\"40\" data-file-height=\"40\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\"><div class=\"mw-collapsible\" style=\"width:95%; margin: 0.2em 0;\"><b>This article has multiple issues.</b> Please help <b><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Gaussian_adaptation&amp;action=edit\">improve it</a></b> or discuss these issues on the <b><a href=\"/wiki/Talk:Gaussian_adaptation\" title=\"Talk:Gaussian adaptation\">talk page</a></b>. <small><i>(<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove these template messages</a>)</i></small>\n<div class=\"mw-collapsible-content\" style=\"margin-top: 0.3em;\">\n      <table class=\"plainlinks metadata ambox ambox-content ambox-Refimprove\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><a href=\"/wiki/File:Question_book-new.svg\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png\" width=\"50\" height=\"39\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x\" data-file-width=\"512\" data-file-height=\"399\" /></a></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article <b>needs additional citations for <a href=\"/wiki/Wikipedia:Verifiability\" title=\"Wikipedia:Verifiability\">verification</a></b>.<span class=\"hide-when-compact\"> Please help <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Gaussian_adaptation&amp;action=edit\">improve this article</a> by <a href=\"/wiki/Help:Introduction_to_referencing_with_Wiki_Markup/1\" title=\"Help:Introduction to referencing with Wiki Markup/1\">adding citations to reliable sources</a>. Unsourced material may be challenged and removed.</span>  <small><i>(July 2008)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<table class=\"plainlinks metadata ambox ambox-content ambox-Primary_sources\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><a href=\"/wiki/File:Question_book-new.svg\" class=\"image\"><img alt=\"Question book-new.svg\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/50px-Question_book-new.svg.png\" width=\"50\" height=\"39\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/75px-Question_book-new.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/9/99/Question_book-new.svg/100px-Question_book-new.svg.png 2x\" data-file-width=\"512\" data-file-height=\"399\" /></a></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article <b>relies too much on <a href=\"/wiki/Wikipedia:Verifiability\" title=\"Wikipedia:Verifiability\">references</a> to <a href=\"/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources\" title=\"Wikipedia:No original research\">primary sources</a></b>.<span class=\"hide-when-compact\"> Please improve this  by adding <a href=\"/wiki/Wikipedia:No_original_research#Primary,_secondary_and_tertiary_sources\" title=\"Wikipedia:No original research\">secondary or tertiary sources</a>.</span>  <small><i>(July 2008)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<table class=\"plainlinks metadata ambox ambox-content\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/40px-Ambox_important.svg.png\" width=\"40\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x\" data-file-width=\"40\" data-file-height=\"40\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article <b>needs attention from an expert in Mathematics</b>.<span class=\"hide-when-compact\"> Please add a <i>reason</i> or a <i>talk</i> parameter to this template to explain the issue with the article. <a href=\"/wiki/Wikipedia:WikiProject_Mathematics\" title=\"Wikipedia:WikiProject Mathematics\">WikiProject Mathematics</a> may be able to help recruit an expert.</span>  <small><i>(January 2015)</i></small></div></td></tr></tbody></table>\n<table class=\"plainlinks metadata ambox ambox-content ambox-COI\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"Unbalanced scales.svg\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/45px-Unbalanced_scales.svg.png\" width=\"45\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/68px-Unbalanced_scales.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/90px-Unbalanced_scales.svg.png 2x\" data-file-width=\"400\" data-file-height=\"354\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\"><b> A major contributor to this article appears to have a <a href=\"/wiki/Wikipedia:Conflict_of_interest\" title=\"Wikipedia:Conflict of interest\">close connection</a> with its subject.</b><span class=\"hide-when-compact\"> It may require <a href=\"/wiki/Wikipedia:Cleanup\" title=\"Wikipedia:Cleanup\">cleanup</a> to comply with Wikipedia's content policies, particularly <a href=\"/wiki/Wikipedia:Neutral_point_of_view\" title=\"Wikipedia:Neutral point of view\">neutral point of view</a>. Please discuss further on the <a href=\"/wiki/Talk:Gaussian_adaptation\" title=\"Talk:Gaussian adaptation\">talk page</a>.</span>  <small><i>(March 2009)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n    </div>\n</div><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<p><b>Gaussian adaptation (GA)</b> (also referred to as normal or natural adaptation and sometimes abbreviated as NA) is an <a href=\"/wiki/Evolutionary_algorithm\" title=\"Evolutionary algorithm\">evolutionary algorithm</a> designed for the maximization of manufacturing yield due to statistical deviation of component values of <a href=\"/wiki/Signal_processing\" title=\"Signal processing\">signal processing</a> systems. In short, GA is a stochastic adaptive process where a number of samples of an <i>n</i>-dimensional vector <i>x</i>[<i>x</i><sup>T</sup> = (<i>x</i><sub>1</sub>, <i>x</i><sub>2</sub>, ..., <i>x<sub>n</sub></i>)] are taken from a <a href=\"/wiki/Multivariate_Gaussian_distribution\" class=\"mw-redirect\" title=\"Multivariate Gaussian distribution\">multivariate Gaussian distribution</a>, <i>N</i>(<i>m</i>,&#160;<i>M</i>), having mean <i>m</i> and <a href=\"/wiki/Covariance_matrix\" title=\"Covariance matrix\">moment matrix</a> <i>M</i>. The samples are tested for fail or pass. The first- and second-order moments of the Gaussian restricted to the pass samples are <i>m*</i> and&#160;<i>M*</i>.\n</p><p>The outcome of <i>x</i> as a pass sample is determined by a function <i>s</i>(<i>x</i>), 0&#160;&lt;&#160;<i>s</i>(<i>x</i>)&#160;&lt;&#160;<i>q</i>&#160;\u2264&#160;1, such that <i>s</i>(<i>x</i>) is the probability that x will be selected as a pass sample. The average probability of finding pass samples (yield) is\n</p>\n<dl><dd><span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle P(m)=\\int s(x)N(x-m)\\,dx}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>m</mi>\n        <mo stretchy=\"false\">)</mo>\n        <mo>=</mo>\n        <mo>&#x222B;<!-- \u222b --></mo>\n        <mi>s</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>x</mi>\n        <mo stretchy=\"false\">)</mo>\n        <mi>N</mi>\n        <mo stretchy=\"false\">(</mo>\n        <mi>x</mi>\n        <mo>&#x2212;<!-- \u2212 --></mo>\n        <mi>m</mi>\n        <mo stretchy=\"false\">)</mo>\n        <mspace width=\"thinmathspace\" />\n        <mi>d</mi>\n        <mi>x</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle P(m)=\\int s(x)N(x-m)\\,dx}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/2c5a7e052f892754f19bc7c123bbf43e02db32e5\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -2.338ex; width:28.52ex; height:5.676ex;\" alt=\"{\\displaystyle P(m)=\\int s(x)N(x-m)\\,dx}\"/></span></dd></dl>\n<p>Then the theorem of GA states:\n</p>\n<blockquote><p>For any <i>s</i>(<i>x</i>) and for any value of <i>P&#160;</i>&lt;&#160;<i>q</i>, there always exist a Gaussian p. d. f. [ <a href=\"/wiki/Probability_density_function\" title=\"Probability density function\">probability density function</a> ] that is adapted for maximum dispersion. The necessary conditions for a local optimum are <i>m</i>&#160;=&#160;<i>m</i>* and <i>M</i> proportional to <i>M</i>*. The dual problem is also solved: <i>P</i> is maximized while keeping the dispersion constant (Kjellstr\u00f6m, 1991).\n</p></blockquote>\n<p>Proofs of the theorem may be found in the papers by Kjellstr\u00f6m, 1970, and Kjellstr\u00f6m &amp; Tax\u00e9n, 1981.\n</p><p>Since dispersion is defined as the exponential of entropy/disorder/<a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a> it immediately follows that the theorem is valid also for those concepts. Altogether, this means that Gaussian adaptation may carry out a simultaneous maximisation of yield and <a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a> (without any need for the yield or the <a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a> to be defined as criterion functions).\n</p><p><b>The theorem is valid for all regions of acceptability and all Gaussian distributions</b>. It may be used by cyclic repetition of random variation and selection (like the natural evolution). In every cycle a sufficiently large number of Gaussian distributed points are sampled and tested for membership in the region of acceptability. The centre of gravity of the Gaussian, <i>m</i>, is then moved to the centre of gravity of the approved (selected) points, <i>m</i>*. Thus, the process converges to a state of equilibrium fulfilling the theorem. A solution is always approximate because the centre of gravity is always determined for a limited number of points.\n</p><p>It was used for the first time in 1969 as a pure optimization algorithm making the regions of acceptability smaller and smaller (in analogy to <a href=\"/wiki/Simulated_annealing\" title=\"Simulated annealing\">simulated annealing</a>, Kirkpatrick 1983). Since 1970 it has been used for both ordinary optimization and yield maximization.\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Natural_evolution_and_Gaussian_adaptation\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Natural evolution and Gaussian adaptation</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#How_to_climb_a_mountain\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">How to climb a mountain</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Computer_simulation_of_Gaussian_adaptation\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Computer simulation of Gaussian adaptation</span></a></li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#The_evolution_in_the_brain\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">The evolution in the brain</span></a></li>\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Gaussian_adaptation_and_free_will\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Gaussian adaptation and free will</span></a></li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#A_theorem_of_efficiency_for_random_search\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">A theorem of efficiency for random search</span></a></li>\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#The_Stauffer_and_Grimson_algorithm\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">The Stauffer and Grimson algorithm</span></a></li>\n<li class=\"toclevel-1 tocsection-8\"><a href=\"#See_also\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-9\"><a href=\"#References\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">References</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Natural_evolution_and_Gaussian_adaptation\">Natural evolution and Gaussian adaptation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=1\" title=\"Edit section: Natural evolution and Gaussian adaptation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>It has also been compared to the natural evolution of populations of living organisms. In this case <i>s</i>(<i>x</i>) is the probability that the individual having an array <i>x</i> of phenotypes will survive by giving offspring to the next generation; a definition of individual fitness given by Hartl 1981. The yield, <i>P</i>, is replaced by the <a href=\"/wiki/Mean_fitness\" class=\"mw-redirect\" title=\"Mean fitness\">mean fitness</a> determined as a mean over the set of individuals in a large population.\n</p><p>Phenotypes are often Gaussian distributed in a large population and a necessary condition for the natural evolution to be able to fulfill the theorem of Gaussian adaptation, with respect to all Gaussian quantitative characters, is that it may push the centre of gravity of the Gaussian to the centre of gravity of the selected individuals. This may be accomplished by the <a href=\"/wiki/Hardy%E2%80%93Weinberg_law\" class=\"mw-redirect\" title=\"Hardy\u2013Weinberg law\">Hardy\u2013Weinberg law</a>. This is possible because the theorem of Gaussian adaptation is valid for any region of acceptability independent of the structure (Kjellstr\u00f6m, 1996).\n</p><p>In this case the rules of genetic variation such as crossover, inversion, transposition etcetera may be seen as random number generators for the phenotypes. So, in this sense Gaussian adaptation may be seen as a genetic algorithm.\n</p>\n<h2><span class=\"mw-headline\" id=\"How_to_climb_a_mountain\">How to climb a mountain</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=2\" title=\"Edit section: How to climb a mountain\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Mean fitness may be calculated provided that the distribution of parameters and the structure of the landscape is known. The real landscape is not known, but figure below shows a fictitious profile (blue) of a landscape along a line (x) in a room spanned by such parameters. The red curve is the mean based on the red bell curve at the bottom of  figure. It is obtained by letting the bell curve slide along the <i>x</i>-axis, calculating the mean at every location. As can be seen, small peaks and pits are smoothed out. Thus, if evolution is started at A with a relatively small variance (the red bell curve), then climbing will take place on the red curve. The process may get stuck for millions of years at B or C, as long as the hollows to the right of these points remain, and the mutation rate is too small.\n</p><p><a href=\"/wiki/File:Fraktal.gif\" class=\"image\"><img alt=\"Fraktal.gif\" src=\"//upload.wikimedia.org/wikipedia/commons/3/34/Fraktal.gif\" width=\"394\" height=\"171\" data-file-width=\"394\" data-file-height=\"171\" /></a>\n</p><p>If the mutation rate is sufficiently high, the disorder or variance  may increase and the parameter(s) may become distributed like the green bell curve. Then the climbing will take place on the green curve, which is even more smoothed out. Because the hollows to the right of B and C have now disappeared, the process may continue up to the peaks at D. But of course the landscape puts a limit on the disorder or variability. Besides &#8212; dependent on the landscape &#8212; the process may become very jerky, and if the ratio between the time spent by the process at a local peak and the time of transition to the next peak is very high, it may as well look like a <a href=\"/wiki/Punctuated_equilibrium\" title=\"Punctuated equilibrium\">punctuated equilibrium</a> as suggested by Gould (see Ridley).\n</p>\n<h2><span class=\"mw-headline\" id=\"Computer_simulation_of_Gaussian_adaptation\">Computer simulation of Gaussian adaptation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=3\" title=\"Edit section: Computer simulation of Gaussian adaptation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Thus far the theory only considers mean values of continuous distributions corresponding to an infinite number of individuals. In reality however, the number of individuals is always limited, which gives rise to an uncertainty in the estimation of <i>m</i> and <i>M</i> (the moment matrix of the Gaussian). And this may also affect the efficiency of the process. Unfortunately very little is known about this, at least theoretically.\n</p><p>The implementation of normal adaptation on a computer is a fairly simple task. The adaptation of m may be done by one sample (individual) at a time, for example\n</p>\n<dl><dd><i>m</i>(<i>i</i> + 1) = (1 \u2013 <i>a</i>) <i>m</i>(<i>i</i>) + <i>ax</i></dd></dl>\n<p>where <i>x</i> is a pass sample, and <i>a</i> &lt; 1 a suitable constant so that the inverse of a represents the number of individuals in the population.\n</p><p><i>M</i> may in principle be updated after every step <i>y</i> leading to a feasible point\n</p>\n<dl><dd><i>x</i> = <i>m</i> + <i>y</i> according to:</dd></dl>\n<dl><dd><i>M</i>(<i>i</i> + 1) = (1 \u2013 2<i>b</i>) <i>M</i>(<i>i</i>) + 2<i>byy</i><sup>T</sup>,</dd></dl>\n<p>where <i>y</i><sup>T</sup> is the transpose of <i>y</i> and <i>b</i> &lt;&lt; 1 is another suitable constant. In order to guarantee a suitable increase of <a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a>, <i>y</i> should be <a href=\"/wiki/Normal_distribution\" title=\"Normal distribution\">normally distributed</a> with moment matrix <i>\u03bc</i><sup>2</sup><i>M</i>, where the scalar <i>\u03bc</i> &gt; 1 is used to increase <a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a> (<a href=\"/wiki/Information_entropy\" class=\"mw-redirect\" title=\"Information entropy\">information entropy</a>, disorder, diversity) at a suitable rate. But <i>M</i> will never be used in the calculations. Instead we use the matrix <i>W</i> defined by <i>WW</i><sup>T</sup> = <i>M</i>.\n</p><p>Thus, we have <i>y</i> = <i>Wg</i>, where <i>g</i> is normally distributed with the moment matrix <i>\u03bcU</i>, and <i>U</i> is the unit matrix. <i>W</i> and <i>W</i><sup>T</sup> may be updated by the formulas\n</p>\n<dl><dd><i>W</i> = (1 \u2013 <i>b</i>)<i>W</i> + <i>byg</i><sup>T</sup>   and   <i>W</i><sup><i>T</i></sup> = (1 \u2013 <i>b</i>)<i>W</i><sup>T</sup> + <i>bgy</i><sup>T</sup></dd></dl>\n<p>because multiplication gives\n</p>\n<dl><dd><i>M</i> = (1 \u2013 2<i>b</i>)<i>M</i> + 2<i>byy</i><sup>T</sup>,</dd></dl>\n<p>where terms including <i>b</i><sup>2</sup> have been neglected. Thus, <i>M</i> will be indirectly adapted with good approximation. In practice it will suffice to update <i>W</i> only\n</p>\n<dl><dd><i>W</i>(<i>i</i> + 1)  = (1 \u2013 <i>b</i>)<i>W</i>(<i>i</i>) + <i>byg</i><sup>T</sup>.</dd></dl>\n<p>This is the formula used in a simple 2-dimensional model of a brain satisfying the Hebbian rule of associative learning; see the next section (Kjellstr\u00f6m, 1996 and 1999).\n</p><p>The figure below illustrates the effect of increased <a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a> in a Gaussian p.d.f. used to climb a mountain Crest (the two lines represent the contour line). Both the red and green cluster have equal mean fitness, about 65%, but the green cluster has a much higher <a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a> making the green process much more efficient. The effect of this adaptation is not very salient in a 2-dimensional case, but in a high-dimensional case, the efficiency of the search process may be increased by many orders of magnitude.\n</p><p><a href=\"/wiki/File:Mountain_crest.GIF\" class=\"image\"><img alt=\"Mountain crest.GIF\" src=\"//upload.wikimedia.org/wikipedia/en/0/0a/Mountain_crest.GIF\" width=\"393\" height=\"194\" data-file-width=\"393\" data-file-height=\"194\" /></a>\n</p>\n<h2><span class=\"mw-headline\" id=\"The_evolution_in_the_brain\">The evolution in the brain</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=4\" title=\"Edit section: The evolution in the brain\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>In the brain the evolution of DNA-messages is supposed to be replaced by an evolution of signal patterns and the phenotypic landscape is replaced by a mental landscape, the complexity of which will hardly be second to the former. The metaphor with the mental landscape is based on the assumption that certain signal patterns give rise to a better well-being or performance. For instance, the control of a group of muscles leads to a better pronunciation of a word or performance of a piece of music.\n</p><p>In this simple model it is assumed that the brain consists of interconnected components that may add, multiply and delay signal values.\n</p>\n<ul><li>A nerve cell kernel may add signal values,</li>\n<li>a synapse may multiply with a constant and</li>\n<li>An axon may delay values.</li></ul>\n<p>This is a basis of the theory of digital filters and neural networks consisting of components that may add, multiply and delay signalvalues and also of many brain models, Levine 1991.\n</p><p>In the figure below the brain stem is supposed to deliver Gaussian distributed signal patterns. This may be possible since certain neurons fire at random (Kandel et al.). The stem also constitutes a disordered structure surrounded by more ordered shells (Bergstr\u00f6m, 1969), and according to the <a href=\"/wiki/Central_limit_theorem\" title=\"Central limit theorem\">central limit theorem</a> the sum of signals from many neurons may be Gaussian distributed. The triangular boxes represent synapses and the boxes with the + sign are cell kernels.\n</p><p>In the cortex signals are supposed to be tested for feasibility. When a signal is accepted the contact areas in the synapses are updated according to the formulas below in agreement with the Hebbian theory. The figure shows a 2-dimensional computer simulation of Gaussian adaptation according to the last formula in the preceding section.\n</p><p><a href=\"/wiki/File:Schematic_of_a_neural_network_executing_the_Gaussian_adaptation_algorithm.GIF\" class=\"image\"><img alt=\"Schematic of a neural network executing the Gaussian adaptation algorithm.GIF\" src=\"//upload.wikimedia.org/wikipedia/commons/0/00/Schematic_of_a_neural_network_executing_the_Gaussian_adaptation_algorithm.GIF\" width=\"365\" height=\"159\" data-file-width=\"365\" data-file-height=\"159\" /></a>\n</p><p><i>m</i> and <i>W</i> are updated according to:\n</p>\n<dl><dd><i>m</i><sub>1</sub> = 0.9 <i>m</i><sub>1</sub> + 0.1 <i>x</i>1;   <i>m</i><sub>2</sub> = 0.9 <i>m</i><sub>2</sub> + 0.1 <i>x</i><sub>2</sub>;</dd></dl>\n<dl><dd><i>w</i><sub>11</sub> = 0.9 <i>w</i><sub>11</sub> + 0.1 <i>y</i><sub>1</sub><i>g</i><sub>1</sub>;    <i>w</i><sub>12</sub> = 0.9 <i>w</i><sub>12</sub> + 0.1 <i>y</i><sub>1</sub><i>g</i><sub>2</sub>;</dd></dl>\n<dl><dd><i>w</i><sub>21</sub> = 0.9 <i>w</i><sub>21</sub> + 0.1 <i>y</i><sub>2</sub><i>g</i><sub>1</sub>;    <i>w</i><sub>22</sub> = 0.9 <i>w</i><sub>22</sub> + 0.1 <i>y</i><sub>2</sub><i>g</i><sub>2</sub>;</dd></dl>\n<p>As can be seen this is very much like a small brain ruled by the theory of <a href=\"/wiki/Hebbian_learning\" class=\"mw-redirect\" title=\"Hebbian learning\">Hebbian learning</a> (Kjellstr\u00f6m, 1996, 1999 and 2002).\n</p>\n<h2><span class=\"mw-headline\" id=\"Gaussian_adaptation_and_free_will\">Gaussian adaptation and free will</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=5\" title=\"Edit section: Gaussian adaptation and free will\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Gaussian adaptation as an evolutionary model of the brain obeying the <a href=\"/wiki/Hebbian_theory\" title=\"Hebbian theory\">Hebbian theory</a> of associative learning offers an alternative view of <a href=\"/wiki/Free_will\" title=\"Free will\">free will</a> due to the ability of the process to maximize the mean fitness of signal patterns in the brain by climbing a mental landscape in analogy with phenotypic evolution.\n</p><p>Such a random process gives us lots of freedom of choice, but hardly any will. An illusion of will may, however, emanate from the ability of the process to maximize mean fitness, making the process goal seeking. I. e., it prefers higher peaks in the landscape prior to lower, or better alternatives prior to worse. In this way an illusive will may appear. A similar view has been given by Zohar 1990. See also Kjellstr\u00f6m 1999.\n</p>\n<h2><span class=\"mw-headline\" id=\"A_theorem_of_efficiency_for_random_search\">A theorem of efficiency for random search</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=6\" title=\"Edit section: A theorem of efficiency for random search\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The efficiency of Gaussian adaptation relies on the theory of information due to Claude E. Shannon (see <a href=\"/wiki/Information_content\" class=\"mw-redirect\" title=\"Information content\">information content</a>). When an event occurs with probability <i>P</i>, then the information &#8722;log(<i>P</i>) may be achieved. For instance, if the mean fitness is <i>P</i>, the information gained for each individual selected for survival will be &#8722;log(<i>P</i>) \u2013 on the average - and the work/time needed to get  the information is proportional to 1/<i>P</i>. Thus, if efficiency, E, is defined as information divided by the work/time needed to get it we have:\n</p>\n<dl><dd><i>E</i> = &#8722;<i>P</i> log(<i>P</i>).</dd></dl>\n<p>This function attains its maximum when <i>P</i> = 1/<i>e</i> = 0.37. The same result has been obtained by Gaines with a different method.\n</p><p><i>E</i> = 0 if <i>P</i> = 0, for a process with infinite mutation rate, and if <i>P</i> = 1, for a process with mutation rate = 0 (provided that the process is alive).\nThis measure of efficiency is valid for a large class of <a href=\"/wiki/Random_search\" title=\"Random search\">random search</a> processes provided that certain conditions are at hand.\n</p><p>1  The search should be statistically independent and equally efficient in different parameter directions. This condition may be approximately fulfilled when the moment matrix of the Gaussian has been adapted for maximum <a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a> to some region of acceptability, because linear transformations of the whole process do not affect efficiency.\n</p><p>2   All individuals have equal cost and the derivative at <i>P</i> = 1 is &lt;&#160;0.\n</p><p>Then, the following theorem may be proved:\n</p>\n<blockquote><p>All measures of efficiency, that satisfy the conditions above, are asymptotically proportional to \u2013<i>P</i> log(<i>P/q</i>) when the number of dimensions increases, and are maximized by <i>P</i> = <i>q</i> exp(-1) (Kjellstr\u00f6m, 1996 and 1999).</p></blockquote>\n<p><a href=\"/wiki/File:Efficiency.GIF\" class=\"image\"><img alt=\"Efficiency.GIF\" src=\"//upload.wikimedia.org/wikipedia/en/e/ee/Efficiency.GIF\" width=\"478\" height=\"193\" data-file-width=\"478\" data-file-height=\"193\" /></a>\n</p><p>The figure above shows a possible efficiency function for a random search process such as Gaussian adaptation. To the left the process is most chaotic when <i>P</i> = 0, while there is perfect order to the right where <i>P</i> = 1.\n</p><p>In an example by Rechenberg, 1971, 1973, a random walk is pushed thru a corridor maximizing the parameter <i>x</i><sub>1</sub>. In this case the region of acceptability is defined as a (<i>n</i>&#160;&#8722;&#160;1)-dimensional interval in the parameters <i>x</i><sub>2</sub>, <i>x</i><sub>3</sub>, ..., <i>x</i><sub><i>n</i></sub>, but a <i>x</i><sub>1</sub>-value below the last accepted will never be accepted. Since <i>P</i> can never exceed 0.5 in this case, the maximum speed towards higher <i>x</i><sub>1</sub>-values is reached for <i>P</i> = 0.5/<i>e</i> = 0.18, in agreement with the findings of Rechenberg.\n</p><p>A point of view that also may be of interest in this context is that no definition of information (other than that sampled points inside some region of acceptability gives information about the extension of the region) is needed for the proof of the theorem. Then, because, the formula may be interpreted as information divided by the work needed to get the information, this is also an indication that &#8722;log(<i>P</i>) is a good candidate for being a measure of information.\n</p>\n<h2><span class=\"mw-headline\" id=\"The_Stauffer_and_Grimson_algorithm\">The Stauffer and Grimson algorithm</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=7\" title=\"Edit section: The Stauffer and Grimson algorithm\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Gaussian adaptation has also been used for other purposes as for instance shadow removal by \"The Stauffer-Grimson algorithm\" which is equivalent to Gaussian adaptation as used in the section \"Computer simulation of Gaussian adaptation\" above. In both cases the maximum likelihood method is used for estimation of mean values by adaptation at one sample at a time.\n</p><p>But there are differences. In the Stauffer-Grimson case the information is not used for the control of a random number generator for centering, maximization of mean fitness, <a href=\"/wiki/Average_information\" class=\"mw-redirect\" title=\"Average information\">average information</a> or manufacturing yield. The adaptation of the moment matrix also differs very much as compared to \"the evolution in the brain\" above.\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=8\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Entropy_in_thermodynamics_and_information_theory\" title=\"Entropy in thermodynamics and information theory\">Entropy in thermodynamics and information theory</a></li>\n<li><a href=\"/wiki/Fisher%27s_fundamental_theorem_of_natural_selection\" title=\"Fisher&#39;s fundamental theorem of natural selection\">Fisher's fundamental theorem of natural selection</a></li>\n<li><a href=\"/wiki/Free_will\" title=\"Free will\">Free will</a></li>\n<li><a href=\"/wiki/Genetic_algorithm\" title=\"Genetic algorithm\">Genetic algorithm</a></li>\n<li><a href=\"/wiki/Hebbian_learning\" class=\"mw-redirect\" title=\"Hebbian learning\">Hebbian learning</a></li>\n<li><a href=\"/wiki/Information_content\" class=\"mw-redirect\" title=\"Information content\">Information content</a></li>\n<li><a href=\"/wiki/Simulated_annealing\" title=\"Simulated annealing\">Simulated annealing</a></li>\n<li><a href=\"/wiki/Stochastic_optimization\" title=\"Stochastic optimization\">Stochastic optimization</a></li>\n<li><a href=\"/wiki/CMA-ES\" title=\"CMA-ES\">Covariance matrix adaptation evolution strategy (CMA-ES)</a></li>\n<li><a href=\"/wiki/Unit_of_selection\" title=\"Unit of selection\">Unit of selection</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Gaussian_adaptation&amp;action=edit&amp;section=9\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li>Bergstr\u00f6m, R. M. An Entropy Model of the Developing Brain. <i><a href=\"/wiki/Developmental_Psychobiology\" class=\"mw-redirect\" title=\"Developmental Psychobiology\">Developmental Psychobiology</a></i>, 2(3): 139\u2013152, 1969.</li>\n<li>Brooks, D. R. &amp; Wiley, E. O. <i>Evolution as Entropy, Towards a unified theory of Biology</i>. The University of Chicago Press, 1986.</li>\n<li>Brooks, D. R. Evolution in the Information Age: Rediscovering the Nature of the Organism. Semiosis, Evolution, Energy, Development, Volume 1, Number 1, March 2001</li>\n<li>Gaines, Brian R. Knowledge Management in Societies of Intelligent Adaptive Agents. <i>Journal of intelligent Information systems</i> 9, 277\u2013298 (1997).</li>\n<li>Hartl, D. L. <i>A Primer of Population Genetics</i>. Sinauer, Sunderland, Massachusetts, 1981.</li>\n<li>Hamilton, WD. 1963. The evolution of altruistic behavior. American Naturalist 97:354\u2013356</li>\n<li>Kandel, E. R., Schwartz, J. H., Jessel, T. M. <i>Essentials of Neural Science and Behavior</i>. Prentice Hall International, London, 1995.</li>\n<li>S. Kirkpatrick and C. D. Gelatt and M. P. Vecchi, Optimization by Simulated Annealing, Science, Vol 220, Number 4598, pages 671\u2013680, 1983.</li>\n<li>Kjellstr\u00f6m, G. Network Optimization by Random Variation of component values. <i>Ericsson Technics</i>, vol. 25, no. 3, pp.&#160;133\u2013151, 1969.</li>\n<li>Kjellstr\u00f6m, G. Optimization of electrical Networks with respect to Tolerance Costs. <i>Ericsson Technics</i>, no. 3, pp.&#160;157\u2013175, 1970.</li>\n<li>Kjellstr\u00f6m, G. &amp; Tax\u00e9n, L. Stochastic Optimization in System Design. IEEE Trans. on Circ. and Syst., vol. CAS-28, no. 7, July 1981.</li>\n<li>Kjellstr\u00f6m, G., Tax\u00e9n, L. and Lindberg, P. O. Discrete Optimization of Digital Filters Using Gaussian Adaptation and Quadratic Function Minimization. IEEE Trans. on Circ. and Syst., vol. CAS-34, no 10, October 1987.</li>\n<li>Kjellstr\u00f6m, G. On the Efficiency of Gaussian Adaptation. <i>Journal of Optimization Theory and Applications</i>, vol. 71, no. 3, December 1991.</li>\n<li>Kjellstr\u00f6m, G. &amp; Tax\u00e9n, L. Gaussian Adaptation, an evolution-based efficient global optimizer; Computational and Applied Mathematics, In, C. Brezinski &amp; U. Kulish (Editors), Elsevier Science Publishers B. V., pp 267\u2013276, 1992.</li>\n<li>Kjellstr\u00f6m, G. Evolution as a statistical optimization algorithm. <i>Evolutionary Theory</i> 11:105\u2013117 (January, 1996).</li>\n<li>Kjellstr\u00f6m, G. The evolution in the brain. <i>Applied Mathematics and Computation</i>, 98(2\u20133):293\u2013300, February, 1999.</li>\n<li>Kjellstr\u00f6m, G. Evolution in a nutshell and some consequences concerning valuations. EVOLVE, <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/91-972936-1-X\" title=\"Special:BookSources/91-972936-1-X\">91-972936-1-X</a>, Stockholm, 2002.</li>\n<li>Levine, D. S. Introduction to Neural &amp; Cognitive Modeling. Laurence Erlbaum Associates, Inc., Publishers, 1991.</li>\n<li>MacLean, P. D. <i>A Triune Concept of the Brain and Behavior</i>. Toronto, Univ. Toronto Press, 1973.</li>\n<li>Maynard Smith, J. 1964. Group Selection and Kin Selection, Nature 201:1145\u20131147.</li>\n<li>Maynard Smith, J. <i>Evolutionary Genetics</i>. Oxford University Press, 1998.</li>\n<li>Mayr, E. <i>What Evolution is</i>. Basic Books, New York, 2001.</li>\n<li>M\u00fcller, Christian L. and Sbalzarini Ivo F. Gaussian Adaptation revisited - an entropic view on Covariance Matrix Adaptation. Institute of Theoretical Computer Science and <a href=\"/wiki/Swiss_Institute_of_Bioinformatics\" title=\"Swiss Institute of Bioinformatics\">Swiss Institute of Bioinformatics</a>, <a href=\"/wiki/ETH_Zurich\" title=\"ETH Zurich\">ETH Zurich</a>, CH-8092 Zurich, Switzerland.</li>\n<li>Pinel, J. F. and Singhal, K. Statistical Design Centering and Tolerancing Using Parametric Sampling. IEEE Transactions on Circuits and Systems, Vol. Das-28, No. 7, July 1981.</li>\n<li>Rechenberg, I. (1971): Evolutionsstrategie &#8212; Optimierung technischer Systeme nach Prinzipien der biologischen Evolution (PhD thesis). Reprinted by Fromman-Holzboog (1973).</li>\n<li>Ridley, M. <i>Evolution</i>. Blackwell Science, 1996.</li>\n<li>Stauffer, C. &amp; Grimson, W.E.L. Learning Patterns of Activity Using Real-Time Tracking, IEEE Trans. on PAMI, 22(8), 2000.</li>\n<li>Stehr, G. On the Performance Space Exploration of Analog Integrated Circuits. Technischen Universit\u00e4t Munchen, Dissertation 2005.</li>\n<li>Tax\u00e9n, L. A Framework for the Coordination of Complex Systems\u2019 Development. Institute of Technology, Link\u00f6ping University, Dissertation, 2003.</li>\n<li>Zohar, D. <i>The quantum self&#160;: a revolutionary view of human nature and consciousness rooted in the new physics</i>. London, Bloomsbury, 1990.</li></ul>\n\n<!-- \nNewPP limit report\nParsed by mw1305\nCached time: 20180904082921\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.196 seconds\nReal time usage: 0.365 seconds\nPreprocessor visited node count: 700/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 38012/2097152 bytes\nTemplate argument size: 11544/2097152 bytes\nHighest expansion depth: 15/40\nExpensive parser function count: 5/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 36/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.049/10.000 seconds\nLua memory usage: 1.54 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  224.755      1 -total\n 99.18%  222.904      5 Template:Ambox\n 74.41%  167.230      1 Template:Multiple_issues\n 30.16%   67.789      1 Template:Refimprove\n 25.36%   57.007      1 Template:ISBN\n 15.51%   34.856      1 Template:Catalog_lookup_link\n 10.34%   23.248      1 Template:Expert-subject\n  7.29%   16.393      6 Template:Trim\n  4.43%    9.957      1 Template:Primary_sources\n  4.09%    9.197      1 Template:Category_handler\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:9210345-0!canonical!math=5 and timestamp 20180904082922 and revision id 793268977\n -->\n</div>"},"langlinks":[],"categories":[{"sortkey":"Gaussian Adaptation","hidden":"","*":"Articles_needing_additional_references_from_July_2008"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"All_articles_needing_additional_references"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"Articles_lacking_reliable_references_from_July_2008"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"All_articles_lacking_reliable_references"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"Articles_needing_expert_attention_with_no_reason_or_talk_parameter"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"Articles_needing_expert_attention_from_January_2015"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"All_articles_needing_expert_attention"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"Mathematics_articles_needing_expert_attention"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"Wikipedia_articles_with_possible_conflicts_of_interest_from_March_2009"},{"sortkey":"Gaussian Adaptation","hidden":"","*":"Articles_with_multiple_maintenance_issues"},{"sortkey":"Gaussian Adaptation","*":"Evolutionary_algorithms"},{"sortkey":"Gaussian Adaptation","*":"Creativity"},{"sortkey":"Gaussian Adaptation","*":"Creationism"},{"sortkey":"Gaussian Adaptation","*":"Free_will"}],"links":[{"ns":14,"exists":"","*":"Category:Articles needing additional references from July 2008"},{"ns":14,"exists":"","*":"Category:Articles lacking reliable references from July 2008"},{"ns":14,"exists":"","*":"Category:Articles needing expert attention from January 2015"},{"ns":14,"exists":"","*":"Category:Mathematics articles needing expert attention"},{"ns":14,"exists":"","*":"Category:Wikipedia articles with possible conflicts of interest from March 2009"},{"ns":0,"exists":"","*":"Average information"},{"ns":0,"exists":"","*":"CMA-ES"},{"ns":0,"exists":"","*":"Central limit theorem"},{"ns":0,"exists":"","*":"Covariance matrix"},{"ns":0,"exists":"","*":"Developmental Psychobiology"},{"ns":0,"exists":"","*":"ETH Zurich"},{"ns":0,"exists":"","*":"Entropy in thermodynamics and information theory"},{"ns":0,"exists":"","*":"Evolutionary algorithm"},{"ns":0,"exists":"","*":"Fisher's fundamental theorem of natural selection"},{"ns":0,"exists":"","*":"Free will"},{"ns":0,"exists":"","*":"Genetic algorithm"},{"ns":0,"exists":"","*":"Hardy\u2013Weinberg law"},{"ns":0,"exists":"","*":"Hebbian learning"},{"ns":0,"exists":"","*":"Hebbian theory"},{"ns":0,"exists":"","*":"Information content"},{"ns":0,"exists":"","*":"Information entropy"},{"ns":0,"exists":"","*":"International Standard Book Number"},{"ns":0,"exists":"","*":"Mean fitness"},{"ns":0,"exists":"","*":"Multivariate Gaussian distribution"},{"ns":0,"exists":"","*":"Normal distribution"},{"ns":0,"exists":"","*":"Probability density function"},{"ns":0,"exists":"","*":"Punctuated equilibrium"},{"ns":0,"exists":"","*":"Random search"},{"ns":0,"exists":"","*":"Signal processing"},{"ns":0,"exists":"","*":"Simulated annealing"},{"ns":0,"exists":"","*":"Stochastic optimization"},{"ns":0,"exists":"","*":"Swiss Institute of Bioinformatics"},{"ns":0,"exists":"","*":"Unit of selection"},{"ns":1,"exists":"","*":"Talk:Gaussian adaptation"},{"ns":4,"exists":"","*":"Wikipedia:Cleanup"},{"ns":4,"exists":"","*":"Wikipedia:Conflict of interest"},{"ns":4,"exists":"","*":"Wikipedia:Neutral point of view"},{"ns":4,"exists":"","*":"Wikipedia:No original research"},{"ns":4,"exists":"","*":"Wikipedia:Verifiability"},{"ns":4,"exists":"","*":"Wikipedia:WikiProject Mathematics"},{"ns":12,"exists":"","*":"Help:Introduction to referencing with Wiki Markup/1"},{"ns":12,"exists":"","*":"Help:Maintenance template removal"}],"templates":[{"ns":10,"exists":"","*":"Template:Multiple issues"},{"ns":10,"exists":"","*":"Template:Ambox"},{"ns":10,"exists":"","*":"Template:Refimprove"},{"ns":10,"exists":"","*":"Template:More citations needed"},{"ns":10,"exists":"","*":"Template:Primary sources"},{"ns":10,"exists":"","*":"Template:Expert-subject"},{"ns":10,"exists":"","*":"Template:Expert needed"},{"ns":10,"exists":"","*":"Template:Category handler"},{"ns":10,"exists":"","*":"Template:Expert needed/catcheck"},{"ns":10,"exists":"","*":"Template:COI"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":10,"exists":"","*":"Template:ISBN"},{"ns":10,"exists":"","*":"Template:Catalog lookup link"},{"ns":10,"exists":"","*":"Template:Trim"},{"ns":10,"exists":"","*":"Template:Yesno-no"},{"ns":10,"exists":"","*":"Template:Yesno"},{"ns":10,"exists":"","*":"Template:Error-small"},{"ns":10,"exists":"","*":"Template:Tl"},{"ns":828,"exists":"","*":"Module:Message box"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Message box/configuration"},{"ns":828,"exists":"","*":"Module:Unsubst"},{"ns":828,"exists":"","*":"Module:Category handler"},{"ns":828,"exists":"","*":"Module:Category handler/data"},{"ns":828,"exists":"","*":"Module:Category handler/config"},{"ns":828,"exists":"","*":"Module:Category handler/shared"},{"ns":828,"exists":"","*":"Module:Category handler/blacklist"},{"ns":828,"exists":"","*":"Module:Namespace detect/data"},{"ns":828,"exists":"","*":"Module:Namespace detect/config"},{"ns":828,"exists":"","*":"Module:String"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"},{"ns":828,"exists":"","*":"Module:Check isxn"},{"ns":828,"exists":"","*":"Module:Error"}],"images":["Ambox_important.svg","Question_book-new.svg","Unbalanced_scales.svg","Fraktal.gif","Mountain_crest.GIF","Schematic_of_a_neural_network_executing_the_Gaussian_adaptation_algorithm.GIF","Efficiency.GIF"],"externallinks":[],"sections":[{"toclevel":1,"level":"2","line":"Natural evolution and Gaussian adaptation","number":"1","index":"1","fromtitle":"Gaussian_adaptation","byteoffset":3156,"anchor":"Natural_evolution_and_Gaussian_adaptation"},{"toclevel":1,"level":"2","line":"How to climb a mountain","number":"2","index":"2","fromtitle":"Gaussian_adaptation","byteoffset":4409,"anchor":"How_to_climb_a_mountain"},{"toclevel":1,"level":"2","line":"Computer simulation of Gaussian adaptation","number":"3","index":"3","fromtitle":"Gaussian_adaptation","byteoffset":6005,"anchor":"Computer_simulation_of_Gaussian_adaptation"},{"toclevel":1,"level":"2","line":"The evolution in the brain","number":"4","index":"4","fromtitle":"Gaussian_adaptation","byteoffset":8962,"anchor":"The_evolution_in_the_brain"},{"toclevel":1,"level":"2","line":"Gaussian adaptation and free will","number":"5","index":"5","fromtitle":"Gaussian_adaptation","byteoffset":11496,"anchor":"Gaussian_adaptation_and_free_will"},{"toclevel":1,"level":"2","line":"A theorem of efficiency for random search","number":"6","index":"6","fromtitle":"Gaussian_adaptation","byteoffset":12284,"anchor":"A_theorem_of_efficiency_for_random_search"},{"toclevel":1,"level":"2","line":"The Stauffer and Grimson algorithm","number":"7","index":"7","fromtitle":"Gaussian_adaptation","byteoffset":15411,"anchor":"The_Stauffer_and_Grimson_algorithm"},{"toclevel":1,"level":"2","line":"See also","number":"8","index":"8","fromtitle":"Gaussian_adaptation","byteoffset":16148,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"References","number":"9","index":"9","fromtitle":"Gaussian_adaptation","byteoffset":16512,"anchor":"References"}],"parsewarnings":[],"displaytitle":"Gaussian adaptation","iwlinks":[],"properties":[{"name":"defaultsort","*":"Gaussian Adaptation"},{"name":"wikibase_item","*":"Q5527832"}]}}