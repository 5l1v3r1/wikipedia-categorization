{"parse":{"title":"Superintelligence","pageid":726659,"revid":851397514,"text":{"*":"<div class=\"mw-parser-output\"><div role=\"note\" class=\"hatnote navigation-not-searchable\">For the book by Nick Bostrom, see <a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a>.</div>\n<p>A <b>superintelligence</b> is a hypothetical <a href=\"/wiki/Intelligent_agent\" title=\"Intelligent agent\">agent</a> that possesses <a href=\"/wiki/Intelligence\" title=\"Intelligence\">intelligence</a> far surpassing that of the <a href=\"/wiki/Genius\" title=\"Genius\">brightest</a> and most <a href=\"/wiki/Intellectual_giftedness\" title=\"Intellectual giftedness\">gifted</a> human minds. \"Superintelligence\" may also refer to a property of problem-solving systems (e.g., superintelligent language translators or engineering assistants) whether or not these high-level intellectual competencies are embodied in agents that act in the world. A superintelligence may or may not be created by an <a href=\"/wiki/Intelligence_explosion\" class=\"mw-redirect\" title=\"Intelligence explosion\">intelligence explosion</a> and associated with a <a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">technological singularity</a>.\n</p><p><a href=\"/wiki/University_of_Oxford\" title=\"University of Oxford\">University of Oxford</a> philosopher <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a> defines <i>superintelligence</i> as \"any intellect that greatly exceeds the cognitive performance of humans in virtually all domains of interest\".<sup id=\"cite_ref-FOOTNOTEBostrom2014Chapter_2_1-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom2014Chapter_2-1\">&#91;1&#93;</a></sup> The program <a href=\"/wiki/Fritz_(chess)\" title=\"Fritz (chess)\">Fritz</a> falls short of superintelligence even though it is much better than humans at chess because Fritz cannot outperform humans in other tasks.<sup id=\"cite_ref-FOOTNOTEBostrom201422_2-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom201422-2\">&#91;2&#93;</a></sup> Following Hutter and Legg, Bostrom treats superintelligence as general dominance at goal-oriented behavior, leaving open whether an artificial or human superintelligence would possess capacities such as <a href=\"/wiki/Intentionality\" title=\"Intentionality\">intentionality</a> (cf. the <a href=\"/wiki/Chinese_room\" title=\"Chinese room\">Chinese room</a> argument) or <a href=\"/wiki/Qualia\" title=\"Qualia\">first-person consciousness</a> (cf. the <a href=\"/wiki/Hard_problem_of_consciousness\" title=\"Hard problem of consciousness\">hard problem of consciousness</a>).\n</p><p>Technological researchers disagree about how likely present-day <a href=\"/wiki/Human_intelligence\" title=\"Human intelligence\">human intelligence</a> is to be surpassed. Some argue that advances in <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> (AI) will probably result in general reasoning systems that lack human cognitive limitations. Others believe that humans will evolve or directly modify their biology so as to achieve radically greater intelligence. A number of <a href=\"/wiki/Futures_studies\" title=\"Futures studies\">futures studies</a> scenarios combine elements from both of these possibilities, suggesting that humans are likely to <a href=\"/wiki/Brain%E2%80%93computer_interface\" title=\"Brain\u2013computer interface\">interface with computers</a>, or <a href=\"/wiki/Mind_uploading\" title=\"Mind uploading\">upload their minds to computers</a>, in a way that enables substantial intelligence amplification.\n</p><p>Some researchers believe that superintelligence will likely follow shortly after the development of <a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">artificial general intelligence</a>. The first generally intelligent machines are likely to immediately hold an enormous advantage in at least some forms of mental capability, including the capacity of <a href=\"/wiki/Perfect_recall\" class=\"mw-redirect\" title=\"Perfect recall\">perfect recall</a>, a vastly superior knowledge base, and the ability to <a href=\"/wiki/Human_multitasking\" title=\"Human multitasking\">multitask</a> in ways not possible to biological entities. This may give them the opportunity to\u2014either as a single being or as a new <a href=\"/wiki/Species\" title=\"Species\">species</a>\u2014become much more powerful than humans, and to displace them.<sup id=\"cite_ref-FOOTNOTEBostrom2014Chapter_2_1-1\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom2014Chapter_2-1\">&#91;1&#93;</a></sup>\n</p><p>A number of scientists and forecasters argue for prioritizing early research into the possible benefits and risks of <a href=\"/wiki/Intelligence_amplification\" title=\"Intelligence amplification\">human and machine cognitive enhancement</a>, because of the potential social impact of such technologies.<sup id=\"cite_ref-FOOTNOTELegg2008135-137_3-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTELegg2008135-137-3\">&#91;3&#93;</a></sup>\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Feasibility_of_artificial_superintelligence\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Feasibility of artificial superintelligence</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Feasibility_of_biological_superintelligence\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Feasibility of biological superintelligence</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Forecasts\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Forecasts</span></a></li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Design_considerations\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Design considerations</span></a></li>\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Danger_to_human_survival_and_the_AI_control_problem\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Danger to human survival and the AI control problem</span></a></li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#See_also\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#Citations\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Citations</span></a></li>\n<li class=\"toclevel-1 tocsection-8\"><a href=\"#Bibliography\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">Bibliography</span></a></li>\n<li class=\"toclevel-1 tocsection-9\"><a href=\"#External_links\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Feasibility_of_artificial_superintelligence\">Feasibility of artificial superintelligence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=1\" title=\"Edit section: Feasibility of artificial superintelligence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a href=\"/wiki/File:Classification_of_images_progress_human.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Classification_of_images_progress_human.png/220px-Classification_of_images_progress_human.png\" width=\"220\" height=\"147\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Classification_of_images_progress_human.png/330px-Classification_of_images_progress_human.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/7c/Classification_of_images_progress_human.png/440px-Classification_of_images_progress_human.png 2x\" data-file-width=\"1445\" data-file-height=\"967\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Classification_of_images_progress_human.png\" class=\"internal\" title=\"Enlarge\"></a></div>Progress in machine classification of images <hr /> The error rate of AI by year. Red line - the error rate of a trained human</div></div></div>\n<p>Philosopher <a href=\"/wiki/David_Chalmers\" title=\"David Chalmers\">David Chalmers</a> argues that <a href=\"/wiki/Artificial_general_intelligence\" title=\"Artificial general intelligence\">artificial general intelligence</a> is a very likely path to superhuman intelligence. Chalmers breaks this claim down into an argument that AI can achieve <i>equivalence</i> to human intelligence, that it can be <i>extended</i> to surpass human intelligence, and that it can be further <i>amplified</i> to completely dominate humans across arbitrary tasks.<sup id=\"cite_ref-FOOTNOTEChalmers20107_4-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEChalmers20107-4\">&#91;4&#93;</a></sup>\n</p><p>Concerning human-level equivalence, Chalmers argues that the human brain is a mechanical system, and therefore ought to be emulatable by synthetic materials.<sup id=\"cite_ref-FOOTNOTEChalmers20107-9_5-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEChalmers20107-9-5\">&#91;5&#93;</a></sup> He also notes that human intelligence was able to biologically evolve, making it more likely that human engineers will be able to recapitulate this invention. <a href=\"/wiki/Evolutionary_algorithm\" title=\"Evolutionary algorithm\">Evolutionary algorithms</a> in particular should be able to produce human-level AI.<sup id=\"cite_ref-FOOTNOTEChalmers201010-11_6-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEChalmers201010-11-6\">&#91;6&#93;</a></sup> Concerning intelligence extension and amplification, Chalmers argues that new AI technologies can generally be improved on, and that this is particularly likely when the invention can assist in designing new technologies.<sup id=\"cite_ref-FOOTNOTEChalmers201011-13_7-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEChalmers201011-13-7\">&#91;7&#93;</a></sup>\n</p><p>If research into strong AI produced sufficiently intelligent software, it would be able to reprogram and improve itself &#8211; a feature called \"recursive self-improvement\". It would then be even better at improving itself, and could continue doing so in a rapidly increasing cycle, leading to a superintelligence. This scenario is known as an <a href=\"/wiki/Intelligence_explosion\" class=\"mw-redirect\" title=\"Intelligence explosion\">intelligence explosion</a>.  Such an intelligence would not have the limitations of human intellect, and may be able to invent or discover almost anything.\n</p><p>Computer components already greatly surpass human performance in speed.  Bostrom writes, \"Biological neurons operate at a peak speed of about 200 Hz, a full seven orders of magnitude slower than a modern microprocessor (~2 GHz).\"<sup id=\"cite_ref-FOOTNOTEBostrom201459_8-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom201459-8\">&#91;8&#93;</a></sup> Moreover, <a href=\"/wiki/Neuron\" title=\"Neuron\">neurons</a> transmit spike signals across <a href=\"/wiki/Axon\" title=\"Axon\">axons</a> at no greater than 120 m/s, \"whereas existing electronic processing cores can communicate optically at the speed of light\". Thus, the simplest example of a superintelligence may be an emulated human mind that's run on much faster hardware than the brain. A human-like reasoner that could think millions of times faster than current humans would have a dominant advantage in most reasoning tasks, particularly ones that require haste or long strings of actions.\n</p><p>Another advantage of computers is modularity, that is, their size or computational capacity can be increased.  A non-human (or modified human) brain could become much larger than a present-day human brain, like many <a href=\"/wiki/Supercomputer\" title=\"Supercomputer\">supercomputers</a>. Bostrom also raises the possibility of <i>collective superintelligence</i>: a large enough number of separate reasoning systems, if they communicated and coordinated well enough, could act in aggregate with far greater capabilities than any sub-agent.\n</p><p>There may also be ways to <i>qualitatively</i> improve on human reasoning and decision-making. Humans appear to differ from <a href=\"/wiki/Chimpanzee\" title=\"Chimpanzee\">chimpanzees</a> in the ways we think more than we differ in brain size or speed.<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup> Humans outperform non-human animals in large part because of new or enhanced reasoning capacities, such as long-term planning and <a href=\"/wiki/Great_ape_language\" title=\"Great ape language\">language use</a>. (See <a href=\"/wiki/Evolution_of_human_intelligence\" title=\"Evolution of human intelligence\">evolution of human intelligence</a> and <a href=\"/wiki/Primate_cognition\" title=\"Primate cognition\">primate cognition</a>.) If there are other possible improvements to reasoning that would have a similarly large impact, this makes it likelier that an agent can be built that outperforms humans in the same fashion humans outperform chimpanzees.<sup id=\"cite_ref-FOOTNOTEBostrom201456-57_10-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom201456-57-10\">&#91;10&#93;</a></sup>\n</p><p>All of the above advantages hold for artificial superintelligence, but it is not clear how many hold for biological superintelligence. Physiological constraints limit the speed and size of biological brains in many ways that are inapplicable to machine intelligence. As such, writers on superintelligence have devoted much more attention to superintelligent AI scenarios.<sup id=\"cite_ref-FOOTNOTEBostrom201452,_59-61_11-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom201452,_59-61-11\">&#91;11&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Feasibility_of_biological_superintelligence\">Feasibility of biological superintelligence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=2\" title=\"Edit section: Feasibility of biological superintelligence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p><a href=\"/wiki/Carl_Sagan\" title=\"Carl Sagan\">Carl Sagan</a> suggested that the advent of <a href=\"/wiki/Caesarean_section\" title=\"Caesarean section\">Caesarean sections</a> and <a href=\"/wiki/In_vitro_fertilisation\" title=\"In vitro fertilisation\"><i>in vitro</i> fertilization</a> may permit humans to evolve larger heads, resulting in improvements via <a href=\"/wiki/Natural_selection\" title=\"Natural selection\">natural selection</a> in the <a href=\"/wiki/Adaptive_evolution_in_the_human_genome\" title=\"Adaptive evolution in the human genome\">heritable</a> component of <a href=\"/wiki/Human_intelligence\" title=\"Human intelligence\">human intelligence</a>.<sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup> By contrast, <a href=\"/wiki/Gerald_Crabtree\" title=\"Gerald Crabtree\">Gerald Crabtree</a> has argued that decreased selection pressure is resulting in a slow, centuries-long <a href=\"/wiki/Fertility_and_Intelligence\" class=\"mw-redirect\" title=\"Fertility and Intelligence\">reduction in human intelligence</a>, and that this process instead is likely to continue into the future. There is no scientific consensus concerning either possibility, and in both cases the biological change would be slow, especially relative to rates of cultural change.\n</p><p><a href=\"/wiki/Selective_breeding\" title=\"Selective breeding\">Selective breeding</a>, <a href=\"/wiki/Nootropics\" class=\"mw-redirect\" title=\"Nootropics\">nootropics</a>, <a href=\"/wiki/NSI-189\" title=\"NSI-189\">NSI-189</a>, <a href=\"/wiki/Monoamine_oxidase_inhibitor\" title=\"Monoamine oxidase inhibitor\">MAO-I's</a>, <a href=\"/wiki/Epigenetics\" title=\"Epigenetics\">epigenetic modulation</a>, and <a href=\"/wiki/Genetic_engineering\" title=\"Genetic engineering\">genetic engineering</a> could improve human intelligence more rapidly. Bostrom writes that if we come to understand the genetic component of intelligence, pre-implantation genetic diagnosis could be used to select for embryos with as much as 4 points of IQ gain (if one embryo is selected out of two), or with larger gains (e.g., up to 24.3 IQ points gained if one embryo is selected out of 1000). If this process is iterated over many generations, the gains could be an order of magnitude greater. Bostrom suggests that deriving new gametes from embryonic stem cells could be used to iterate the selection process very rapidly.<sup id=\"cite_ref-FOOTNOTEBostrom201437-39_13-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom201437-39-13\">&#91;13&#93;</a></sup> A well-organized society of high-intelligence humans of this sort could potentially achieve <a href=\"/wiki/Collective_intelligence\" title=\"Collective intelligence\">collective</a> superintelligence.<sup id=\"cite_ref-FOOTNOTEBostrom201439_14-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom201439-14\">&#91;14&#93;</a></sup>\n</p><p>Alternatively, collective intelligence might be constructible by better organizing humans at present levels of individual intelligence. A number of writers have suggested that human civilization, or some aspect of it (e.g., the Internet, or the economy), is coming to function like a <a href=\"/wiki/Global_brain\" title=\"Global brain\">global brain</a> with capacities far exceeding its component agents. If this systems-based superintelligence relies heavily on artificial components, however, it may qualify as an AI rather than as a biology-based <a href=\"/wiki/Superorganism\" title=\"Superorganism\">superorganism</a>.<sup id=\"cite_ref-FOOTNOTEBostrom201448-49_15-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom201448-49-15\">&#91;15&#93;</a></sup>\n</p><p>A final method of intelligence amplification would be to directly <a href=\"/wiki/Neuroenhancement\" title=\"Neuroenhancement\">enhance</a> individual humans, as opposed to enhancing their social or reproductive dynamics. This could be achieved using <a href=\"/wiki/Nootropics\" class=\"mw-redirect\" title=\"Nootropics\">nootropics</a>, somatic <a href=\"/wiki/Gene_therapy\" title=\"Gene therapy\">gene therapy</a>, or <a href=\"/wiki/Brain%E2%80%93computer_interface\" title=\"Brain\u2013computer interface\">brain\u2013computer interfaces</a>. However, Bostrom expresses skepticism about the scalability of the first two approaches, and argues that designing a superintelligent <a href=\"/wiki/Cyborg\" title=\"Cyborg\">cyborg</a> interface is an <a href=\"/wiki/AI-complete\" title=\"AI-complete\">AI-complete</a> problem.<sup id=\"cite_ref-FOOTNOTEBostrom201436-37,_42,_47_16-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom201436-37,_42,_47-16\">&#91;16&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Forecasts\">Forecasts</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=3\" title=\"Edit section: Forecasts\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Most surveyed AI researchers expect machines to eventually be able to rival humans in intelligence, though there is little consensus on when this will likely happen. At the 2006 <a href=\"/wiki/AI@50\" title=\"AI@50\">AI@50</a> conference, 18% of attendees reported expecting machines to be able \"to simulate learning and every other aspect of human intelligence\" by 2056; 41% of attendees expected this to happen sometime after 2056; and 41% expected machines to never reach that milestone.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup> \n</p><p>In a survey of the 100 most cited authors in AI (as of May 2013, according to Microsoft academic search), the median year by which respondents expected machines \"that can carry out most human professions at least as well as a typical human\" (assuming no <a href=\"/wiki/Global_catastrophic_risks\" class=\"mw-redirect\" title=\"Global catastrophic risks\">global catastrophe</a> occurs) with 10% confidence is 2024 (mean 2034, st. dev. 33 years), with 50% confidence is 2050 (mean 2072, st. dev. 110 years), and with 90% confidence is 2070 (mean 2168, st. dev. 342 years). These estimates exclude the 1.2% of respondents who said no year would ever reach 10% confidence, the 4.1% who said 'never' for 50% confidence, and the 16.5% who said 'never' for 90% confidence. Respondents assigned a median 50% probability to the possibility that machine superintelligence will be invented within 30 years of the invention of approximately human-level machine intelligence.<sup id=\"cite_ref-FOOTNOTEM\u00fcllerBostrom20163-4,_6,_9-12_18-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEM\u00fcllerBostrom20163-4,_6,_9-12-18\">&#91;18&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Design_considerations\">Design considerations</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=4\" title=\"Edit section: Design considerations\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Bostrom expressed concern about what values a superintelligence should be designed to have. He compared several proposals:<sup id=\"cite_ref-FOOTNOTEBostrom2014209-221_19-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEBostrom2014209-221-19\">&#91;19&#93;</a></sup>\n</p>\n<ul><li>The coherent extrapolated volition (CEV) proposal is that it should have the values upon which humans would converge.</li>\n<li>The moral rightness (MR) proposal is that it should value moral rightness.</li>\n<li>The moral permissibility (MP) proposal is that it should value staying within the bounds of moral permissibility (and otherwise have CEV values).</li></ul>\n<p>Responding to Bostrom, Santos-Lang raised concern that developers may attempt to start with a single kind of superintelligence.<sup id=\"cite_ref-FOOTNOTESantos-Lang201416-19_20-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTESantos-Lang201416-19-20\">&#91;20&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Danger_to_human_survival_and_the_AI_control_problem\">Danger to human survival and the AI control problem</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=5\" title=\"Edit section: Danger to human survival and the AI control problem\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main articles: <a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a> and <a href=\"/wiki/AI_control_problem\" title=\"AI control problem\">AI control problem</a></div>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Further information: <a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">Friendly artificial intelligence</a></div>\n<p>Learning computers that rapidly become superintelligent may take unforeseen actions or <a href=\"/wiki/Robot#Social_impact\" title=\"Robot\">robots</a> might out-compete humanity (one potential <a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">technological singularity</a> scenario).<sup id=\"cite_ref-billjoy_21-0\" class=\"reference\"><a href=\"#cite_note-billjoy-21\">&#91;21&#93;</a></sup> Researchers have argued that, by way of an \"intelligence explosion\" sometime over the next century, a self-improving AI could become so powerful as to be unstoppable by humans.<sup id=\"cite_ref-Muehlhauser,_Luke_2012_22-0\" class=\"reference\"><a href=\"#cite_note-Muehlhauser,_Luke_2012-22\">&#91;22&#93;</a></sup>\n</p><p>Concerning human extinction scenarios, <a href=\"#CITEREFBostrom2002\">Bostrom (2002)</a> identifies superintelligence as a possible cause:\n</p>\n<style data-mw-deduplicate=\"TemplateStyles:r856303468\">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class=\"templatequote\"><p>When we create the first superintelligent entity, we might make a mistake and give it goals that lead it to annihilate humankind, assuming its enormous intellectual advantage gives it the power to do so. For example, we could mistakenly elevate a subgoal to the status of a supergoal. We tell it to solve a mathematical problem, and it complies by turning all the matter in the solar system into a giant calculating device, in the process killing the person who asked the question.\n</p></blockquote>\n<p>In theory, since a superintelligent AI would be able to bring about almost any possible outcome and to thwart any attempt to prevent the implementation of its goals, many uncontrolled, <a href=\"/wiki/Unintended_consequences\" title=\"Unintended consequences\">unintended consequences</a> could arise. It could kill off all other agents, persuade them to change their behavior, or block their attempts at interference.<sup id=\"cite_ref-Bostrom,_Nick_2003_23-0\" class=\"reference\"><a href=\"#cite_note-Bostrom,_Nick_2003-23\">&#91;23&#93;</a></sup>\n</p><p><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a> explains: \"The AI does not hate you, nor does it love you, but you are made out of atoms which it can use for something else.\"<sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\">&#91;24&#93;</a></sup>\n</p><p>This presents the <a href=\"/wiki/AI_control_problem\" title=\"AI control problem\">AI control problem</a>: how to build a superintelligent agent that will aid its creators, while avoiding inadvertently building a superintelligence that will harm its creators. The danger of not designing control right \"the first time\", is that a misprogrammed superintelligence might rationally decide to \"take over the world\" and refuse to permit its programmers to modify it once it has been activated. Potential design strategies include \"capability control\" (preventing an AI from being able to pursue harmful plans), and \"motivational control\" (building an AI that wants to be helpful).\n</p><p><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a> advocates for public education about superintelligence and public control over the development of superintelligence.<sup id=\"cite_ref-FOOTNOTEHibbard2002155-163_25-0\" class=\"reference\"><a href=\"#cite_note-FOOTNOTEHibbard2002155-163-25\">&#91;25&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=6\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"div-col columns column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em;\">\n<ul><li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/Artificial_brain\" title=\"Artificial brain\">Artificial brain</a></li>\n<li><a href=\"/wiki/Artificial_intelligence_arms_race\" title=\"Artificial intelligence arms race\">Artificial intelligence arms race</a></li>\n<li><a href=\"/wiki/Effective_altruism#Far_future_and_global_catastrophic_risks\" title=\"Effective altruism\">Effective altruism</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics of artificial intelligence</a></li>\n<li><a href=\"/wiki/Existential_risk\" class=\"mw-redirect\" title=\"Existential risk\">Existential risk</a></li>\n<li><a href=\"/wiki/Future_of_Humanity_Institute\" title=\"Future of Humanity Institute\">Future of Humanity Institute</a></li>\n<li><a href=\"/wiki/Future_of_robotics\" class=\"mw-redirect\" title=\"Future of robotics\">Future of robotics</a></li>\n<li><a href=\"/wiki/Global_catastrophic_risk\" title=\"Global catastrophic risk\">Global catastrophic risk</a></li>\n<li><a href=\"/wiki/Intelligent_agent\" title=\"Intelligent agent\">Intelligent agent</a></li>\n<li><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></li>\n<li><a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a></li>\n<li><a href=\"/wiki/Outline_of_artificial_intelligence\" title=\"Outline of artificial intelligence\">Outline of artificial intelligence</a></li>\n<li><a href=\"/wiki/Posthumanism\" title=\"Posthumanism\">Posthumanism</a></li>\n<li><a href=\"/wiki/Self-replication\" title=\"Self-replication\">Self-replication</a></li>\n<li><a href=\"/wiki/Self-replicating_machine\" title=\"Self-replicating machine\">Self-replicating machine</a></li>\n<li><i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i></li></ul>\n</div>\n<h2><span class=\"mw-headline\" id=\"Citations\">Citations</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=7\" title=\"Edit section: Citations\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist\" style=\"list-style-type: decimal;\">\n<div class=\"mw-references-wrap mw-references-columns\"><ol class=\"references\">\n<li id=\"cite_note-FOOTNOTEBostrom2014Chapter_2-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-FOOTNOTEBostrom2014Chapter_2_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-FOOTNOTEBostrom2014Chapter_2_1-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, Chapter 2.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201422-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201422_2-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, p.&#160;22.</span>\n</li>\n<li id=\"cite_note-FOOTNOTELegg2008135-137-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTELegg2008135-137_3-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFLegg2008\">Legg 2008</a>, pp.&#160;135-137.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEChalmers20107-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEChalmers20107_4-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFChalmers2010\">Chalmers 2010</a>, p.&#160;7.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEChalmers20107-9-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEChalmers20107-9_5-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFChalmers2010\">Chalmers 2010</a>, p.&#160;7-9.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEChalmers201010-11-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEChalmers201010-11_6-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFChalmers2010\">Chalmers 2010</a>, p.&#160;10-11.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEChalmers201011-13-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEChalmers201011-13_7-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFChalmers2010\">Chalmers 2010</a>, p.&#160;11-13.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201459-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201459_8-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, p.&#160;59.</span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation techreport\"><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Yudkowsky, Eliezer</a> (2013). <a rel=\"nofollow\" class=\"external text\" href=\"http://intelligence.org/files/IEM.pdf\"><i>Intelligence Explosion Microeconomics</i></a> <span style=\"font-size:85%;\">(PDF)</span> (Technical report). <a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a>. p.&#160;35. 2013-1.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=report&amp;rft.btitle=Intelligence+Explosion+Microeconomics&amp;rft.pages=35&amp;rft.pub=Machine+Intelligence+Research+Institute&amp;rft.date=2013&amp;rft.aulast=Yudkowsky&amp;rft.aufirst=Eliezer&amp;rft_id=http%3A%2F%2Fintelligence.org%2Ffiles%2FIEM.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201456-57-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201456-57_10-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp.&#160;56-57.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201452,_59-61-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201452,_59-61_11-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp.&#160;52, 59-61.</span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\"><a href=\"/wiki/Carl_Sagan\" title=\"Carl Sagan\">Sagan, Carl</a> (1977). <i><a href=\"/wiki/The_Dragons_of_Eden\" title=\"The Dragons of Eden\">The Dragons of Eden</a></i>. Random House.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Dragons+of+Eden&amp;rft.pub=Random+House&amp;rft.date=1977&amp;rft.aulast=Sagan&amp;rft.aufirst=Carl&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201437-39-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201437-39_13-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp.&#160;37-39.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201439-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201439_14-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, p.&#160;39.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201448-49-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201448-49_15-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp.&#160;48-49.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom201436-37,_42,_47-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom201436-37,_42,_47_16-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp.&#160;36-37, 42, 47.</span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Maker, Meg Houston (July 13, 2006). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20140513052243/http://www.megmaker.com/2006/07/ai50_first_poll.html\">\"AI@50: First Poll\"</a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://www.megmaker.com/2006/07/ai50_first_poll.html\">the original</a> on 2014-05-13.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=AI%4050%3A+First+Poll&amp;rft.date=2006-07-13&amp;rft.aulast=Maker&amp;rft.aufirst=Meg+Houston&amp;rft_id=http%3A%2F%2Fwww.megmaker.com%2F2006%2F07%2Fai50_first_poll.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-FOOTNOTEM\u00fcllerBostrom20163-4,_6,_9-12-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEM\u00fcllerBostrom20163-4,_6,_9-12_18-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFM\u00fcllerBostrom2016\">M\u00fcller &amp; Bostrom 2016</a>, pp.&#160;3-4, 6, 9-12.</span>\n</li>\n<li id=\"cite_note-FOOTNOTEBostrom2014209-221-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEBostrom2014209-221_19-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFBostrom2014\">Bostrom 2014</a>, pp.&#160;209-221.</span>\n</li>\n<li id=\"cite_note-FOOTNOTESantos-Lang201416-19-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTESantos-Lang201416-19_20-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFSantos-Lang2014\">Santos-Lang 2014</a>, pp.&#160;16-19.</span>\n</li>\n<li id=\"cite_note-billjoy-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-billjoy_21-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"/wiki/Bill_Joy\" title=\"Bill Joy\">Bill Joy</a>, <a rel=\"nofollow\" class=\"external text\" href=\"https://www.wired.com/wired/archive/8.04/joy_pr.html\">Why the future doesn't need us</a>. In: <a href=\"/wiki/Wired_magazine\" class=\"mw-redirect\" title=\"Wired magazine\">Wired magazine</a>. See also <a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">technological singularity</a>. <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a> 2002 Ethical Issues in Advanced Artificial Intelligence</span>\n</li>\n<li id=\"cite_note-Muehlhauser,_Luke_2012-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Muehlhauser,_Luke_2012_22-0\">^</a></b></span> <span class=\"reference-text\">Muehlhauser, Luke, and Louie Helm. 2012. \"Intelligence Explosion and Machine Ethics.\" In Singularity Hypotheses: A Scientific and Philosophical Assessment, edited by Amnon Eden, Johnny S\u00f8raker, James H. Moor, and Eric Steinhart. Berlin: Springer.</span>\n</li>\n<li id=\"cite_note-Bostrom,_Nick_2003-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Bostrom,_Nick_2003_23-0\">^</a></b></span> <span class=\"reference-text\">Bostrom, Nick. 2003. \"Ethical Issues in Advanced Artificial Intelligence.\" In Cognitive, Emotive and Ethical Aspects of Decision Making in Humans and in Artificial Intelligence, edited by Iva Smit and George E. Lasker, 12\u201317. Vol. 2. Windsor, ON: International Institute for Advanced Studies in Systems Research / Cybernetics.</span>\n</li>\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a> (2008) in <i><a rel=\"nofollow\" class=\"external text\" href=\"http://intelligence.org/files/AIPosNegFactor.pdf\">Artificial Intelligence as a Positive and Negative Factor in Global Risk</a></i></span>\n</li>\n<li id=\"cite_note-FOOTNOTEHibbard2002155-163-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-FOOTNOTEHibbard2002155-163_25-0\">^</a></b></span> <span class=\"reference-text\"><a href=\"#CITEREFHibbard2002\">Hibbard 2002</a>, pp.&#160;155-163.</span>\n</li>\n</ol></div></div>\n<h2><span class=\"mw-headline\" id=\"Bibliography\">Bibliography</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=8\" title=\"Edit section: Bibliography\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><cite id=\"CITEREFBostrom2002\" class=\"citation\"><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Bostrom, Nick</a> (2002), <a rel=\"nofollow\" class=\"external text\" href=\"http://www.nickbostrom.com/existential/risks.html\">\"Existential Risks\"</a>, <i><a href=\"/wiki/Journal_of_Evolution_and_Technology\" class=\"mw-redirect\" title=\"Journal of Evolution and Technology\">Journal of Evolution and Technology</a></i>, <b>9</b><span class=\"reference-accessdate\">, retrieved <span class=\"nowrap\">2007-08-07</span></span></cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Evolution+and+Technology&amp;rft.atitle=Existential+Risks&amp;rft.volume=9&amp;rft.date=2002&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rft_id=http%3A%2F%2Fwww.nickbostrom.com%2Fexistential%2Frisks.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFBostrom2014\" class=\"citation book\"><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Bostrom, Nick</a> (2014). <i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i>. Oxford University Press.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Superintelligence%3A+Paths%2C+Dangers%2C+Strategies&amp;rft.pub=Oxford+University+Press&amp;rft.date=2014&amp;rft.aulast=Bostrom&amp;rft.aufirst=Nick&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFChalmers2010\" class=\"citation journal\"><a href=\"/wiki/David_Chalmers\" title=\"David Chalmers\">Chalmers, David</a> (2010). <a rel=\"nofollow\" class=\"external text\" href=\"http://consc.net/papers/singularity.pdf\">\"The Singularity: A Philosophical Analysis\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Journal of Consciousness Studies</i>. <b>17</b>: 7\u201365.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Consciousness+Studies&amp;rft.atitle=The+Singularity%3A+A+Philosophical+Analysis&amp;rft.volume=17&amp;rft.pages=7-65&amp;rft.date=2010&amp;rft.aulast=Chalmers&amp;rft.aufirst=David&amp;rft_id=http%3A%2F%2Fconsc.net%2Fpapers%2Fsingularity.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFHibbard2002\" class=\"citation book\"><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Hibbard, Bill</a> (2002). <i><a href=\"/wiki/Super-Intelligent_Machines\" class=\"mw-redirect\" title=\"Super-Intelligent Machines\">Super-Intelligent Machines</a></i>. Kluwer Academic/Plenum Publishers.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Super-Intelligent+Machines&amp;rft.pub=Kluwer+Academic%2FPlenum+Publishers&amp;rft.date=2002&amp;rft.aulast=Hibbard&amp;rft.aufirst=Bill&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFLegg2008\" class=\"citation thesis\">Legg, Shane (2008). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.vetta.org/documents/Machine_Super_Intelligence.pdf\"><i>Machine Super Intelligence</i></a> <span style=\"font-size:85%;\">(PDF)</span> (PhD). Department of Informatics, University of Lugano<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 19,</span> 2014</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Adissertation&amp;rft.title=Machine+Super+Intelligence&amp;rft.inst=Department+of+Informatics%2C+University+of+Lugano&amp;rft.date=2008&amp;rft.aulast=Legg&amp;rft.aufirst=Shane&amp;rft_id=http%3A%2F%2Fwww.vetta.org%2Fdocuments%2FMachine_Super_Intelligence.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFM\u00fcllerBostrom2016\" class=\"citation encyclopaedia\"><a href=\"/wiki/Vincent_C._M%C3%BCller\" title=\"Vincent C. M\u00fcller\">M\u00fcller, Vincent C.</a>; <a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Bostrom, Nick</a> (2016). \"Future Progress in Artificial Intelligence: A Survey of Expert Opinion\".  In M\u00fcller, Vincent C. <a rel=\"nofollow\" class=\"external text\" href=\"http://www.philpapers.org/archive/MLLFPI\"><i>Fundamental Issues of Artificial Intelligence</i></a>. Springer. pp.&#160;553\u2013571.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Future+Progress+in+Artificial+Intelligence%3A+A+Survey+of+Expert+Opinion&amp;rft.btitle=Fundamental+Issues+of+Artificial+Intelligence&amp;rft.pages=553-571&amp;rft.pub=Springer&amp;rft.date=2016&amp;rft.aulast=M%C3%BCller&amp;rft.aufirst=Vincent+C.&amp;rft.au=Bostrom%2C+Nick&amp;rft_id=http%3A%2F%2Fwww.philpapers.org%2Farchive%2FMLLFPI&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFSantos-Lang2014\" class=\"citation journal\">Santos-Lang, Christopher (2014). <a rel=\"nofollow\" class=\"external text\" href=\"http://grinfree.com/Responsibility.pdf\">\"Our responsibility to manage evaluative diversity\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>ACM SIGCAS Computers &amp; Society</i>. <b>44</b> (2): 16\u201319. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1145/2656870.2656874\">10.1145/2656870.2656874</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=ACM+SIGCAS+Computers+%26+Society&amp;rft.atitle=Our+responsibility+to+manage+evaluative+diversity&amp;rft.volume=44&amp;rft.issue=2&amp;rft.pages=16-19&amp;rft.date=2014&amp;rft_id=info%3Adoi%2F10.1145%2F2656870.2656874&amp;rft.aulast=Santos-Lang&amp;rft.aufirst=Christopher&amp;rft_id=http%3A%2F%2Fgrinfree.com%2FResponsibility.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASuperintelligence\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li></ul>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Superintelligence&amp;action=edit&amp;section=9\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><i><a rel=\"nofollow\" class=\"external text\" href=\"http://www.evolutionnews.org/2015/02/bill_gates_join093191.html\">Bill Gates Joins Stephen Hawking in Fears of a Coming Threat from \"Superintelligence\"</a></i></li>\n<li><i><a rel=\"nofollow\" class=\"external text\" href=\"http://reason.com/archives/2014/09/12/will-superintelligent-machines-destroy-h\">Will Superintelligent Machines Destroy Humanity?</a></i></li>\n<li><i><a rel=\"nofollow\" class=\"external text\" href=\"http://www.ign.com/articles/2015/03/25/apple-co-founder-has-sense-of-foreboding-about-artificial-superintelligence\">Apple Co-founder Has Sense of Foreboding About Artificial Superintelligence</a></i></li></ul>\n<div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Risks_from_artificial_intelligence\" style=\"padding:3px\"><table class=\"nowraplinks collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Existential_risk_from_artificial_intelligence\" title=\"Template:Existential risk from artificial intelligence\"><abbr title=\"View this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Existential_risk_from_artificial_intelligence\" title=\"Template talk:Existential risk from artificial intelligence\"><abbr title=\"Discuss this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Template:Existential_risk_from_artificial_intelligence&amp;action=edit\"><abbr title=\"Edit this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">e</abbr></a></li></ul></div><div id=\"Risks_from_artificial_intelligence\" style=\"font-size:114%;margin:0 4em\">Risks from <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a></div></th></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Concepts</th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/AI_box\" title=\"AI box\">AI box</a></li>\n<li><a href=\"/wiki/AI_takeover\" title=\"AI takeover\">AI takeover</a></li>\n<li><a href=\"/wiki/AI_control_problem\" title=\"AI control problem\">Control problem</a></li>\n<li><a href=\"/wiki/Existential_risk_from_artificial_general_intelligence\" title=\"Existential risk from artificial general intelligence\">Existential risk from artificial general intelligence</a></li>\n<li><a href=\"/wiki/Friendly_artificial_intelligence\" title=\"Friendly artificial intelligence\">Friendly artificial intelligence</a></li>\n<li><a href=\"/wiki/Instrumental_convergence\" title=\"Instrumental convergence\">Instrumental convergence</a></li>\n<li><a href=\"/wiki/Intelligence_explosion\" class=\"mw-redirect\" title=\"Intelligence explosion\">Intelligence explosion</a></li>\n<li><a href=\"/wiki/Machine_ethics\" title=\"Machine ethics\">Machine ethics</a></li>\n<li><a class=\"mw-selflink selflink\">Superintelligence</a></li>\n<li><a href=\"/wiki/Technological_singularity\" title=\"Technological singularity\">Technological singularity</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Organizations</th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Allen_Institute_for_Artificial_Intelligence\" title=\"Allen Institute for Artificial Intelligence\">Allen Institute for Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Center_for_Applied_Rationality\" title=\"Center for Applied Rationality\">Center for Applied Rationality</a></li>\n<li><a href=\"/wiki/Centre_for_the_Study_of_Existential_Risk\" title=\"Centre for the Study of Existential Risk\">Centre for the Study of Existential Risk</a></li>\n<li><a href=\"/wiki/Foundational_Questions_Institute\" title=\"Foundational Questions Institute\">Foundational Questions Institute</a></li>\n<li><a href=\"/wiki/Future_of_Humanity_Institute\" title=\"Future of Humanity Institute\">Future of Humanity Institute</a></li>\n<li><a href=\"/wiki/Future_of_Life_Institute\" title=\"Future of Life Institute\">Future of Life Institute</a></li>\n<li><a href=\"/wiki/Humanity%2B\" title=\"Humanity+\">Humanity+</a></li>\n<li><a href=\"/wiki/Institute_for_Ethics_and_Emerging_Technologies\" title=\"Institute for Ethics and Emerging Technologies\">Institute for Ethics and Emerging Technologies</a></li>\n<li><a href=\"/wiki/Leverhulme_Centre_for_the_Future_of_Intelligence\" title=\"Leverhulme Centre for the Future of Intelligence\">Leverhulme Centre for the Future of Intelligence</a></li>\n<li><a href=\"/wiki/Machine_Intelligence_Research_Institute\" title=\"Machine Intelligence Research Institute\">Machine Intelligence Research Institute</a></li>\n<li><a href=\"/wiki/OpenAI\" title=\"OpenAI\">OpenAI</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">People</th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Nick_Bostrom\" title=\"Nick Bostrom\">Nick Bostrom</a></li>\n<li><a href=\"/wiki/Stephen_Hawking\" title=\"Stephen Hawking\">Stephen Hawking</a></li>\n<li><a href=\"/wiki/Bill_Hibbard\" title=\"Bill Hibbard\">Bill Hibbard</a></li>\n<li><a href=\"/wiki/Bill_Joy\" title=\"Bill Joy\">Bill Joy</a></li>\n<li><a href=\"/wiki/Elon_Musk\" title=\"Elon Musk\">Elon Musk</a></li>\n<li><a href=\"/wiki/Steve_Omohundro\" title=\"Steve Omohundro\">Steve Omohundro</a></li>\n<li><a href=\"/wiki/Huw_Price\" title=\"Huw Price\">Huw Price</a></li>\n<li><a href=\"/wiki/Martin_Rees\" title=\"Martin Rees\">Martin Rees</a></li>\n<li><a href=\"/wiki/Stuart_J._Russell\" title=\"Stuart J. Russell\">Stuart J. Russell</a></li>\n<li><a href=\"/wiki/Jaan_Tallinn\" title=\"Jaan Tallinn\">Jaan Tallinn</a></li>\n<li><a href=\"/wiki/Max_Tegmark\" title=\"Max Tegmark\">Max Tegmark</a></li>\n<li><a href=\"/wiki/Frank_Wilczek\" title=\"Frank Wilczek\">Frank Wilczek</a></li>\n<li><a href=\"/wiki/Roman_Yampolskiy\" title=\"Roman Yampolskiy\">Roman Yampolskiy</a></li>\n<li><a href=\"/wiki/Eliezer_Yudkowsky\" title=\"Eliezer Yudkowsky\">Eliezer Yudkowsky</a></li>\n<li><a href=\"/wiki/Sam_Harris\" title=\"Sam Harris\">Sam Harris</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Other</th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Open_Letter_on_Artificial_Intelligence\" title=\"Open Letter on Artificial Intelligence\">Open Letter on Artificial Intelligence</a></li>\n<li><a href=\"/wiki/Ethics_of_artificial_intelligence\" title=\"Ethics of artificial intelligence\">Ethics of artificial intelligence</a></li>\n<li><a href=\"/wiki/Artificial_general_intelligence#Controversies_and_dangers\" title=\"Artificial general intelligence\">Controversies and dangers of artificial general intelligence</a></li>\n<li><a href=\"/wiki/Global_catastrophic_risk#Artificial_intelligence\" title=\"Global catastrophic risk\">Artificial intelligence as a global catastrophic risk</a></li>\n<li><i><a href=\"/wiki/Superintelligence:_Paths,_Dangers,_Strategies\" title=\"Superintelligence: Paths, Dangers, Strategies\">Superintelligence: Paths, Dangers, Strategies</a></i></li>\n<li><i><a href=\"/wiki/Our_Final_Invention\" title=\"Our Final Invention\">Our Final Invention</a></i></li></ul>\n</div></td></tr></tbody></table></div>\n\n<!-- \nNewPP limit report\nParsed by mw2181\nCached time: 20180915022552\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.232 seconds\nReal time usage: 0.285 seconds\nPreprocessor visited node count: 1142/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 29418/2097152 bytes\nTemplate argument size: 655/2097152 bytes\nHighest expansion depth: 7/40\nExpensive parser function count: 0/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 12655/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.099/10.000 seconds\nLua memory usage: 3.85 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  212.567      1 -total\n 27.53%   58.510      1 Template:Reflist\n 17.46%   37.112     19 Template:Sfn\n 13.83%   29.390      1 Template:Cite_techreport\n  9.83%   20.906      1 Template:For\n  6.26%   13.299      3 Template:Cite_book\n  5.68%   12.067      2 Template:Cite_journal\n  5.45%   11.580      1 Template:Existential_risk_from_artificial_intelligence\n  5.39%   11.461      1 Template:Div_col\n  5.01%   10.644      1 Template:Quote\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:726659-0!canonical and timestamp 20180915022552 and revision id 851397514\n -->\n</div>"},"langlinks":[{"lang":"de","url":"https://de.wikipedia.org/wiki/Superintelligenz","langname":"German","autonym":"Deutsch","*":"Superintelligenz"},{"lang":"el","url":"https://el.wikipedia.org/wiki/%CE%A5%CF%80%CE%B5%CF%81%CE%B5%CF%85%CF%86%CF%85%CE%90%CE%B1","langname":"Greek","autonym":"\u0395\u03bb\u03bb\u03b7\u03bd\u03b9\u03ba\u03ac","*":"\u03a5\u03c0\u03b5\u03c1\u03b5\u03c5\u03c6\u03c5\u0390\u03b1"},{"lang":"es","url":"https://es.wikipedia.org/wiki/Superinteligencia","langname":"Spanish","autonym":"espa\u00f1ol","*":"Superinteligencia"},{"lang":"fa","url":"https://fa.wikipedia.org/wiki/%D9%81%D8%B1%D8%A7%D9%87%D9%88%D8%B4","langname":"Persian","autonym":"\u0641\u0627\u0631\u0633\u06cc","*":"\u0641\u0631\u0627\u0647\u0648\u0634"},{"lang":"fr","url":"https://fr.wikipedia.org/wiki/Superintelligence","langname":"French","autonym":"fran\u00e7ais","*":"Superintelligence"},{"lang":"nl","url":"https://nl.wikipedia.org/wiki/Superintelligentie","langname":"Dutch","autonym":"Nederlands","*":"Superintelligentie"},{"lang":"ja","url":"https://ja.wikipedia.org/wiki/%E8%B6%85%E7%9F%A5%E8%83%BD","langname":"Japanese","autonym":"\u65e5\u672c\u8a9e","*":"\u8d85\u77e5\u80fd"},{"lang":"pl","url":"https://pl.wikipedia.org/wiki/Superinteligencja","langname":"Polish","autonym":"polski","*":"Superinteligencja"},{"lang":"pt","url":"https://pt.wikipedia.org/wiki/Superintelig%C3%AAncia","langname":"Portuguese","autonym":"portugu\u00eas","*":"Superintelig\u00eancia"},{"lang":"ro","url":"https://ro.wikipedia.org/wiki/Superinteligen%C8%9B%C4%83","langname":"Romanian","autonym":"rom\u00e2n\u0103","*":"Superinteligen\u021b\u0103"},{"lang":"sv","url":"https://sv.wikipedia.org/wiki/Superintelligens_(AI)","langname":"Swedish","autonym":"svenska","*":"Superintelligens (AI)"},{"lang":"uk","url":"https://uk.wikipedia.org/wiki/%D0%A1%D1%83%D0%BF%D0%B5%D1%80%D1%96%D0%BD%D1%82%D0%B5%D0%BB%D0%B5%D0%BA%D1%82","langname":"Ukrainian","autonym":"\u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430","*":"\u0421\u0443\u043f\u0435\u0440\u0456\u043d\u0442\u0435\u043b\u0435\u043a\u0442"},{"lang":"vi","url":"https://vi.wikipedia.org/wiki/Si%C3%AAu_tr%C3%AD_th%C3%B4ng_minh","langname":"Vietnamese","autonym":"Ti\u1ebfng Vi\u1ec7t","*":"Si\u00eau tr\u00ed th\u00f4ng minh"}],"categories":[{"sortkey":"","*":"Futurology"},{"sortkey":"","*":"Singularitarianism"},{"sortkey":"","*":"Intelligence"},{"sortkey":"","*":"Artificial_Intelligence_existential_risk"}],"links":[{"ns":10,"exists":"","*":"Template:Existential risk from artificial intelligence"},{"ns":0,"exists":"","*":"AI-complete"},{"ns":0,"exists":"","*":"AI@50"},{"ns":0,"exists":"","*":"AI box"},{"ns":0,"exists":"","*":"AI control problem"},{"ns":0,"exists":"","*":"AI takeover"},{"ns":0,"exists":"","*":"Adaptive evolution in the human genome"},{"ns":0,"exists":"","*":"Allen Institute for Artificial Intelligence"},{"ns":0,"exists":"","*":"Artificial brain"},{"ns":0,"exists":"","*":"Artificial general intelligence"},{"ns":0,"exists":"","*":"Artificial intelligence"},{"ns":0,"exists":"","*":"Artificial intelligence arms race"},{"ns":0,"exists":"","*":"Axon"},{"ns":0,"exists":"","*":"Bill Hibbard"},{"ns":0,"exists":"","*":"Bill Joy"},{"ns":0,"exists":"","*":"Brain\u2013computer interface"},{"ns":0,"exists":"","*":"Caesarean section"},{"ns":0,"exists":"","*":"Carl Sagan"},{"ns":0,"exists":"","*":"Center for Applied Rationality"},{"ns":0,"exists":"","*":"Centre for the Study of Existential Risk"},{"ns":0,"exists":"","*":"Chimpanzee"},{"ns":0,"exists":"","*":"Chinese room"},{"ns":0,"exists":"","*":"Collective intelligence"},{"ns":0,"exists":"","*":"Cyborg"},{"ns":0,"exists":"","*":"David Chalmers"},{"ns":0,"exists":"","*":"Digital object identifier"},{"ns":0,"exists":"","*":"Effective altruism"},{"ns":0,"exists":"","*":"Eliezer Yudkowsky"},{"ns":0,"exists":"","*":"Elon Musk"},{"ns":0,"exists":"","*":"Epigenetics"},{"ns":0,"exists":"","*":"Ethics of artificial intelligence"},{"ns":0,"exists":"","*":"Evolution of human intelligence"},{"ns":0,"exists":"","*":"Evolutionary algorithm"},{"ns":0,"exists":"","*":"Existential risk"},{"ns":0,"exists":"","*":"Existential risk from artificial general intelligence"},{"ns":0,"exists":"","*":"Fertility and Intelligence"},{"ns":0,"exists":"","*":"Foundational Questions Institute"},{"ns":0,"exists":"","*":"Frank Wilczek"},{"ns":0,"exists":"","*":"Friendly artificial intelligence"},{"ns":0,"exists":"","*":"Fritz (chess)"},{"ns":0,"exists":"","*":"Future of Humanity Institute"},{"ns":0,"exists":"","*":"Future of Life Institute"},{"ns":0,"exists":"","*":"Future of robotics"},{"ns":0,"exists":"","*":"Futures studies"},{"ns":0,"exists":"","*":"Gene therapy"},{"ns":0,"exists":"","*":"Genetic engineering"},{"ns":0,"exists":"","*":"Genius"},{"ns":0,"exists":"","*":"Gerald Crabtree"},{"ns":0,"exists":"","*":"Global brain"},{"ns":0,"exists":"","*":"Global catastrophic risk"},{"ns":0,"exists":"","*":"Global catastrophic risks"},{"ns":0,"exists":"","*":"Great ape language"},{"ns":0,"exists":"","*":"Hard problem of consciousness"},{"ns":0,"exists":"","*":"Human intelligence"},{"ns":0,"exists":"","*":"Human multitasking"},{"ns":0,"exists":"","*":"Humanity+"},{"ns":0,"exists":"","*":"Huw Price"},{"ns":0,"exists":"","*":"In vitro fertilisation"},{"ns":0,"exists":"","*":"Institute for Ethics and Emerging Technologies"},{"ns":0,"exists":"","*":"Instrumental convergence"},{"ns":0,"exists":"","*":"Intellectual giftedness"},{"ns":0,"exists":"","*":"Intelligence"},{"ns":0,"exists":"","*":"Intelligence amplification"},{"ns":0,"exists":"","*":"Intelligence explosion"},{"ns":0,"exists":"","*":"Intelligent agent"},{"ns":0,"exists":"","*":"Intentionality"},{"ns":0,"exists":"","*":"Jaan Tallinn"},{"ns":0,"exists":"","*":"Journal of Evolution and Technology"},{"ns":0,"exists":"","*":"Leverhulme Centre for the Future of Intelligence"},{"ns":0,"exists":"","*":"Machine Intelligence Research Institute"},{"ns":0,"exists":"","*":"Machine ethics"},{"ns":0,"exists":"","*":"Machine learning"},{"ns":0,"exists":"","*":"Martin Rees"},{"ns":0,"exists":"","*":"Max Tegmark"},{"ns":0,"exists":"","*":"Mind uploading"},{"ns":0,"exists":"","*":"Monoamine oxidase inhibitor"},{"ns":0,"exists":"","*":"NSI-189"},{"ns":0,"exists":"","*":"Natural selection"},{"ns":0,"exists":"","*":"Neuroenhancement"},{"ns":0,"exists":"","*":"Neuron"},{"ns":0,"exists":"","*":"Nick Bostrom"},{"ns":0,"exists":"","*":"Nootropics"},{"ns":0,"exists":"","*":"OpenAI"},{"ns":0,"exists":"","*":"Open Letter on Artificial Intelligence"},{"ns":0,"exists":"","*":"Our Final Invention"},{"ns":0,"exists":"","*":"Outline of artificial intelligence"},{"ns":0,"exists":"","*":"Perfect recall"},{"ns":0,"exists":"","*":"Posthumanism"},{"ns":0,"exists":"","*":"Primate cognition"},{"ns":0,"exists":"","*":"Qualia"},{"ns":0,"exists":"","*":"Robot"},{"ns":0,"exists":"","*":"Roman Yampolskiy"},{"ns":0,"exists":"","*":"Sam Harris"},{"ns":0,"exists":"","*":"Selective breeding"},{"ns":0,"exists":"","*":"Self-replicating machine"},{"ns":0,"exists":"","*":"Self-replication"},{"ns":0,"exists":"","*":"Species"},{"ns":0,"exists":"","*":"Stephen Hawking"},{"ns":0,"exists":"","*":"Steve Omohundro"},{"ns":0,"exists":"","*":"Stuart J. Russell"},{"ns":0,"exists":"","*":"Super-Intelligent Machines"},{"ns":0,"exists":"","*":"Supercomputer"},{"ns":0,"exists":"","*":"Superintelligence: Paths, Dangers, Strategies"},{"ns":0,"exists":"","*":"Superorganism"},{"ns":0,"exists":"","*":"Technological singularity"},{"ns":0,"exists":"","*":"The Dragons of Eden"},{"ns":0,"exists":"","*":"Unintended consequences"},{"ns":0,"exists":"","*":"University of Oxford"},{"ns":0,"exists":"","*":"Vincent C. M\u00fcller"},{"ns":0,"exists":"","*":"Wired magazine"},{"ns":11,"exists":"","*":"Template talk:Existential risk from artificial intelligence"}],"templates":[{"ns":10,"exists":"","*":"Template:For"},{"ns":10,"exists":"","*":"Template:Sfn"},{"ns":10,"exists":"","*":"Template:Main"},{"ns":10,"exists":"","*":"Template:Further"},{"ns":10,"exists":"","*":"Template:Harvtxt"},{"ns":10,"exists":"","*":"Template:Harvard citation text"},{"ns":10,"exists":"","*":"Template:Quote"},{"ns":10,"exists":"","*":"Template:Quote/styles.css"},{"ns":10,"exists":"","*":"Template:Div col"},{"ns":10,"exists":"","*":"Template:Column-width"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":10,"exists":"","*":"Template:Div col end"},{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Cite techreport"},{"ns":10,"exists":"","*":"Template:Cite book"},{"ns":10,"exists":"","*":"Template:Cite web"},{"ns":10,"exists":"","*":"Template:Citation"},{"ns":10,"exists":"","*":"Template:Cite journal"},{"ns":10,"exists":"","*":"Template:Cite thesis"},{"ns":10,"exists":"","*":"Template:Cite encyclopedia"},{"ns":10,"exists":"","*":"Template:Existential risk from artificial intelligence"},{"ns":10,"exists":"","*":"Template:Navbox"},{"ns":828,"exists":"","*":"Module:For"},{"ns":828,"exists":"","*":"Module:Hatnote list"},{"ns":828,"exists":"","*":"Module:Hatnote"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Footnotes"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:Main"},{"ns":828,"exists":"","*":"Module:Labelled list hatnote"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"},{"ns":828,"exists":"","*":"Module:Citation/CS1"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Configuration"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Whitelist"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Utilities"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Date validation"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Identifiers"},{"ns":828,"exists":"","*":"Module:Citation/CS1/COinS"},{"ns":828,"exists":"","*":"Module:Navbox"},{"ns":828,"exists":"","*":"Module:Navbar"}],"images":["Classification_of_images_progress_human.png"],"externallinks":["http://intelligence.org/files/IEM.pdf","https://web.archive.org/web/20140513052243/http://www.megmaker.com/2006/07/ai50_first_poll.html","http://www.megmaker.com/2006/07/ai50_first_poll.html","https://www.wired.com/wired/archive/8.04/joy_pr.html","http://intelligence.org/files/AIPosNegFactor.pdf","http://www.nickbostrom.com/existential/risks.html","http://consc.net/papers/singularity.pdf","http://www.vetta.org/documents/Machine_Super_Intelligence.pdf","http://www.philpapers.org/archive/MLLFPI","http://grinfree.com/Responsibility.pdf","//doi.org/10.1145/2656870.2656874","http://www.evolutionnews.org/2015/02/bill_gates_join093191.html","http://reason.com/archives/2014/09/12/will-superintelligent-machines-destroy-h","http://www.ign.com/articles/2015/03/25/apple-co-founder-has-sense-of-foreboding-about-artificial-superintelligence"],"sections":[{"toclevel":1,"level":"2","line":"Feasibility of artificial superintelligence","number":"1","index":"1","fromtitle":"Superintelligence","byteoffset":3047,"anchor":"Feasibility_of_artificial_superintelligence"},{"toclevel":1,"level":"2","line":"Feasibility of biological superintelligence","number":"2","index":"2","fromtitle":"Superintelligence","byteoffset":7561,"anchor":"Feasibility_of_biological_superintelligence"},{"toclevel":1,"level":"2","line":"Forecasts","number":"3","index":"3","fromtitle":"Superintelligence","byteoffset":10542,"anchor":"Forecasts"},{"toclevel":1,"level":"2","line":"Design considerations","number":"4","index":"4","fromtitle":"Superintelligence","byteoffset":12229,"anchor":"Design_considerations"},{"toclevel":1,"level":"2","line":"Danger to human survival and the AI control problem","number":"5","index":"5","fromtitle":"Superintelligence","byteoffset":12918,"anchor":"Danger_to_human_survival_and_the_AI_control_problem"},{"toclevel":1,"level":"2","line":"See also","number":"6","index":"6","fromtitle":"Superintelligence","byteoffset":16445,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"Citations","number":"7","index":"7","fromtitle":"Superintelligence","byteoffset":17091,"anchor":"Citations"},{"toclevel":1,"level":"2","line":"Bibliography","number":"8","index":"8","fromtitle":"Superintelligence","byteoffset":17118,"anchor":"Bibliography"},{"toclevel":1,"level":"2","line":"External links","number":"9","index":"9","fromtitle":"Superintelligence","byteoffset":19110,"anchor":"External_links"}],"parsewarnings":[],"displaytitle":"Superintelligence","iwlinks":[],"properties":[{"name":"wikibase_item","*":"Q1566000"}]}}