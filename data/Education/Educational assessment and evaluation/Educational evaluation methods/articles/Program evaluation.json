{"parse":{"title":"Program evaluation","pageid":619350,"revid":859258119,"text":{"*":"<div class=\"mw-parser-output\"><p><b>Program evaluation</b> is a systematic method for collecting, analyzing, and using information to answer questions about projects, policies and <a href=\"/wiki/Program_(management)\" class=\"mw-redirect\" title=\"Program (management)\">programs</a>,<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup> particularly about their effectiveness and efficiency.  In both the public and private sectors, <a href=\"/wiki/Project_stakeholder\" title=\"Project stakeholder\">stakeholders</a> often want to know whether the programs they are funding, implementing, voting for, receiving or objecting to are producing the intended effect. While <i>program evaluation</i> first focuses around this definition, important considerations often include how much the program costs per participant, how the program could be improved, whether the program is worthwhile, whether there are better alternatives, if there are <i>unintended</i> outcomes, and whether the program goals are appropriate and useful.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup> Evaluators help to answer these questions, but the best way to answer the questions is for the evaluation to be a joint project between evaluators and stakeholders.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup>\n</p><p>The process of evaluation is considered to be a relatively recent phenomenon. However, planned social evaluation has been documented as dating as far back as 2200 BC.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\">&#91;4&#93;</a></sup>  Evaluation became particularly relevant in the U.S. in the 1960s during the period of the <a href=\"/wiki/Great_Society\" title=\"Great Society\">Great Society</a> social programs associated with the <a href=\"/wiki/John_F._Kennedy\" title=\"John F. Kennedy\">Kennedy</a> and <a href=\"/wiki/Lyndon_Johnson\" class=\"mw-redirect\" title=\"Lyndon Johnson\">Johnson</a> administrations.<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\">&#91;5&#93;</a></sup><sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup> Extraordinary sums were invested in social programs, but the impacts of these investments were largely unknown.\n</p><p>Program evaluations can involve both <a href=\"/wiki/Quantitative_method\" class=\"mw-redirect\" title=\"Quantitative method\">quantitative</a> and <a href=\"/wiki/Qualitative_method\" class=\"mw-redirect\" title=\"Qualitative method\">qualitative methods</a> of <a href=\"/wiki/Social_research\" title=\"Social research\">social research</a>. People who do program evaluation come from many different backgrounds, such as <a href=\"/wiki/Sociology\" title=\"Sociology\">sociology</a>, <a href=\"/wiki/Psychology\" title=\"Psychology\">psychology</a>, <a href=\"/wiki/Economics\" title=\"Economics\">economics</a>, <a href=\"/wiki/Social_work\" title=\"Social work\">social work</a>, and <a href=\"/wiki/Public_policy\" title=\"Public policy\">public policy</a>. Some graduate schools also have specific training programs for program evaluation.\n</p>\n<div class=\"toclimit-3\"><div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Doing_an_evaluation\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Doing an evaluation</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-2\"><a href=\"#Assessing_needs\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\">Assessing needs</span></a></li>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"#Assessing_program_theory\"><span class=\"tocnumber\">1.2</span> <span class=\"toctext\">Assessing program theory</span></a></li>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Assessing_implementation\"><span class=\"tocnumber\">1.3</span> <span class=\"toctext\">Assessing implementation</span></a></li>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"#Assessing_the_impact_(effectiveness)\"><span class=\"tocnumber\">1.4</span> <span class=\"toctext\">Assessing the impact (effectiveness)</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-6\"><a href=\"#Program_Outcomes\"><span class=\"tocnumber\">1.4.1</span> <span class=\"toctext\">Program Outcomes</span></a></li>\n<li class=\"toclevel-3 tocsection-7\"><a href=\"#Measuring_Program_Outcomes\"><span class=\"tocnumber\">1.4.2</span> <span class=\"toctext\">Measuring Program Outcomes</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Assessing_efficiency\"><span class=\"tocnumber\">1.5</span> <span class=\"toctext\">Assessing efficiency</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-9\"><a href=\"#Determining_causation\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Determining causation</span></a></li>\n<li class=\"toclevel-1 tocsection-10\"><a href=\"#Reliability,_validity_and_sensitivity_in_program_evaluation\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Reliability, validity and sensitivity in program evaluation</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-11\"><a href=\"#Reliability\"><span class=\"tocnumber\">3.1</span> <span class=\"toctext\">Reliability</span></a></li>\n<li class=\"toclevel-2 tocsection-12\"><a href=\"#Validity\"><span class=\"tocnumber\">3.2</span> <span class=\"toctext\">Validity</span></a></li>\n<li class=\"toclevel-2 tocsection-13\"><a href=\"#Sensitivity\"><span class=\"tocnumber\">3.3</span> <span class=\"toctext\">Sensitivity</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-14\"><a href=\"#Evaluating_Collective_Impact\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Evaluating Collective Impact</span></a></li>\n<li class=\"toclevel-1 tocsection-15\"><a href=\"#Planning_a_program_evaluation\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Planning a program evaluation</span></a></li>\n<li class=\"toclevel-1 tocsection-16\"><a href=\"#Methodological_constraints_and_challenges\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Methodological constraints and challenges</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-17\"><a href=\"#The_shoestring_approach\"><span class=\"tocnumber\">6.1</span> <span class=\"toctext\">The shoestring approach</span></a></li>\n<li class=\"toclevel-2 tocsection-18\"><a href=\"#Budget_constraints\"><span class=\"tocnumber\">6.2</span> <span class=\"toctext\">Budget constraints</span></a></li>\n<li class=\"toclevel-2 tocsection-19\"><a href=\"#Time_constraints\"><span class=\"tocnumber\">6.3</span> <span class=\"toctext\">Time constraints</span></a></li>\n<li class=\"toclevel-2 tocsection-20\"><a href=\"#Data_constraints\"><span class=\"tocnumber\">6.4</span> <span class=\"toctext\">Data constraints</span></a></li>\n<li class=\"toclevel-2 tocsection-21\"><a href=\"#Five-tiered_approach\"><span class=\"tocnumber\">6.5</span> <span class=\"toctext\">Five-tiered approach</span></a></li>\n<li class=\"toclevel-2 tocsection-22\"><a href=\"#Methodological_challenges_presented_by_language_and_culture\"><span class=\"tocnumber\">6.6</span> <span class=\"toctext\">Methodological challenges presented by language and culture</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-23\"><a href=\"#Utilization_results\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Utilization results</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-24\"><a href=\"#Persuasive_utilization\"><span class=\"tocnumber\">7.1</span> <span class=\"toctext\">Persuasive utilization</span></a></li>\n<li class=\"toclevel-2 tocsection-25\"><a href=\"#Direct_(instrumental)_utilization\"><span class=\"tocnumber\">7.2</span> <span class=\"toctext\">Direct (instrumental) utilization</span></a></li>\n<li class=\"toclevel-2 tocsection-26\"><a href=\"#Conceptual_utilization\"><span class=\"tocnumber\">7.3</span> <span class=\"toctext\">Conceptual utilization</span></a></li>\n<li class=\"toclevel-2 tocsection-27\"><a href=\"#Variables_affecting_utilization\"><span class=\"tocnumber\">7.4</span> <span class=\"toctext\">Variables affecting utilization</span></a></li>\n<li class=\"toclevel-2 tocsection-28\"><a href=\"#Guidelines_for_maximizing_utilization\"><span class=\"tocnumber\">7.5</span> <span class=\"toctext\">Guidelines for maximizing utilization</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-29\"><a href=\"#Internal_versus_external_program_evaluators\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">Internal versus external program evaluators</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-30\"><a href=\"#Internal_evaluators\"><span class=\"tocnumber\">8.1</span> <span class=\"toctext\">Internal evaluators</span></a></li>\n<li class=\"toclevel-2 tocsection-31\"><a href=\"#External_evaluators\"><span class=\"tocnumber\">8.2</span> <span class=\"toctext\">External evaluators</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-32\"><a href=\"#Three_paradigms\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">Three paradigms</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-33\"><a href=\"#Positivist\"><span class=\"tocnumber\">9.1</span> <span class=\"toctext\">Positivist</span></a></li>\n<li class=\"toclevel-2 tocsection-34\"><a href=\"#Interpretive\"><span class=\"tocnumber\">9.2</span> <span class=\"toctext\">Interpretive</span></a></li>\n<li class=\"toclevel-2 tocsection-35\"><a href=\"#Critical-emancipatory\"><span class=\"tocnumber\">9.3</span> <span class=\"toctext\">Critical-emancipatory</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-36\"><a href=\"#Empowerment_evaluation\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">Empowerment evaluation</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-37\"><a href=\"#Establishing_a_mission\"><span class=\"tocnumber\">10.1</span> <span class=\"toctext\">Establishing a mission</span></a></li>\n<li class=\"toclevel-2 tocsection-38\"><a href=\"#Taking_stock\"><span class=\"tocnumber\">10.2</span> <span class=\"toctext\">Taking stock</span></a></li>\n<li class=\"toclevel-2 tocsection-39\"><a href=\"#Planning_for_the_future\"><span class=\"tocnumber\">10.3</span> <span class=\"toctext\">Planning for the future</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-40\"><a href=\"#Transformative_Paradigm\"><span class=\"tocnumber\">11</span> <span class=\"toctext\">Transformative Paradigm</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-41\"><a href=\"#Paradigms\"><span class=\"tocnumber\">11.1</span> <span class=\"toctext\">Paradigms</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-42\"><a href=\"#Axiology_(Values_and_Value_Judgements)\"><span class=\"tocnumber\">11.1.1</span> <span class=\"toctext\">Axiology (Values and Value Judgements)</span></a></li>\n<li class=\"toclevel-3 tocsection-43\"><a href=\"#Ontology_(Reality)\"><span class=\"tocnumber\">11.1.2</span> <span class=\"toctext\">Ontology (Reality)</span></a></li>\n<li class=\"toclevel-3 tocsection-44\"><a href=\"#Epistemology_(Knowledge)\"><span class=\"tocnumber\">11.1.3</span> <span class=\"toctext\">Epistemology (Knowledge)</span></a></li>\n<li class=\"toclevel-3 tocsection-45\"><a href=\"#Methodology_(Systematic_Inquiry)\"><span class=\"tocnumber\">11.1.4</span> <span class=\"toctext\">Methodology (Systematic Inquiry)</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-2 tocsection-46\"><a href=\"#Lenses\"><span class=\"tocnumber\">11.2</span> <span class=\"toctext\">Lenses</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-47\"><a href=\"#Critical_Race_Theory\"><span class=\"tocnumber\">11.2.1</span> <span class=\"toctext\">Critical Race Theory</span></a></li>\n<li class=\"toclevel-3 tocsection-48\"><a href=\"#Feminist_Theory\"><span class=\"tocnumber\">11.2.2</span> <span class=\"toctext\">Feminist Theory</span></a></li>\n<li class=\"toclevel-3 tocsection-49\"><a href=\"#Queer/LGBTQ_Theory\"><span class=\"tocnumber\">11.2.3</span> <span class=\"toctext\">Queer/LGBTQ Theory</span></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-50\"><a href=\"#Government_requirements\"><span class=\"tocnumber\">12</span> <span class=\"toctext\">Government requirements</span></a></li>\n<li class=\"toclevel-1 tocsection-51\"><a href=\"#Types_of_Evaluation\"><span class=\"tocnumber\">13</span> <span class=\"toctext\">Types of Evaluation</span></a></li>\n<li class=\"toclevel-1 tocsection-52\"><a href=\"#CIPP_Model_of_evaluation\"><span class=\"tocnumber\">14</span> <span class=\"toctext\">CIPP Model of evaluation</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-53\"><a href=\"#History_of_the_CIPP_model\"><span class=\"tocnumber\">14.1</span> <span class=\"toctext\">History of the CIPP model</span></a></li>\n<li class=\"toclevel-2 tocsection-54\"><a href=\"#CIPP_model\"><span class=\"tocnumber\">14.2</span> <span class=\"toctext\">CIPP model</span></a></li>\n<li class=\"toclevel-2 tocsection-55\"><a href=\"#Four_aspects_of_CIPP_evaluation\"><span class=\"tocnumber\">14.3</span> <span class=\"toctext\">Four aspects of CIPP evaluation</span></a></li>\n<li class=\"toclevel-2 tocsection-56\"><a href=\"#Using_CIPP_in_the_different_stages_of_the_evaluation\"><span class=\"tocnumber\">14.4</span> <span class=\"toctext\">Using CIPP in the different stages of the evaluation</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-57\"><a href=\"#See_also\"><span class=\"tocnumber\">15</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-58\"><a href=\"#References\"><span class=\"tocnumber\">16</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-59\"><a href=\"#Further_reading\"><span class=\"tocnumber\">17</span> <span class=\"toctext\">Further reading</span></a></li>\n<li class=\"toclevel-1 tocsection-60\"><a href=\"#External_links\"><span class=\"tocnumber\">18</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n</div>\n<h2><span class=\"mw-headline\" id=\"Doing_an_evaluation\">Doing an evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=1\" title=\"Edit section: Doing an evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Program evaluation may be conducted at several stages during a program's lifetime. Each of these stages raises different questions to be answered by the evaluator, and correspondingly different evaluation approaches are needed. Rossi, Lipsey and Freeman (2004) suggest the following kinds of assessment, which may be appropriate at these different stages:\n</p>\n<ul><li>Assessment of the need for the program</li>\n<li>Assessment of program design and logic/theory</li>\n<li>Assessment of how the program is being implemented (i.e., is it being implemented according to plan? Are the program's processes maximizing possible outcomes?)</li>\n<li>Assessment of the program's outcome or impact (i.e., what it has actually achieved)</li>\n<li>Assessment of the program's cost and efficiency</li></ul>\n<h3><span class=\"mw-headline\" id=\"Assessing_needs\">Assessing needs</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=2\" title=\"Edit section: Assessing needs\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>A needs assessment examines the population that the program intends to target, to see whether the need as conceptualized in the program actually exists in the population; whether it is, in fact, a problem; and if so, how it might best be dealt with. This includes identifying and diagnosing the actual problem the program is trying to address, who or what is affected by the problem, how widespread the problem is, and what are the measurable effects that are caused by the problem. For example, for a housing program aimed at mitigating homelessness, a program evaluator may want to find out how many people are homeless in a given geographic area and what their demographics are. Rossi, Lipsey and Freeman (2004) caution against undertaking an intervention without properly assessing the need for one, because this might result in a great deal of wasted funds if the need did not exist or was misconceived.\n</p><p>Needs assessment involves the processes or methods used by evaluators to describe and diagnose social needs<sup id=\"cite_ref-Rossi_7-0\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\nThis is essential for evaluators because they need to identify whether programs are effective and they cannot do this unless they have identified what the problem/need is. Programs that do not do a needs assessment can have the illusion that they have eradicated the problem/need when in fact there was no need in the first place. Needs assessment involves research and regular consultation with community stakeholders and with the people that will benefit from the project before the program can be developed and implemented. Hence it should be a bottom-up approach. In this way potential problems can be realized early because the process would have involved the community in identifying the need and thereby allowed the opportunity to identify potential barriers.\n</p><p>The important task of a program evaluator is thus to:\nFirst, construct a precise definition of what the problem is.<sup id=\"cite_ref-Rossi_7-1\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> Evaluators need to first identify the problem/need. This is most effectively done by collaboratively including all possible stakeholders, i.e., the community impacted by the potential problem, the agents/actors working to address and resolve the problem, funders, etc.  Including buy-in early on in the process reduces potential for push-back, miscommunication, and incomplete information later on.\n</p><p>Second, assess the extent of the problem.<sup id=\"cite_ref-Rossi_7-2\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\nHaving clearly identified what the problem is, evaluators need to then assess the extent of the problem. They need to answer the \u2018where\u2019 and \u2018how big\u2019 questions. Evaluators need to work out where the problem is located and how big it is. Pointing out that a problem exists is much easier than having to specify where it is located and how rife it is. Rossi, Lipsey &amp; Freeman (2004) gave an example that: a person identifying some battered children may be enough evidence to persuade one that child abuse exists. But indicating how many children it affects and where it is located geographically and socially would require knowledge about abused children, the characteristics of perpetrators and the impact of the problem throughout the political authority in question.\n</p><p>This can be difficult considering that child abuse is not a public behavior, also keeping in mind that estimates of the rates on private behavior are usually not possible because of factors like unreported cases. In this case evaluators would have to use data from several sources and apply different approaches in order to estimate incidence rates. There are two more questions that need to be answered:<sup id=\"cite_ref-Barbazette_8-0\" class=\"reference\"><a href=\"#cite_note-Barbazette-8\">&#91;8&#93;</a></sup>\nEvaluators need to also answer the \u2019how\u2019 and \u2018what\u2019 questions<sup id=\"cite_ref-Barbazette_8-1\" class=\"reference\"><a href=\"#cite_note-Barbazette-8\">&#91;8&#93;</a></sup> The \u2018how\u2019 question requires that evaluators determine how the need will be addressed. Having identified the need and having familiarized oneself with the community evaluators should conduct a performance analysis to identify whether the proposed plan in the program will actually be able to eliminate the need. The \u2018what\u2019 question requires that evaluators conduct a task analysis to find out what the best way to perform would be. For example, whether the job performance standards are set by an organization or whether some governmental rules need to be considered when undertaking the task.<sup id=\"cite_ref-Barbazette_8-2\" class=\"reference\"><a href=\"#cite_note-Barbazette-8\">&#91;8&#93;</a></sup>\n</p><p>Third, define and identify the target of interventions and accurately describe the nature of the service needs of that population<sup id=\"cite_ref-Rossi_7-3\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\nIt is important to know what/who the target population is/are \u2013 it might be individuals, groups, communities, etc. There are three units of the population: population at risk, population in need and population in demand<sup id=\"cite_ref-Rossi_7-4\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<ul><li>Population  at risk: are people with a significant probability of developing the risk e.g. the population at risk for birth control programs are women of child-bearing age.</li>\n<li>Population in need: are people with the condition that the program seeks to address; e.g. the population in need for a program that aims to provide ARV\u2019s to HIV positive people are people that are HIV positive.</li>\n<li>Population in demand: that part of the population in need that agrees to be having the need and are willing to take part in what the program has to offer e.g. not all HIV positive people will be willing to take ARV\u2019s.</li></ul>\n<p>Being able to specify what/who the target is will assist in establishing appropriate boundaries, so that interventions can correctly address the target population and be feasible to apply&lt;<sup id=\"cite_ref-Rossi_7-5\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p><p>There are four steps in conducting a needs assessment:<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup>\n</p>\n<ol><li>Perform a \u2018gap\u2019 analyses\n<dl><dd>Evaluators need to compare current situation to the desired or necessary situation. The difference or the gap between the two situations will help identify the need, purpose and aims of the program.</dd></dl></li>\n<li>Identify priorities and importance\n<dl><dd>In the first step above, evaluators would have identified a number of interventions that could potentially address the need e.g. training and development, organization development etc. These must now be examined in view of their significance to the program\u2019s goals and constraints. This must be done by considering the following factors: cost effectiveness (consider the budget of the program, assess cost/benefit ratio), executive pressure (whether top management expects a solution) and population (whether many key people are involved).</dd></dl></li>\n<li>Identify causes of performance problems and/or opportunities\n<dl><dd>When the needs have been prioritized the next step is to identify specific problem areas within the need to be addressed. And to also assess the skills of the people that will be carrying out the interventions.</dd></dl></li>\n<li>Identify possible solutions and growth opportunities\n<dl><dd>Compare the consequences of the interventions if it was to be implemented or not.</dd></dl></li></ol>\n<p>Needs analysis is hence a very crucial step in evaluating programs because the effectiveness of a program cannot be assessed unless we know what the problem was in the first place.\n</p>\n<h3><span class=\"mw-headline\" id=\"Assessing_program_theory\">Assessing program theory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=3\" title=\"Edit section: Assessing program theory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The program theory, also called a <a href=\"/wiki/Logic_model\" title=\"Logic model\">logic model</a> or impact pathway,<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\">&#91;10&#93;</a></sup> is an assumption, implicit in the way the program is designed, about how the program's actions are supposed to achieve the outcomes it intends. This 'logic model' is often not stated explicitly by people who run programs, it is simply assumed, and so an evaluator will need to draw out from the program staff how exactly the program is supposed to achieve its aims and assess whether this logic is plausible. For example, in an HIV prevention program, it may be assumed that educating people about HIV/AIDS transmission, risk and safe sex practices will result in safer sex being practiced. However, research in South Africa increasingly shows that in spite of increased education and knowledge, people still often do not practice safe sex.<sup id=\"cite_ref-11\" class=\"reference\"><a href=\"#cite_note-11\">&#91;11&#93;</a></sup> Therefore, the logic of a program which relies on education as a means to get people to use condoms may be faulty. This is why it is important to read research that has been done in the area.\nExplicating this logic can also reveal unintended or unforeseen consequences of a program, both positive and negative. The program theory drives the hypotheses to test for impact evaluation. Developing a logic model can also build common understanding amongst program staff and stakeholders about what the program is actually supposed to do and how it is supposed to do it, which is often lacking (see <a href=\"/wiki/Participatory_impact_pathways_analysis\" title=\"Participatory impact pathways analysis\">Participatory impact pathways analysis</a>). Of course, it is also possible that during the process of trying to elicit the logic model behind a program the evaluators may discover that such a model is either incompletely developed, internally contradictory, or (in worst cases) essentially nonexisistent. This decidedly limits the effectiveness of the evaluation, although it does not necessarily reduce or eliminate the program.<sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup>\n</p><p>Creating a logic model is a wonderful way to help visualize important aspects of programs, especially when preparing for an evaluation. An evaluator should create a logic model with input from many different stake holders. Logic Models have 5 major components: Resources or Inputs, Activities, Outputs, Short-term outcomes, and Long-term outcomes <sup id=\"cite_ref-McLaughlin,_J._A._1999_13-0\" class=\"reference\"><a href=\"#cite_note-McLaughlin,_J._A._1999-13\">&#91;13&#93;</a></sup>  Creating a logic model helps articulate the problem, the resources and capacity that are currently being used to address the problem, and the measurable outcomes from the program. Looking at the different components of a program in relation to the overall short-term and long-term goals allows for illumination of potential misalignments. Creating an actual logic model is particularly important because it helps clarify for all stakeholders: the definition of the problem, the overarching goals, and the capacity and outputs of the program.<sup id=\"cite_ref-McLaughlin,_J._A._1999_13-1\" class=\"reference\"><a href=\"#cite_note-McLaughlin,_J._A._1999-13\">&#91;13&#93;</a></sup>\n</p><p>Rossi, Lipsey &amp; Freeman (2004) suggest four approaches and procedures that can be used to assess the program theory.<sup id=\"cite_ref-Rossi_7-6\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> These approaches are discussed below.\n</p>\n<ul><li>Assessment in relation to social needs <sup id=\"cite_ref-Rossi_7-7\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup></li></ul>\n<p>This entails assessing the program theory by relating it to the needs of the target population the program is intended to serve. If the program theory fails to address the needs of the target population it will be rendered ineffective even when if it is well implemented.<sup id=\"cite_ref-Rossi_7-8\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<ul><li>Assessment of logic and plausibility<sup id=\"cite_ref-Rossi_7-9\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup></li></ul>\n<p>This form of assessment  involves asking a panel of expert reviewers to critically review the logic and plausibility of the assumptions and expectations inherent in the program's design.<sup id=\"cite_ref-Rossi_7-10\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> The review process is unstructured and open ended so as to address certain issues on the program design. Rutman (1980), Smith (1989), and Wholly (1994) suggested the questions listed below to assist with the review process.<sup id=\"cite_ref-Rossi_7-11\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<dl><dd>Are the program goals and objectives well defined?</dd>\n<dd>Are the program goals and objectives feasible?</dd>\n<dd>Is the change process presumed in the program theory feasible?</dd>\n<dd>Are the procedures for identifying members of the target population, delivering service to them, and sustaining that service through completion well defined and suffiient?</dd>\n<dd>Are the constituent components, activities, and functions of the program well defined and sufficient?</dd>\n<dd>Are the resources allocated to the program and its various activities adequate?</dd></dl>\n<ul><li>Assessment through comparison with research and practice <sup id=\"cite_ref-Rossi_7-12\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup></li></ul>\n<p>This form of assessment requires gaining information from research literature and existing practices to assess various components of the program theory. The evaluator can assess whether the program theory is congruent with research evidence and practical experiences of programs with similar concepts.<sup id=\"cite_ref-Rossi_7-13\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<ul><li>Assessment via preliminary observation <sup id=\"cite_ref-Rossi_7-14\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup></li></ul>\n<p>This approach involves incorporating firsthand observations into the assessment process as it provides a reality check on the concordance between the program theory and the program itself.<sup id=\"cite_ref-Rossi_7-15\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> The observations can focus on the attainability of the outcomes, circumstances of the target population, and the plausibility of the program activities and the supporting resources.<sup id=\"cite_ref-Rossi_7-16\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p><p>These different forms of assessment of program theory can be conducted to ensure that the program theory is sound.\n</p>\n<h3><span class=\"mw-headline\" id=\"Assessing_implementation\">Assessing implementation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=4\" title=\"Edit section: Assessing implementation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Process analysis looks beyond the theory of what the program is supposed to do and instead evaluates how the program is being implemented. This evaluation determines whether the components identified as critical to the success of the program are being implemented.  The evaluation determines whether target populations are being reached, people are receiving the intended services, staff are adequately qualified. Process evaluation is an ongoing process in which repeated measures may be used to evaluate whether the program is being implemented effectively. This problem is particularly critical because many innovations, particularly in areas like education and public policy, consist of fairly complex chains of action. Many of which these elements rely on the prior correct implementation of other elements, and will fail if the prior implementation was not done correctly. This was conclusively demonstrated by <a href=\"/wiki/Gene_V._Glass\" title=\"Gene V. Glass\">Gene V. Glass</a> and many others during the 1980s. Since incorrect or ineffective implementation will produce the same kind of neutral or negative results that would be produced by correct implementation of a poor innovation, it is essential that evaluation research assess the implementation process itself.<sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\">&#91;14&#93;</a></sup> Otherwise, a good innovative idea may be mistakenly characterized as ineffective, where in fact it simply had never been implemented as designed.\n</p>\n<h3><span id=\"Assessing_the_impact_.28effectiveness.29\"></span><span class=\"mw-headline\" id=\"Assessing_the_impact_(effectiveness)\">Assessing the impact (effectiveness)</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=5\" title=\"Edit section: Assessing the impact (effectiveness)\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The impact evaluation determines the causal effects of the program. This involves trying to measure if the program has achieved its intended outcomes, i.e. program outcomes.\n</p>\n<h4><span class=\"mw-headline\" id=\"Program_Outcomes\">Program Outcomes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=6\" title=\"Edit section: Program Outcomes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>An outcome is the state of the target population or the social conditions that a program is expected to have changed.<sup id=\"cite_ref-Rossi_7-17\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>  Program outcomes are the observed characteristics of the target population or social conditions, not of the program. Thus the concept of an outcome does not necessarily mean that the program targets have actually changed or that the program has caused them to change in any way.<sup id=\"cite_ref-Rossi_7-18\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p><p>There are two kinds of outcomes, namely outcome level and outcome change, also associated with program effect.<sup id=\"cite_ref-Rossi_7-19\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<ul><li><b>Outcome level</b> refers to the status of an outcome at some point in time.</li>\n<li><b>Outcome change</b> refers to the difference between outcome levels at different points in time.</li>\n<li><b>Program effect</b> refers to that portion of an outcome change that can be attributed uniquely to a program as opposed to the influence of some other factor.</li></ul>\n<h4><span class=\"mw-headline\" id=\"Measuring_Program_Outcomes\">Measuring Program Outcomes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=7\" title=\"Edit section: Measuring Program Outcomes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Outcome measurement is a matter of representing the circumstances defined as the outcome by means of observable indicators that vary systematically with changes or differences in those circumstances.<sup id=\"cite_ref-Rossi_7-20\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> Outcome measurement is a systematic way to assess the extent to which a program has achieved its intended outcomes.<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\">&#91;15&#93;</a></sup> According to Mouton (2009) measuring the impact of a program means demonstrating or estimating the accumulated differentiated proximate and emergent effect, some of which might be unintended and therefore unforeseen.<sup id=\"cite_ref-Mouton,_J_2009_16-0\" class=\"reference\"><a href=\"#cite_note-Mouton,_J_2009-16\">&#91;16&#93;</a></sup>\n</p><p>Outcome measurement serves to help you understand whether the program is effective or not. It further helps you to clarify your understanding of your program. But the most important reason for undertaking the effort is to understand the impacts of your work on the people you serve.<sup id=\"cite_ref-Mouton,_J_2009_16-1\" class=\"reference\"><a href=\"#cite_note-Mouton,_J_2009-16\">&#91;16&#93;</a></sup> With the information you collect, you can determine which activities to continue and build upon, and which you need to change in order to improve the effectiveness of the program.\n</p><p>This can involve using sophisticated statistical techniques in order to measure the effect of the program and to find causal relationship between the program and the various outcomes. More information about impact evaluation is found under the heading 'Determining Causation'.\n</p>\n<h3><span class=\"mw-headline\" id=\"Assessing_efficiency\">Assessing efficiency</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=8\" title=\"Edit section: Assessing efficiency\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Finally, cost-benefit or cost-efficiency analysis assesses the efficiency of a program. Evaluators outline the benefits and cost of the program for comparison. An efficient program has a lower cost-benefit ratio. There are two types of efficiency, namely, static and dynamic. While static efficiency concerns achieving the objectives with least costs, dynamic efficiency concerns continuous improvement.\n<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Determining_causation\">Determining causation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=9\" title=\"Edit section: Determining causation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Perhaps the most difficult part of evaluation is determining whether the program itself is causing the changes that are observed in the population it was aimed at. Events or processes outside of the program may be the real cause of the observed outcome (or the real prevention of the anticipated outcome).\n</p><p>Causation is difficult to determine. One main reason for this is <a href=\"/wiki/Self-selection\" class=\"mw-redirect\" title=\"Self-selection\">self selection</a> bias.<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup> People select themselves to participate in a program. For example, in a job training program, some people decide to participate and others do not. Those who do participate may differ from those who do not in important ways. They may be more determined to find a job or have better support resources. These characteristics may actually be causing the observed outcome of increased employment, not the job training program.\n</p><p>Evaluations conducted with random assignment are able to make stronger inferences about causation. Randomly assigning people to participate or to not participate in the program, reduces or eliminates <a href=\"/wiki/Self-selection_bias\" title=\"Self-selection bias\">self-selection bias</a>. Thus, the group of people who participate would likely be more comparable to the group who did not participate.\n</p><p>However, since most programs cannot use random assignment, causation cannot be determined. Impact analysis can still provide useful information. For example, the outcomes of the program can be described. Thus the evaluation can describe that people who participated in the program were more likely to experience a given outcome than people who did not participate.\n</p><p>If the program is fairly large, and there are enough data, statistical analysis can be used to make a reasonable case for the program by showing, for example, that other causes are unlikely.\n</p>\n<h2><span id=\"Reliability.2C_validity_and_sensitivity_in_program_evaluation\"></span><span class=\"mw-headline\" id=\"Reliability,_validity_and_sensitivity_in_program_evaluation\">Reliability, validity and sensitivity in program evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=10\" title=\"Edit section: Reliability, validity and sensitivity in program evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>It is important to ensure that the instruments (for example, tests, questionnaires, etc.) used in program evaluation are as reliable, valid and sensitive as possible. According to Rossi et al. (2004, p.&#160;222),<sup id=\"cite_ref-Rossi_7-21\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> 'a measure that is poorly chosen or poorly conceived can completely undermine the worth of an impact assessment by producing misleading estimates. Only if outcome measures are valid, reliable and appropriately sensitive can impact assessments be regarded as credible'.\n</p>\n<h3><span class=\"mw-headline\" id=\"Reliability\">Reliability</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=11\" title=\"Edit section: Reliability\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The reliability of a measurement instrument is the 'extent to which the measure produces the same results when used repeatedly to measure the same thing' (Rossi et al., 2004, p.&#160;218).<sup id=\"cite_ref-Rossi_7-22\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> The more reliable a measure is, the greater its statistical power and the more credible its findings. If a measuring instrument is unreliable, it may dilute and obscure the real effects of a program, and the program will 'appear to be less effective than it actually is' (Rossi et al., 2004, p.&#160;219).<sup id=\"cite_ref-Rossi_7-23\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> Hence, it is important to ensure the evaluation is as reliable as possible.\n</p>\n<h3><span class=\"mw-headline\" id=\"Validity\">Validity</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=12\" title=\"Edit section: Validity\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The validity of a measurement instrument is 'the extent to which it measures what it is intended to measure' (Rossi et al., 2004, p.&#160;219).<sup id=\"cite_ref-Rossi_7-24\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> This concept can be difficult to accurately measure: in general use in evaluations, an instrument may be deemed valid if accepted as valid by the stakeholders (stakeholders may include, for example, funders, program administrators, et cetera).\n</p>\n<h3><span class=\"mw-headline\" id=\"Sensitivity\">Sensitivity</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=13\" title=\"Edit section: Sensitivity\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The principal purpose of the evaluation process is to measure whether the program has an effect on the social problem it seeks to redress; hence, the measurement instrument must be sensitive enough to discern these potential changes (Rossi et al., 2004).<sup id=\"cite_ref-Rossi_7-25\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> A measurement instrument may be insensitive if it contains items measuring outcomes which the program couldn't possibly effect, or if the instrument was originally developed for applications to individuals (for example standardized psychological measures) rather than to a group setting (Rossi et al., 2004).<sup id=\"cite_ref-Rossi_7-26\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup> These factors may result in 'noise' which may obscure any effect the program may have had.\n</p><p>Only measures which adequately achieve the benchmarks of reliability, validity and sensitivity can be said to be credible evaluations. It is the duty of evaluators to produce credible evaluations, as their findings may have far reaching effects. A discreditable evaluation which is unable to show that a program is achieving its purpose when it is in fact creating positive change may cause the program to lose its funding undeservedly.<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:No_original_research#Synthesis_of_published_material\" title=\"Wikipedia:No original research\"><span title=\"The material near this tag may be based upon an improper synthesis of sources. (February 2012)\">improper synthesis?</span></a></i>&#93;</sup>\n</p><p>Steps to Program Evaluation Framework\n</p><p>According to the Center for Disease Control (CDC) there are six steps to a complete program evaluation. The steps described are: engage stakeholder, describe the program, focus the evaluation design, gather credible evidence, justify conclusions, and ensure use and share lessons learned.<sup id=\"cite_ref-19\" class=\"reference\"><a href=\"#cite_note-19\">&#91;19&#93;</a></sup>  These steps can happen in a cycle framework to represent the continuing process of evaluation.\n</p>\n<h2><span class=\"mw-headline\" id=\"Evaluating_Collective_Impact\">Evaluating Collective Impact</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=14\" title=\"Edit section: Evaluating Collective Impact\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Though program evaluation processes mentioned here are appropriate for most programs, highly complex non-linear initiatives, such as those using the <a href=\"/wiki/Collective_impact\" title=\"Collective impact\">collective impact</a> (CI) model, require a dynamic approach to evaluation.  Collective impact is \"the commitment of a group of important actors from different sectors to a common agenda for solving a specific social problem\"<sup id=\"cite_ref-20\" class=\"reference\"><a href=\"#cite_note-20\">&#91;20&#93;</a></sup> and typically involves three stages, each with a different recommended evaluation approach:\n</p>\n<ul><li><b>Early phase:</b> CI participants are exploring possible strategies and developing plans for action.  Characterized by uncertainty.</li></ul>\n<p><i>Recommended evaluation approach:</i> Developmental evaluation to help CI partners understand the context of the initiative and its development:<sup id=\"cite_ref-21\" class=\"reference\"><a href=\"#cite_note-21\">&#91;21&#93;</a></sup> \"Developmental evaluation involves real time feedback about what is emerging in complex dynamic systems as innovators seek to bring about systems change.\"<sup id=\"cite_ref-22\" class=\"reference\"><a href=\"#cite_note-22\">&#91;22&#93;</a></sup>\n</p>\n<ul><li><b>Middle phase:</b> CI partners implement agreed upon strategies.  Some outcomes become easier to anticipate.</li></ul>\n<p><i>Recommended evaluation approach:</i> Formative evaluation to refine and improve upon the progress, as well as continued developmental evaluation to explore new elements as they emerge.  Formative evaluation involves \"careful monitoring of processes in order to respond to emergent properties and any unexpected outcomes.\"<sup id=\"cite_ref-23\" class=\"reference\"><a href=\"#cite_note-23\">&#91;23&#93;</a></sup>\n</p>\n<ul><li><b>Later phase:</b>  Activities achieve stability and are no longer in formation. Experience informs knowledge about which activities may be effective.</li></ul>\n<p><i>Recommended evaluation approach:</i> Summative evaluation \"uses both quantitative and qualitative methods in order to get a better understanding of what [the] project has achieved, and how or why this has occurred.\"<sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\">&#91;24&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Planning_a_program_evaluation\">Planning a program evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=15\" title=\"Edit section: Planning a program evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Planning a program evaluation can be broken up into four parts: focusing the evaluation, collecting the information, using the information, and managing the evaluation.<sup id=\"cite_ref-25\" class=\"reference\"><a href=\"#cite_note-25\">&#91;25&#93;</a></sup> Program evaluation involves reflecting on questions about evaluation purpose, what questions are necessary to ask, and what will be done with information gathered. Critical questions for consideration include:\n</p>\n<ul><li>What am I going to evaluate?</li>\n<li>What is the purpose of this evaluation?</li>\n<li>Who will use this evaluation? How will they use it?</li>\n<li>What questions is this evaluation seeking to answer?</li>\n<li>What information do I need to answer the questions?</li>\n<li>When is the evaluation needed? What resources do I need?</li>\n<li>How will I collect the data I need?</li>\n<li>How will data be analyzed?</li>\n<li>What is my implementation timeline?</li></ul>\n<h2><span class=\"mw-headline\" id=\"Methodological_constraints_and_challenges\">Methodological constraints and challenges</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=16\" title=\"Edit section: Methodological constraints and challenges\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<h3><span class=\"mw-headline\" id=\"The_shoestring_approach\">The shoestring approach</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=17\" title=\"Edit section: The shoestring approach\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The \"shoestring evaluation approach\" is designed to assist evaluators operating under  limited budget, limited access or availability of data and  limited turnaround time, to conduct effective evaluations that are methodologically rigorous(Bamberger, Rugh, Church &amp; Fort, 2004).<sup id=\"cite_ref-Bamberger_26-0\" class=\"reference\"><a href=\"#cite_note-Bamberger-26\">&#91;26&#93;</a></sup> This approach has responded to the continued greater need for evaluation processes that are more rapid and economical under difficult circumstances of budget, time constraints and limited availability of data.  However, it is not always possible to design an evaluation to achieve the highest standards available. Many programs do not build an evaluation procedure into their design or budget. Hence, many evaluation processes do not begin until the program is already underway, which can result in time, budget or data constraints for the evaluators, which in turn can affect the reliability, validity or sensitivity of the evaluation. &gt; The shoestring approach helps to ensure that the maximum possible methodological rigor is achieved under these constraints.\n</p>\n<h3><span class=\"mw-headline\" id=\"Budget_constraints\">Budget constraints</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=18\" title=\"Edit section: Budget constraints\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Frequently, programs are faced with budget constraints because most original projects do not include a budget to conduct an evaluation (Bamberger et al., 2004). Therefore, this automatically results in evaluations being allocated smaller budgets that are inadequate for a rigorous evaluation. Due to the budget constraints it might be difficult to effectively apply the most appropriate   methodological instruments. These constraints may consequently affect the time available in which to do the evaluation (Bamberger et al., 2004).<sup id=\"cite_ref-Bamberger_26-1\" class=\"reference\"><a href=\"#cite_note-Bamberger-26\">&#91;26&#93;</a></sup> Budget constraints may be addressed by simplifying the evaluation design, revising the sample size, exploring economical data collection methods (such as using volunteers to collect data, shortening surveys, or using focus groups and key informants) or looking for reliable secondary data (Bamberger et al., 2004).<sup id=\"cite_ref-Bamberger_26-2\" class=\"reference\"><a href=\"#cite_note-Bamberger-26\">&#91;26&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Time_constraints\">Time constraints</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=19\" title=\"Edit section: Time constraints\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The most time constraint that can be faced by an evaluator is when the evaluator is summoned to conduct an evaluation when a project is already underway if they are given limited time to do the evaluation compared to the life of the study, or if they are not given enough time for adequate planning. Time constraints are particularly problematic when the evaluator is not familiar with the area or country in which the program is situated (Bamberger et al., 2004).<sup id=\"cite_ref-Bamberger_26-3\" class=\"reference\"><a href=\"#cite_note-Bamberger-26\">&#91;26&#93;</a></sup> Time constraints can be addressed by the methods listed under budget constraints as above, and also by careful planning to ensure effective data collection and analysis within the limited time space.\n</p>\n<h3><span class=\"mw-headline\" id=\"Data_constraints\">Data constraints</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=20\" title=\"Edit section: Data constraints\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>If the evaluation is initiated late in the program, there may be no baseline data on the conditions of the target group before the intervention began (Bamberger et al., 2004).<sup id=\"cite_ref-Bamberger_26-4\" class=\"reference\"><a href=\"#cite_note-Bamberger-26\">&#91;26&#93;</a></sup> Another possible cause of data constraints is if the data have been collected by program staff and contain systematic reporting biases or poor record keeping standards and is subsequently of little use (Bamberger et al., 2004).<sup id=\"cite_ref-Bamberger_26-5\" class=\"reference\"><a href=\"#cite_note-Bamberger-26\">&#91;26&#93;</a></sup> Another source of data constraints may result if the target group are difficult to reach to collect data from - for example homeless people, drug addicts, migrant workers, et cetera (Bamberger et al., 2004).<sup id=\"cite_ref-Bamberger_26-6\" class=\"reference\"><a href=\"#cite_note-Bamberger-26\">&#91;26&#93;</a></sup> Data constraints can be addressed by reconstructing baseline data from secondary data or through the use of multiple methods. Multiple methods, such as the combination of qualitative and quantitative data can increase validity through triangulation and save time and money.\nAdditionally, these constraints may be dealt with through <b>careful planning and consultation</b> with program stakeholders. By clearly identifying and understanding client needs ahead of the evaluation, costs and time of the evaluative process can be streamlined and reduced, while still maintaining credibility.\n</p><p>All in all,  time, monetary and data constraints can have negative implications on the validity, reliability and transferability of the evaluation. The shoestring approach has been created to assist evaluators   to correct  the limitations identified above by identifying ways to reduce costs and time, reconstruct baseline data and to ensure maximum quality under existing constraints (Bamberger et al., 2004).\n</p>\n<h3><span class=\"mw-headline\" id=\"Five-tiered_approach\">Five-tiered approach</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=21\" title=\"Edit section: Five-tiered approach\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The five-tiered approach to evaluation further develops the strategies that the shoestring approach to evaluation is based upon.<sup id=\"cite_ref-F.H.J_27-0\" class=\"reference\"><a href=\"#cite_note-F.H.J-27\">&#91;27&#93;</a></sup>  It was originally developed by Jacobs (1988) as an alternative way to evaluate community-based programs and as such was applied to a statewide child and family program in Massachusetts, U.S.A.<sup id=\"cite_ref-W&amp;G_28-0\" class=\"reference\"><a href=\"#cite_note-W&amp;G-28\">&#91;28&#93;</a></sup> The five-tiered approach is offered as a conceptual framework for matching evaluations more precisely to the characteristics of the programs themselves, and to the particular resources and constraints inherent in each evaluation context.<sup id=\"cite_ref-F.H.J_27-1\" class=\"reference\"><a href=\"#cite_note-F.H.J-27\">&#91;27&#93;</a></sup> In other words, the five-tiered approach seeks to tailor the evaluation to the specific needs of each evaluation context.\n</p><p>The earlier tiers (1-3) generate descriptive and process-oriented information while the later tiers (4-5) determine both the short-term and the long-term effects of the program.<sup id=\"cite_ref-NPEN_29-0\" class=\"reference\"><a href=\"#cite_note-NPEN-29\">&#91;29&#93;</a></sup> The five levels are organized as follows:\n</p>\n<ul><li>Tier 1: needs assessment (sometimes referred to as pre-implementation)<sup id=\"cite_ref-B&amp;D_30-0\" class=\"reference\"><a href=\"#cite_note-B&amp;D-30\">&#91;30&#93;</a></sup></li>\n<li>Tier 2: monitoring and accountability</li>\n<li>Tier 3: quality review and program clarification (sometimes referred to as understanding and refining)<sup id=\"cite_ref-CYF_31-0\" class=\"reference\"><a href=\"#cite_note-CYF-31\">&#91;31&#93;</a></sup></li>\n<li>Tier 4: achieving outcomes</li>\n<li>Tier 5: establishing impact</li></ul>\n<p>For each tier, purpose(s) are identified, along with corresponding tasks that enable the identified purpose of the tier to be achieved.<sup id=\"cite_ref-CYF_31-1\" class=\"reference\"><a href=\"#cite_note-CYF-31\">&#91;31&#93;</a></sup> For example, the purpose of the first tier, Needs assessment, would be to document a need for a program in a community. The task for that tier would be to assess the community's needs and assets by working with all relevant stakeholders.<sup id=\"cite_ref-CYF_31-2\" class=\"reference\"><a href=\"#cite_note-CYF-31\">&#91;31&#93;</a></sup>\n</p><p>While the tiers are structured for consecutive use, meaning that information gathered in the earlier tiers is required for tasks on higher tiers, it acknowledges the fluid nature of evaluation.<sup id=\"cite_ref-NPEN_29-1\" class=\"reference\"><a href=\"#cite_note-NPEN-29\">&#91;29&#93;</a></sup> Therefore, it is possible to move from later tiers back to preceding ones, or even to work in two tiers at the same time.<sup id=\"cite_ref-CYF_31-3\" class=\"reference\"><a href=\"#cite_note-CYF-31\">&#91;31&#93;</a></sup> It is important for program evaluators to note, however, that a program must be evaluated at the appropriate level.<sup id=\"cite_ref-B&amp;D_30-1\" class=\"reference\"><a href=\"#cite_note-B&amp;D-30\">&#91;30&#93;</a></sup>\n</p><p>The five-tiered approach is said to be useful for family support programs which emphasise community and participant empowerment. This is because it encourages a participatory approach involving all stakeholders and it is through this process of reflection that empowerment is achieved.<sup id=\"cite_ref-W&amp;G_28-1\" class=\"reference\"><a href=\"#cite_note-W&amp;G-28\">&#91;28&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Methodological_challenges_presented_by_language_and_culture\">Methodological challenges presented by language and culture</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=22\" title=\"Edit section: Methodological challenges presented by language and culture\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The purpose of this section is to draw attention to some of the methodological challenges and dilemmas evaluators are potentially faced with when conducting a program evaluation in a developing country.  In many developing countries the major sponsors of evaluation are donor agencies from the developed world, and these agencies require regular evaluation reports in order to maintain accountability and control of resources, as well as generate evidence for the program\u2019s success or failure.<sup id=\"cite_ref-32\" class=\"reference\"><a href=\"#cite_note-32\">&#91;32&#93;</a></sup>  However, there are many hurdles and challenges which evaluators face when attempting to implement an evaluation program which attempts to make use of techniques and systems which are not developed within the context to which they are applied.<sup id=\"cite_ref-Smith,_1990_33-0\" class=\"reference\"><a href=\"#cite_note-Smith,_1990-33\">&#91;33&#93;</a></sup>  Some of the issues include differences in culture, attitudes, language and political process.<sup id=\"cite_ref-Smith,_1990_33-1\" class=\"reference\"><a href=\"#cite_note-Smith,_1990-33\">&#91;33&#93;</a></sup><sup id=\"cite_ref-Ebbutt,_1998_34-0\" class=\"reference\"><a href=\"#cite_note-Ebbutt,_1998-34\">&#91;34&#93;</a></sup>\n</p><p>Culture is defined by Ebbutt (1998, p.&#160;416) as a \"constellation of both written and unwritten expectations, values, norms, rules, laws, artifacts, rituals and behaviors that permeate a society and influence how people behave socially\".<sup id=\"cite_ref-Ebbutt,_1998_34-1\" class=\"reference\"><a href=\"#cite_note-Ebbutt,_1998-34\">&#91;34&#93;</a></sup> Culture can influence many facets of the evaluation process, including data collection, evaluation program implementation and the analysis and understanding of the results of the evaluation.<sup id=\"cite_ref-Ebbutt,_1998_34-2\" class=\"reference\"><a href=\"#cite_note-Ebbutt,_1998-34\">&#91;34&#93;</a></sup>  In particular, instruments which are traditionally used to collect data such as questionnaires and semi-structured interviews need to be sensitive to differences in culture, if they were originally developed in a different cultural context.<sup id=\"cite_ref-Bulmer_&amp;_Warwick,_1993_35-0\" class=\"reference\"><a href=\"#cite_note-Bulmer_&amp;_Warwick,_1993-35\">&#91;35&#93;</a></sup>  The understanding and meaning of constructs which the evaluator is attempting to measure may not be shared between the evaluator and the sample population and thus the transference of concepts is an important notion, as this will influence the quality of the data collection carried out by evaluators as well as the analysis and results generated by the data.<sup id=\"cite_ref-Bulmer_&amp;_Warwick,_1993_35-1\" class=\"reference\"><a href=\"#cite_note-Bulmer_&amp;_Warwick,_1993-35\">&#91;35&#93;</a></sup>\n</p><p>Language also plays an important part in the evaluation process, as language is tied closely to culture.<sup id=\"cite_ref-Bulmer_&amp;_Warwick,_1993_35-2\" class=\"reference\"><a href=\"#cite_note-Bulmer_&amp;_Warwick,_1993-35\">&#91;35&#93;</a></sup>  Language can be a major barrier to communicating concepts which the evaluator is trying to access, and translation is often required.<sup id=\"cite_ref-Ebbutt,_1998_34-3\" class=\"reference\"><a href=\"#cite_note-Ebbutt,_1998-34\">&#91;34&#93;</a></sup>  There are a multitude of problems with translation, including the loss of meaning as well as the exaggeration or enhancement of meaning by translators.<sup id=\"cite_ref-Ebbutt,_1998_34-4\" class=\"reference\"><a href=\"#cite_note-Ebbutt,_1998-34\">&#91;34&#93;</a></sup>  For example, terms which are contextually specific may not translate into another language with the same weight or meaning.  In particular, data collection instruments need to take meaning into account as the subject matter may not be considered sensitive in a particular context might prove to be sensitive in the context in which the evaluation is taking place.<sup id=\"cite_ref-Bulmer_&amp;_Warwick,_1993_35-3\" class=\"reference\"><a href=\"#cite_note-Bulmer_&amp;_Warwick,_1993-35\">&#91;35&#93;</a></sup>  Thus, evaluators need to take into account two important concepts when administering data collection tools: lexical equivalence and conceptual equivalence.<sup id=\"cite_ref-Bulmer_&amp;_Warwick,_1993_35-4\" class=\"reference\"><a href=\"#cite_note-Bulmer_&amp;_Warwick,_1993-35\">&#91;35&#93;</a></sup>  Lexical equivalence asks the question: how does one phrase a question in two languages using the same words?  This is a difficult task to accomplish, and uses of techniques such as back-translation may aid the evaluator but may not result in perfect transference of meaning.<sup id=\"cite_ref-Bulmer_&amp;_Warwick,_1993_35-5\" class=\"reference\"><a href=\"#cite_note-Bulmer_&amp;_Warwick,_1993-35\">&#91;35&#93;</a></sup>  This leads to the next point, conceptual equivalence.  It is not a common occurrence for concepts to transfer unambiguously from one culture to another.<sup id=\"cite_ref-Bulmer_&amp;_Warwick,_1993_35-6\" class=\"reference\"><a href=\"#cite_note-Bulmer_&amp;_Warwick,_1993-35\">&#91;35&#93;</a></sup>  Data collection instruments which have not undergone adequate testing and piloting may therefore render results which are not useful as the concepts which are measured by the instrument may have taken on a different meaning and thus rendered the instrument unreliable and invalid.<sup id=\"cite_ref-Bulmer_&amp;_Warwick,_1993_35-7\" class=\"reference\"><a href=\"#cite_note-Bulmer_&amp;_Warwick,_1993-35\">&#91;35&#93;</a></sup>\n</p><p>Thus, it can be seen that evaluators need to take into account the methodological challenges created by differences in culture and language when attempting to conduct a program evaluation in a developing country.\n</p>\n<h2><span class=\"mw-headline\" id=\"Utilization_results\">Utilization results</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=23\" title=\"Edit section: Utilization results\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>There are three conventional uses of evaluation results: <b>persuasive utilization</b>, <b>direct (instrumental) utilization</b>, and <b>conceptual utilization</b>.\n</p>\n<h3><span class=\"mw-headline\" id=\"Persuasive_utilization\">Persuasive utilization</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=24\" title=\"Edit section: Persuasive utilization\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Persuasive utilization is the enlistment of evaluation results in an effort to persuade an audience to either support an agenda or to oppose it. Unless the 'persuader' is the same person that ran the evaluation, this form of utilization is not of much interest to evaluators as they often cannot foresee possible future efforts of persuasion.<sup id=\"cite_ref-Rossi_7-27\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<h3><span id=\"Direct_.28instrumental.29_utilization\"></span><span class=\"mw-headline\" id=\"Direct_(instrumental)_utilization\">Direct (instrumental) utilization</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=25\" title=\"Edit section: Direct (instrumental) utilization\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Evaluators often tailor their evaluations to produce results that can have a direct influence in the improvement of the structure, or on the process, of a program. For example, the evaluation of a novel educational intervention may produce results that indicate no improvement in students' marks. This may be due to the intervention not having a sound theoretical background, or it may be that the intervention is not conducted as originally intended. The results of the evaluation would hopefully cause to the creators of the intervention to go back to the drawing board to re-create the core structure of the intervention, or even change the implementation processes.<sup id=\"cite_ref-Rossi_7-28\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Conceptual_utilization\">Conceptual utilization</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=26\" title=\"Edit section: Conceptual utilization\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>But even if evaluation results do not have a direct influence in the re-shaping of a program, they may still be used to make people aware of the issues the program is trying to address. Going back to the example of an evaluation of a novel educational intervention, the results can also be used to inform educators and students about the different barriers that may influence students' learning difficulties. A number of studies on these barriers may then be initiated by this new information.<sup id=\"cite_ref-Rossi_7-29\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Variables_affecting_utilization\">Variables affecting utilization</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=27\" title=\"Edit section: Variables affecting utilization\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>There are five conditions that seem to affect the utility of evaluation results, namely <i>relevance</i>, <i>communication between the evaluators and the users of the results</i>, <i>information processing by the users</i>, <i>the plausibility of the results</i>, as well as <i>the level of involvement or advocacy of the users</i>.<sup id=\"cite_ref-Rossi_7-30\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Guidelines_for_maximizing_utilization\">Guidelines for maximizing utilization</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=28\" title=\"Edit section: Guidelines for maximizing utilization\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Quoted directly from Rossi et al. (2004, p.&#160;416).:<sup id=\"cite_ref-Rossi_7-31\" class=\"reference\"><a href=\"#cite_note-Rossi-7\">&#91;7&#93;</a></sup>\n</p>\n<ul><li>Evaluators must understand the cognitive styles of decisionmakers</li>\n<li>Evaluation results must be timely and available when needed</li>\n<li>Evaluations must respect stakeholders' program commitments</li>\n<li>Utilization and dissemination plans should be part of the evaluation design</li>\n<li>Evaluations should include an assessment of utilization</li></ul>\n<h2><span class=\"mw-headline\" id=\"Internal_versus_external_program_evaluators\">Internal versus external program evaluators</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=29\" title=\"Edit section: Internal versus external program evaluators\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The choice of the evaluator chosen to evaluate the program may be regarded as equally important as the process of the evaluation. Evaluators may be internal (persons associated with the program to be executed) or external (Persons not associated with any part of the execution/implementation of the program). (Division for oversight services,2004).  The following provides a brief summary of the advantages and disadvantages of internal and external evaluators adapted from the Division of oversight services (2004), for a more comprehensive list of advantages and disadvantages of internal and external evaluators, see (Division of oversight services, 2004).\n</p>\n<h3><span class=\"mw-headline\" id=\"Internal_evaluators\">Internal evaluators</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=30\" title=\"Edit section: Internal evaluators\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p><b>Advantages</b>\n</p>\n<ul><li>May have better overall knowledge of the program and possess informal knowledge of the program</li>\n<li>Less threatening as already familiar with staff</li>\n<li>Less costly</li></ul>\n<p><b>Disadvantages</b>\n</p>\n<ul><li>May be less objective</li>\n<li>May be more preoccupied with other activities of the program and not give the evaluation complete attention</li>\n<li>May not be adequately trained as an evaluator.</li></ul>\n<h3><span class=\"mw-headline\" id=\"External_evaluators\">External evaluators</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=31\" title=\"Edit section: External evaluators\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p><b>Advantages</b>\n</p>\n<ul><li>More objective of the process, offers new perspectives, different angles to observe and critique the process</li>\n<li>May be able to dedicate greater amount of time and attention to the evaluation</li>\n<li>May have greater expertise and evaluation brain</li></ul>\n<p><b>Disadvantages</b>\n</p>\n<ul><li>May be more costly and require more time for the contract, monitoring, negotiations etc.</li>\n<li>May be unfamiliar with program staff and create anxiety about being evaluated</li>\n<li>May be unfamiliar with organization policies, certain constraints affecting the program.</li></ul>\n<h2><span class=\"mw-headline\" id=\"Three_paradigms\">Three paradigms</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=32\" title=\"Edit section: Three paradigms\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<h3><span class=\"mw-headline\" id=\"Positivist\">Positivist</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=33\" title=\"Edit section: Positivist\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Potter (2006)<sup id=\"cite_ref-36\" class=\"reference\"><a href=\"#cite_note-36\">&#91;36&#93;</a></sup> identifies and describes three broad paradigms within program evaluation . The first, and probably most common, is the <a href=\"/wiki/Positivist\" class=\"mw-redirect\" title=\"Positivist\">positivist</a> approach, in which evaluation can only occur where there are \"objective\", observable and measurable aspects of a program, requiring predominantly quantitative evidence.  The positivist approach includes evaluation dimensions such as needs assessment, assessment of program theory, assessment of program process, impact assessment and efficiency assessment (Rossi, Lipsey and Freeman, 2004).<sup id=\"cite_ref-37\" class=\"reference\"><a href=\"#cite_note-37\">&#91;37&#93;</a></sup>\nA detailed example of the positivist approach is a study conducted by the Public Policy Institute of California report titled \"Evaluating Academic Programs in California's Community Colleges\", in which the evaluators examine measurable activities (i.e. enrollment data) and conduct quantitive assessments like factor analysis.<sup id=\"cite_ref-38\" class=\"reference\"><a href=\"#cite_note-38\">&#91;38&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Interpretive\">Interpretive</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=34\" title=\"Edit section: Interpretive\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The second paradigm identified by Potter (2006) is that of interpretive approaches, where it is argued that it is essential that the evaluator develops an understanding of the perspective, experiences and expectations of all stakeholders.  This would lead to a better understanding of the various meanings and needs held by stakeholders, which is crucial before one is able to make judgments about the merit or value of a program.  The evaluator\u2019s contact with the program is often over an extended period of time and, although there is no standardized method, observation, interviews and focus groups are commonly used.\nA report commissioned by the World Bank details 8 approaches in which qualitative and quantitative methods can be integrated and perhaps yield insights not achievable through only one method.<sup id=\"cite_ref-39\" class=\"reference\"><a href=\"#cite_note-39\">&#91;39&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Critical-emancipatory\">Critical-emancipatory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=35\" title=\"Edit section: Critical-emancipatory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Potter (2006) also identifies critical-emancipatory approaches to program evaluation, which are largely based on <a href=\"/wiki/Action_research\" title=\"Action research\">action research</a> for the purposes of social transformation.  This type of approach is much more ideological and often includes a greater degree of social activism on the part of the evaluator.  This approach would be appropriate for qualitative and participative evaluations.  Because of its critical focus on societal power structures and its emphasis on participation and empowerment, Potter argues this type of evaluation can be particularly useful in developing countries.\n</p><p>Despite the paradigm which is used in any program evaluation, whether it be positivist, interpretive or critical-emancipatory, it is essential to acknowledge that evaluation takes place in specific socio-political contexts.  Evaluation does not exist in a vacuum and all evaluations, whether they are aware of it or not, are influenced by socio-political factors.  It is important to recognize the evaluations and the findings which result from this kind of evaluation process can be used in favour or against particular ideological, social and political agendas (Weiss, 1999).<sup id=\"cite_ref-40\" class=\"reference\"><a href=\"#cite_note-40\">&#91;40&#93;</a></sup>  This is especially true in an age when resources are limited and there is competition between organizations for certain projects to be prioritised over others (Louw, 1999).<sup id=\"cite_ref-41\" class=\"reference\"><a href=\"#cite_note-41\">&#91;41&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Empowerment_evaluation\">Empowerment evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=36\" title=\"Edit section: Empowerment evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Empowerment_evaluation\" title=\"Empowerment evaluation\">Empowerment evaluation</a></div>\n<p>Empowerment evaluation makes use of evaluation concepts, techniques, and findings to foster improvement and self-determination of a particular program aimed at a specific target population/program participants.<sup id=\"cite_ref-F.D.M_42-0\" class=\"reference\"><a href=\"#cite_note-F.D.M-42\">&#91;42&#93;</a></sup> Empowerment evaluation is value oriented towards getting program participants involved in bringing about change in the programs they are targeted for. One of the main focuses in empowerment evaluation is to incorporate the program participants in the conducting of the evaluation process. This process is then often followed by some sort of critical reflection of the program. In such cases, an external/outsider evaluator serves as a consultant/coach/facilitator to the program participants and seeks to understand the program from the perspective of the participants. Once a clear understanding of the participants perspective has been gained appropriate steps and strategies can be devised (with the valuable input of the participants) and implemented in order to reach desired outcomes.\n</p><p>According to Fetterman (2002)<sup id=\"cite_ref-F.D.M_42-1\" class=\"reference\"><a href=\"#cite_note-F.D.M-42\">&#91;42&#93;</a></sup> empowerment evaluation has three steps;\n</p>\n<ul><li>Establishing a mission</li>\n<li>Taking stock</li>\n<li>Planning for the future</li></ul>\n<h3><span class=\"mw-headline\" id=\"Establishing_a_mission\">Establishing a mission</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=37\" title=\"Edit section: Establishing a mission\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The first step involves evaluators asking the program participants and staff members (of the program) to define the mission of the program. Evaluators may opt to carry this step out by bringing such parties together and asking them to generate and discuss the mission of the program. The logic behind this approach is to show each party that there may be divergent views of what the program mission actually is.\n</p>\n<h3><span class=\"mw-headline\" id=\"Taking_stock\">Taking stock</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=38\" title=\"Edit section: Taking stock\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Taking stock as the second step consists of two important tasks. The first task is concerned with program participants and program staff generating a list of current key activities that are crucial to the functioning of the program. The second task is concerned with rating the identified key activities, also known as <i>prioritization</i>. For example, each party member may be asked to rate each key activity on a scale from 1 to 10, where 10 is the most important and 1 the least important. The role of the evaluator during this task is to facilitate interactive discussion amongst members in an attempt to establish some baseline of shared meaning and understanding pertaining to the key activities.In addition, relevant documentation (such as financial reports and curriculum information) may be brought into the discussion when considering some of the key activities.\n</p>\n<h3><span class=\"mw-headline\" id=\"Planning_for_the_future\">Planning for the future</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=39\" title=\"Edit section: Planning for the future\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>After prioritizing the key activities the next step is to plan for the future. Here the evaluator asks program participants and program staff how they would like to improve the program in relation to the key activities listed. The objective is to create a thread of coherence whereby the mission generated (step 1) guides the stock take (step 2) which forms the basis for the plans for the future (step 3). Thus, in planning for the future specific goals are aligned with relevant key activities. In addition to this it is also important for program participants and program staff to identify possible forms of evidence (measurable indicators) which can be used to monitor progress towards specific goals. Goals must be related to the program's activities, talents, resources and scope of capability- in short the goals formulated must be realistic.\n</p><p>These three steps of empowerment evaluation produce the potential for a program to run more effectively and more in touch with the needs of the target population. Empowerment evaluation as a process which is facilitated by a skilled evaluator equips as well as empowers participants by providing them with a 'new' way of critically thinking and reflecting on programs. Furthermore, it empowers program participants and staff to recognize their own capacity to bring about program change through collective action.<sup id=\"cite_ref-F.D.M2_43-0\" class=\"reference\"><a href=\"#cite_note-F.D.M2-43\">&#91;43&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Transformative_Paradigm\">Transformative Paradigm</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=40\" title=\"Edit section: Transformative Paradigm\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The transformative paradigm is integral in incorporating social justice in evaluation. Donna Mertens, primary researcher in this field, states that the transformative paradigm, \"focuses primarily on viewpoints of marginalized groups and interrogating systemic power structures through mixed methods to further social justice and human rights\".<sup id=\"cite_ref-Mertens_(2012)_44-0\" class=\"reference\"><a href=\"#cite_note-Mertens_(2012)-44\">&#91;44&#93;</a></sup> The transformative paradigm arose after marginalized groups, who have historically been pushed to the side in evaluation, began to collaborate with scholars to advocate for social justice and human rights in evaluation.  The transformative paradigm introduces many different paradigms and lenses to the evaluation process, leading it to continually call into question the evaluation process.\n</p><p>Both the <a href=\"/wiki/American_Evaluation_Association\" title=\"American Evaluation Association\">American Evaluation Association</a> and <a href=\"/wiki/National_Association_of_Social_Workers\" title=\"National Association of Social Workers\">National Association of Social Workers</a> call attention to the ethical duty to possess <a href=\"/wiki/Cultural_competence\" class=\"mw-redirect\" title=\"Cultural competence\">cultural competence</a> when conducting evaluations.  Cultural competence in evaluation can be broadly defined as a systemic, response inquiry that is actively cognizant, understanding, and appreciative of the cultural context in which the evaluation takes place; that frames and articulates epistemology of the evaluation endeavor; that employs culturally and contextually appropriate methodology; and that uses stakeholder-generated, interpretive means to arrive at the results and further use of the findings.<sup id=\"cite_ref-SenGupta_45-0\" class=\"reference\"><a href=\"#cite_note-SenGupta-45\">&#91;45&#93;</a></sup>   Many health and evaluation leaders are careful to point out that cultural competence cannot be determined by a simple checklist, but rather it is an attribute that develops over time.  The root of cultural competency in evaluation is a genuine respect for communities being studied and openness to seek depth in understanding different cultural contexts, practices and paradigms of thinking.  This includes being creative and flexible to capture different cultural contexts, and heightened awareness of power differentials that exist in an evaluation context.  Important skills include: ability to build rapport across difference, gain the trust of the community members, and self-reflect and recognize one\u2019s own biases.<sup id=\"cite_ref-Endo_46-0\" class=\"reference\"><a href=\"#cite_note-Endo-46\">&#91;46&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Paradigms\">Paradigms</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=41\" title=\"Edit section: Paradigms\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The paradigms <a href=\"/wiki/Axiology\" title=\"Axiology\">axiology</a>, <a href=\"/wiki/Ontology\" title=\"Ontology\">ontology</a>, <a href=\"/wiki/Epistemology\" title=\"Epistemology\">epistemology</a>, and <a href=\"/wiki/Methodology\" title=\"Methodology\">methodology</a> are reflective of social justice practice in evaluation. These examples focus on addressing inequalities and injustices in society by promoting inclusion and equality in human rights.\n</p>\n<h4><span id=\"Axiology_.28Values_and_Value_Judgements.29\"></span><span class=\"mw-headline\" id=\"Axiology_(Values_and_Value_Judgements)\">Axiology (Values and Value Judgements)</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=42\" title=\"Edit section: Axiology (Values and Value Judgements)\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>The transformative paradigm\u2019s axiological assumption rests on four primary principles:<sup id=\"cite_ref-Mertens_(2012)_44-1\" class=\"reference\"><a href=\"#cite_note-Mertens_(2012)-44\">&#91;44&#93;</a></sup>\n</p>\n<ul><li>The importance of being culturally respectful</li>\n<li>The promotion of social justice</li>\n<li>The furtherance of human rights</li>\n<li>Addressing inequities</li></ul>\n<h4><span id=\"Ontology_.28Reality.29\"></span><span class=\"mw-headline\" id=\"Ontology_(Reality)\">Ontology (Reality)</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=43\" title=\"Edit section: Ontology (Reality)\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Differences in perspectives on  what is real are determined by diverse values and life experiences.  In turn these values and life experiences are often associated with differences in access to privilege, based on such characteristics as disability, gender, sexual identity, religion, race/ethnicity, national origins, political party, income level, are, language,  and immigration or refugee status.<sup id=\"cite_ref-Mertens_(2012)_44-2\" class=\"reference\"><a href=\"#cite_note-Mertens_(2012)-44\">&#91;44&#93;</a></sup>\n</p>\n<h4><span id=\"Epistemology_.28Knowledge.29\"></span><span class=\"mw-headline\" id=\"Epistemology_(Knowledge)\">Epistemology (Knowledge)</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=44\" title=\"Edit section: Epistemology (Knowledge)\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Knowledge is constructed within the context of power and privilege with consequences attached to which version of knowledge is given privilege.<sup id=\"cite_ref-Mertens_(2012)_44-3\" class=\"reference\"><a href=\"#cite_note-Mertens_(2012)-44\">&#91;44&#93;</a></sup>  \"Knowledge is socially and historically located within a complex cultural context\".<sup id=\"cite_ref-Mertens_2010_47-0\" class=\"reference\"><a href=\"#cite_note-Mertens_2010-47\">&#91;47&#93;</a></sup>\n</p>\n<h4><span id=\"Methodology_.28Systematic_Inquiry.29\"></span><span class=\"mw-headline\" id=\"Methodology_(Systematic_Inquiry)\">Methodology (Systematic Inquiry)</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=45\" title=\"Edit section: Methodology (Systematic Inquiry)\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Methodological decisions are aimed at determining the approach that will best facilitate use of the process and findings to enhance social justice; identify the systemic forces that support the status quo and those that will allow change to happen; and acknowledge the need for a critical and reflexive relationship between the evaluator and the stakeholders.<sup id=\"cite_ref-Mertens_(2012)_44-4\" class=\"reference\"><a href=\"#cite_note-Mertens_(2012)-44\">&#91;44&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Lenses\">Lenses</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=46\" title=\"Edit section: Lenses\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>While operating through social justice, it is imperative to be able to view the world through the lens of those who experience injustices. Critical Race Theory, Feminist Theory, and Queer/LGBTQ Theory are frameworks for how we think should think about providing justice for marginalized groups. These lenses create opportunity to make each theory priority in addressing inequality.\n</p>\n<h4><span class=\"mw-headline\" id=\"Critical_Race_Theory\">Critical Race Theory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=47\" title=\"Edit section: Critical Race Theory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p><a href=\"/wiki/Critical_Race_Theory\" class=\"mw-redirect\" title=\"Critical Race Theory\">Critical Race Theory</a>(CRT)is an extension of critical theory that is focused in inequities based on race and ethnicity.  Daniel Solorzano  describes the role of CRT as providing a framework to investigate and make visible those systemic aspects of society that allow the discriminatory and oppressive status quo of racism to continue.<sup id=\"cite_ref-Solorzano_48-0\" class=\"reference\"><a href=\"#cite_note-Solorzano-48\">&#91;48&#93;</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"Feminist_Theory\">Feminist Theory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=48\" title=\"Edit section: Feminist Theory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>The essence of <a href=\"/wiki/Feminist_theories\" class=\"mw-redirect\" title=\"Feminist theories\">feminist theories</a> is to \"expose the individual and institutional practices that have denied access to women and other oppressed groups and have ignored or devalued women\" <sup id=\"cite_ref-Brabeck_49-0\" class=\"reference\"><a href=\"#cite_note-Brabeck-49\">&#91;49&#93;</a></sup>\n</p>\n<h4><span id=\"Queer.2FLGBTQ_Theory\"></span><span class=\"mw-headline\" id=\"Queer/LGBTQ_Theory\">Queer/LGBTQ Theory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=49\" title=\"Edit section: Queer/LGBTQ Theory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p><a href=\"/wiki/Queer_theory\" title=\"Queer theory\">Queer/LGBTQ theorists</a> question the heterosexist bias that pervades society in terms of power over and discrimination toward sexual orientation minorities.  Because of the sensitivity of issues surrounding LGBTQ status, evaluators need to be aware of safe ways to protect such individuals\u2019 identities and ensure that discriminatory practices are brought to light in order to bring about a more just society.<sup id=\"cite_ref-Mertens_(2012)_44-5\" class=\"reference\"><a href=\"#cite_note-Mertens_(2012)-44\">&#91;44&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Government_requirements\">Government requirements</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=50\" title=\"Edit section: Government requirements\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Given the Federal budget deficit, the Obama Administration moved to apply an \"evidence-based approach\" to government spending, including rigorous methods of program evaluation.  The President's 2011 Budget earmarked funding for 19 government program evaluations for agencies such  as the Department of Education and the United States Agency for International Development (USAID).  An inter-agency group delivers the goal of increasing transparency and accountability by creating effective evaluation networks and drawing on best practices.<sup id=\"cite_ref-50\" class=\"reference\"><a href=\"#cite_note-50\">&#91;50&#93;</a></sup>\nA six-step framework for conducting evaluation of public health programs, published by the <a href=\"/wiki/Centers_for_Disease_Control_and_Prevention\" title=\"Centers for Disease Control and Prevention\">Centers for Disease Control and Prevention</a> (CDC), initially increased the emphasis on program evaluation of government programs in the US.  The framework is as follows:\n</p>\n<ol><li>Engage <a href=\"/wiki/Project_stakeholder\" title=\"Project stakeholder\">stakeholders</a></li>\n<li>Describe the program.</li>\n<li>Focus the evaluation.</li>\n<li>Gather credible evidence.</li>\n<li>Justify conclusions.</li>\n<li>Ensure use and share lessons learned.</li></ol>\n<h2><span class=\"mw-headline\" id=\"Types_of_Evaluation\">Types of Evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=51\" title=\"Edit section: Types of Evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>There are many different approaches to program evaluation. Each serves a different purpose. \n</p>\n<ul><li><a href=\"/w/index.php?title=Utilization-Focused_Evaluation&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Utilization-Focused Evaluation (page does not exist)\">Utilization-Focused Evaluation</a></li>\n<li>CIPP Model of evaluation</li>\n<li>Formative Evaluation</li>\n<li>Summative Evaluation</li>\n<li>Developmental Evaluation</li>\n<li>Principles-Focused Evaluation</li>\n<li>Theory-Driven Evaluation</li>\n<li>Realist-Driven Evaluation</li></ul>\n<h2><span class=\"mw-headline\" id=\"CIPP_Model_of_evaluation\">CIPP Model of evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=52\" title=\"Edit section: CIPP Model of evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/CIPP_evaluation_model\" title=\"CIPP evaluation model\">CIPP evaluation model</a></div>\n<h3><span class=\"mw-headline\" id=\"History_of_the_CIPP_model\">History of the CIPP model</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=53\" title=\"Edit section: History of the CIPP model\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The CIPP model of evaluation was developed by Daniel Stufflebeam and colleagues in the 1960s.CIPP is an acronym for Context, Input, Process and Product. CIPP is an evaluation model that requires the evaluation of <b>context</b>, <b>input</b>, <b>process</b> and <b>product</b> in judging a programme\u2019s value. CIPP is a decision-focused approach to evaluation and emphasises the systematic provision of information for <a href=\"/wiki/Programme_management\" class=\"mw-redirect\" title=\"Programme management\">programme management</a> and operation.<sup id=\"cite_ref-Robinson_51-0\" class=\"reference\"><a href=\"#cite_note-Robinson-51\">&#91;51&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"CIPP_model\">CIPP model</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=54\" title=\"Edit section: CIPP model\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The CIPP framework was developed as a means of linking evaluation with programme <a href=\"/wiki/Decision-making\" title=\"Decision-making\">decision-making</a>. It aims to provide an analytic and rational basis for programme decision-making, based on a cycle of planning, structuring, implementing and reviewing and revising decisions, each examined through a different aspect of evaluation \u2013context, input, process and product evaluation.<sup id=\"cite_ref-Robinson_51-1\" class=\"reference\"><a href=\"#cite_note-Robinson-51\">&#91;51&#93;</a></sup>\n</p><p>The CIPP model is an attempt to make evaluation directly relevant to the needs of decision-makers during the phases and activities of a programme.<sup id=\"cite_ref-Robinson_51-2\" class=\"reference\"><a href=\"#cite_note-Robinson-51\">&#91;51&#93;</a></sup> Stufflebeam\u2019s context, <a href=\"/wiki/Factor_of_production\" class=\"mw-redirect\" title=\"Factor of production\">input</a>, process, and <a href=\"/wiki/Product_breakdown_structure\" title=\"Product breakdown structure\">product</a> (CIPP) evaluation model is recommended as a framework to systematically guide the conception, design, <a href=\"/wiki/Implementation\" title=\"Implementation\">implementation</a>, and assessment of service-learning projects, and provide feedback and judgment of the project\u2019s effectiveness for continuous improvement.<sup id=\"cite_ref-Robinson_51-3\" class=\"reference\"><a href=\"#cite_note-Robinson-51\">&#91;51&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Four_aspects_of_CIPP_evaluation\">Four aspects of CIPP evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=55\" title=\"Edit section: Four aspects of CIPP evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>These aspects are context, inputs, process, and product. These four aspects of CIPP evaluation assist a decision-maker to answer four basic questions:\n</p>\n<ul><li>What should we do?</li></ul>\n<p>This involves collecting and analysing needs assessment data to determine goals, priorities and objectives. For example, a context evaluation of a literacy program might involve an analysis of the existing objectives of the literacy programme, literacy achievement test scores, staff concerns (general and particular), literacy policies and plans and community concerns, perceptions or attitudes and needs.<sup id=\"cite_ref-Robinson_51-4\" class=\"reference\"><a href=\"#cite_note-Robinson-51\">&#91;51&#93;</a></sup>\n</p>\n<ul><li>How should we do it?</li></ul>\n<p>This involves the steps and resources needed to meet the new goals and objectives and might include identifying successful external programs and materials as well as gathering information.<sup id=\"cite_ref-Robinson_51-5\" class=\"reference\"><a href=\"#cite_note-Robinson-51\">&#91;51&#93;</a></sup>\n</p>\n<ul><li>Are we doing it as planned?</li></ul>\n<p>This provides decision-makers with information about how well the programme is being implemented. By continuously monitoring the program, decision-makers learn such things as how well it is following the plans and guidelines, conflicts arising, staff support and morale, strengths and weaknesses of materials, delivery and budgeting problems.<sup id=\"cite_ref-Robinson_51-6\" class=\"reference\"><a href=\"#cite_note-Robinson-51\">&#91;51&#93;</a></sup>\n</p>\n<ul><li>Did the programme work?</li></ul>\n<p>By measuring the actual outcomes and comparing them to the anticipated outcomes, decision-makers are better able to decide if the program should be continued, modified, or dropped altogether. This is the essence of product evaluation.<sup id=\"cite_ref-Robinson_51-7\" class=\"reference\"><a href=\"#cite_note-Robinson-51\">&#91;51&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Using_CIPP_in_the_different_stages_of_the_evaluation\">Using CIPP in the different stages of the evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=56\" title=\"Edit section: Using CIPP in the different stages of the evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The CIPP model is unique as an <a href=\"/wiki/Evaluation\" title=\"Evaluation\">evaluation</a> guide as it allows evaluators to evaluate the program at different stages, namely: before the program commences by helping evaluators to assess the need and at the end of the program to assess whether or not the program had an effect.\n</p><p>CIPP model allows you to ask formative questions at the beginning of the program, then later gives you a guide of how to evaluate the programs impact by allowing you to ask summative questions on all aspects of the program.\n</p>\n<ul><li><b>Context</b>: What needs to be done? Vs. Were important needs addressed?</li>\n<li><b>Input</b>: How should it be done? Vs. Was a defensible design employed?</li>\n<li><b>Process</b>: Is it being done? Vs. Was the design well executed?</li>\n<li><b>Product</b>: Is it succeeding? Vs. Did the effort succeed?</li></ul>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=57\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Impact_assessment\" title=\"Impact assessment\">Impact assessment</a></li>\n<li><a href=\"/wiki/Impact_evaluation\" title=\"Impact evaluation\">Impact evaluation</a></li>\n<li><a href=\"/wiki/Participatory_Impact_Pathways_Analysis\" class=\"mw-redirect\" title=\"Participatory Impact Pathways Analysis\">Participatory Impact Pathways Analysis</a></li>\n<li><a href=\"/wiki/Policy_analysis\" title=\"Policy analysis\">Policy analysis</a></li>\n<li><a href=\"/wiki/Policy_studies\" title=\"Policy studies\">Policy studies</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=58\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\n<ol class=\"references\">\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\">Administration for Children and Families (2010) <i><a rel=\"nofollow\" class=\"external text\" href=\"http://www.acf.hhs.gov/programs/opre/other_resrch/pm_guide_eval/index.html\">The Program Manager's Guide to Evaluation</a>. Chapter 2: What is program evaluation?</i>.</span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Shackman, Gene. <a rel=\"nofollow\" class=\"external text\" href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3060080\">\"What Is Program Evaluation: A Beginner's Guide\"</a>. The Global Social Change Research Project<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">April 8,</span> 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=What+Is+Program+Evaluation%3A+A+Beginner%27s+Guide&amp;rft.pub=The+Global+Social+Change+Research+Project&amp;rft.aulast=Shackman&amp;rft.aufirst=Gene&amp;rft_id=https%3A%2F%2Fpapers.ssrn.com%2Fsol3%2Fpapers.cfm%3Fabstract_id%3D3060080&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.cdc.gov/eval/strongevaluations/index.htm\">\"Hints for Conducting Strong Evaluations\"</a>. <i>Program Evaluation</i>. CDC - Office of the Associate Director for Program - Program Evaluation<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">April 8,</span> 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Program+Evaluation&amp;rft.atitle=Hints+for+Conducting+Strong+Evaluations&amp;rft_id=https%3A%2F%2Fwww.cdc.gov%2Feval%2Fstrongevaluations%2Findex.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Shadish, W. R., Cook, T. D., &amp; Leviton, L. C. (1991). <i>Foundations of program evaluation: Theories of practice</i>. Newbury Park, CA: Sage.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Foundations+of+program+evaluation%3A+Theories+of+practice.&amp;rft.place=Newbury+Park%2C+CA&amp;rft.pub=Sage.&amp;rft.date=1991&amp;rft.au=Shadish%2C+W.+R.%2C+Cook%2C+T.+D.%2C+%26+Leviton%2C+L.+C.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.dol.gov/oasam/programs/history/dolchp06.htm\">\"U.S. Department of Labor -- Brief History of DOL - Eras of the New Frontier and the Great Society, 1961-1969\"</a>. dol.gov.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=U.S.+Department+of+Labor+--+Brief+History+of+DOL+-+Eras+of+the+New+Frontier+and+the+Great+Society%2C+1961-1969&amp;rft.pub=dol.gov&amp;rft_id=http%3A%2F%2Fwww.dol.gov%2Foasam%2Fprograms%2Fhistory%2Fdolchp06.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.archives.gov/research/guide-fed-records/groups/051.html\">National Archives, Records of the Office of Management and Budget (1995) <i>51.8.8 Records of the Office of Program Evaluation</i></a></span>\n</li>\n<li id=\"cite_note-Rossi-7\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Rossi_7-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-7\"><sup><i><b>h</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-8\"><sup><i><b>i</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-9\"><sup><i><b>j</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-10\"><sup><i><b>k</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-11\"><sup><i><b>l</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-12\"><sup><i><b>m</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-13\"><sup><i><b>n</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-14\"><sup><i><b>o</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-15\"><sup><i><b>p</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-16\"><sup><i><b>q</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-17\"><sup><i><b>r</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-18\"><sup><i><b>s</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-19\"><sup><i><b>t</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-20\"><sup><i><b>u</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-21\"><sup><i><b>v</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-22\"><sup><i><b>w</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-23\"><sup><i><b>x</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-24\"><sup><i><b>y</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-25\"><sup><i><b>z</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-26\"><sup><i><b>aa</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-27\"><sup><i><b>ab</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-28\"><sup><i><b>ac</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-29\"><sup><i><b>ad</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-30\"><sup><i><b>ae</b></i></sup></a> <a href=\"#cite_ref-Rossi_7-31\"><sup><i><b>af</b></i></sup></a></span> <span class=\"reference-text\">Rossi, P. Lipsey, M. W., &amp; Freeman, H.E. (2004). <i>Evaluation: A systematic approach</i> (7th ed.). Thousand Oaks, CA: Sage.</span>\n</li>\n<li id=\"cite_note-Barbazette-8\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Barbazette_8-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Barbazette_8-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Barbazette_8-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Barbazette, J. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://media.wiley.com/product_data/excerpt/57/07879752/0787975257.pdf\">\"What is needs assessment?\"</a> <span style=\"font-size:85%;\">(PDF)</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=What+is+needs+assessment%3F&amp;rft.date=2006&amp;rft.aulast=Barbazette&amp;rft.aufirst=J.&amp;rft_id=http%3A%2F%2Fmedia.wiley.com%2Fproduct_data%2Fexcerpt%2F57%2F07879752%2F0787975257.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Rouda, R. H.; Kusy, M. E. (1995). <a rel=\"nofollow\" class=\"external text\" href=\"http://alumnus.caltech.edu/~rouda/T2_NA.html#website=alumnus.caltech.edu\">\"Needs assessment: The first step\"</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Needs+assessment%3A+The+first+step&amp;rft.date=1995&amp;rft.aulast=Rouda&amp;rft.aufirst=R.+H.&amp;rft.au=Kusy%2C+M.+E.&amp;rft_id=http%3A%2F%2Falumnus.caltech.edu%2F~rouda%2FT2_NA.html%23website%3Dalumnus.caltech.edu&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\">Centers for Disease Control and Prevention.  Framework for Program Evaluation in Public Health. MMWR 1999;48(No. RR-11).</span>\n</li>\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\">Van der Riet, M. (2009). 'The production of context: using activity theory to understand behaviour change in response to HIV and AIDS.' Unpublished doctoral dissertation. University of KwaZulu-Natal, Pietermaritzburg.</span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\">Eveland, JD. (1986) \"Small Business Innovation Research Programs: Solutions Seeking Problems\". In D. Gray, T. Solomon, and W. Hetzner (eds.), Technological Innovation: Strategies for a New Partnership. Amsterdam: North-Holland</span>\n</li>\n<li id=\"cite_note-McLaughlin,_J._A._1999-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-McLaughlin,_J._A._1999_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-McLaughlin,_J._A._1999_13-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">McLaughlin, J. A., &amp; Jordan, G. B. (1999). Logic models: a tool for telling your programs performance story. Evaluation and program planning, 22(1), 65-72.</span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\">Eveland, JD (1986) \"Diffusion, Technology Transfer, and Implementation: Thinking and Talking about Change\". Knowledge. 8(2):303-322.</span>\n</li>\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\">The Intermediary Development Series. <i> Measuring Outcomes. Dare Mighty Things, INC</i>.</span>\n</li>\n<li id=\"cite_note-Mouton,_J_2009-16\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Mouton,_J_2009_16-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Mouton,_J_2009_16-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Mouton, J (2009). \"Assessing the impact of complex social interventions\". <i>Journal of Public Administration</i>. <b>44</b>: 849\u2013865.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Public+Administration&amp;rft.atitle=Assessing+the+impact+of+complex+social+interventions&amp;rft.volume=44&amp;rft.pages=849-865&amp;rft.date=2009&amp;rft.aulast=Mouton&amp;rft.aufirst=J&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Shahab, Sina; Clinch, J Peter; O\u2019Neill, Eoin (21 July 2017). \"Impact-based planning evaluation: Advancing normative criteria for policy analysis\". <i>Environment and Planning B: Urban Analytics and City Science</i>: 239980831772044. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1177/2399808317720446\">10.1177/2399808317720446</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Environment+and+Planning+B%3A+Urban+Analytics+and+City+Science&amp;rft.atitle=Impact-based+planning+evaluation%3A+Advancing+normative+criteria+for+policy+analysis&amp;rft.pages=239980831772044&amp;rft.date=2017-07-21&amp;rft_id=info%3Adoi%2F10.1177%2F2399808317720446&amp;rft.aulast=Shahab&amp;rft.aufirst=Sina&amp;rft.au=Clinch%2C+J+Peter&amp;rft.au=O%E2%80%99Neill%2C+Eoin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\">Delbert Charles Miller, Neil J. Salkind (2002) <i>Handbook of Research Design &amp; Social Measurement. Edition: 6, revised. Published by SAGE,</i>.</span>\n</li>\n<li id=\"cite_note-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-19\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Center for Disease Control. <a rel=\"nofollow\" class=\"external text\" href=\"https://www.cdc.gov/eval/framework/index.htm\">\"Evaluation Framework\"</a>. CDC<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">20 September</span> 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Evaluation+Framework&amp;rft.pub=CDC&amp;rft.au=Center+for+Disease+Control&amp;rft_id=https%3A%2F%2Fwww.cdc.gov%2Feval%2Fframework%2Findex.htm&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Kania, John; Kramer, Mark. <a rel=\"nofollow\" class=\"external text\" href=\"http://www.ssireview.org/articles/entry/collective_impact\">\"Collective Impact\"</a>. <i>Stanford Social Innovation Review</i>. Stanford Center on Philanthropy and Civil Society at Stanford University<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">19 September</span> 2014</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Stanford+Social+Innovation+Review&amp;rft.atitle=Collective+Impact&amp;rft.aulast=Kania&amp;rft.aufirst=John&amp;rft.au=Kramer%2C+Mark&amp;rft_id=http%3A%2F%2Fwww.ssireview.org%2Farticles%2Fentry%2Fcollective_impact&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Preskill, Hallie; Parkhurst, Marcie; Juster, Jennifer Splansky. <a rel=\"nofollow\" class=\"external text\" href=\"http://www.fsg.org/Portals/0/Uploads/Documents/PDF/Evaluating_Collective_Impact_Assessing_Progress_2.pdf\">\"Guide to Evaluating Collective Impact\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>www.fsg.org</i>. FSG, Inc<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2014-09-19</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.fsg.org&amp;rft.atitle=Guide+to+Evaluating+Collective+Impact&amp;rft.aulast=Preskill&amp;rft.aufirst=Hallie&amp;rft.au=Parkhurst%2C+Marcie&amp;rft.au=Juster%2C+Jennifer+Splansky&amp;rft_id=http%3A%2F%2Fwww.fsg.org%2FPortals%2F0%2FUploads%2FDocuments%2FPDF%2FEvaluating_Collective_Impact_Assessing_Progress_2.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Patton, Michael. <a rel=\"nofollow\" class=\"external text\" href=\"http://tei.gwu.edu/evaluation-approaches-and-techniques#developmental-evaluation\">\"Evaluation Approaches and Techniques\"</a>. <i>The Evaluators' Institute</i>. George Washington University<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2014-09-19</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=The+Evaluators%27+Institute&amp;rft.atitle=Evaluation+Approaches+and+Techniques&amp;rft.aulast=Patton&amp;rft.aufirst=Michael&amp;rft_id=http%3A%2F%2Ftei.gwu.edu%2Fevaluation-approaches-and-techniques%23developmental-evaluation&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-23\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-23\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://evaluationtoolbox.net.au/index.php?option=com_content&amp;view=article&amp;id=24:formative-evaluation&amp;catid=17:formative-evaluation&amp;Itemid=125\">\"Formative Evaluation\"</a>. <i>Community Sustainability Engagement Evaluation Toolbox</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">19 September</span> 2014</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Community+Sustainability+Engagement+Evaluation+Toolbox&amp;rft.atitle=Formative+Evaluation&amp;rft_id=http%3A%2F%2Fevaluationtoolbox.net.au%2Findex.php%3Foption%3Dcom_content%26view%3Darticle%26id%3D24%3Aformative-evaluation%26catid%3D17%3Aformative-evaluation%26Itemid%3D125&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://evaluationtoolbox.net.au/index.php?option=com_content&amp;view=article&amp;id=40&amp;Itemid=126\">\"Summative Evaluation\"</a>. <i>Community Sustainability Engagement Evaluation Toolbox</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">19 September</span> 2014</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Community+Sustainability+Engagement+Evaluation+Toolbox&amp;rft.atitle=Summative+Evaluation&amp;rft_id=http%3A%2F%2Fevaluationtoolbox.net.au%2Findex.php%3Foption%3Dcom_content%26view%3Darticle%26id%3D40%26Itemid%3D126&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-25\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Taylor-Powell, Ellen; Steele, Sarah; Douglah, Mohammad. <a rel=\"nofollow\" class=\"external text\" href=\"http://learningstore.uwex.edu/assets/pdfs/g3658-1.pdf\">\"Planning a Program Evaluation\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>University of Wisconsin Extension</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 20,</span> 2014</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=University+of+Wisconsin+Extension&amp;rft.atitle=Planning+a+Program+Evaluation&amp;rft.aulast=Taylor-Powell&amp;rft.aufirst=Ellen&amp;rft.au=Steele%2C+Sarah&amp;rft.au=Douglah%2C+Mohammad&amp;rft_id=http%3A%2F%2Flearningstore.uwex.edu%2Fassets%2Fpdfs%2Fg3658-1.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Bamberger-26\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Bamberger_26-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Bamberger_26-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Bamberger_26-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Bamberger_26-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-Bamberger_26-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-Bamberger_26-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-Bamberger_26-6\"><sup><i><b>g</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Bamberger, M. (2004). \"Shoestring Evaluation: Designing Impact Evaluations under Budget, Time and Data Constraints\". <i>American Journal of Evaluation</i>. <b>25</b>: 5\u201337. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1177/109821400402500102\">10.1177/109821400402500102</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=American+Journal+of+Evaluation&amp;rft.atitle=Shoestring+Evaluation%3A+Designing+Impact+Evaluations+under+Budget%2C+Time+and+Data+Constraints&amp;rft.volume=25&amp;rft.pages=5-37&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1177%2F109821400402500102&amp;rft.aulast=Bamberger&amp;rft.aufirst=M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-F.H.J-27\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-F.H.J_27-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-F.H.J_27-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Jacobs, F. H. (2003). \"Child and Family Program Evaluation: Learning to Enjoy Complexity\". <i>Applied Developmental Science</i>. <b>7</b> (2): 62\u201375. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1207/S1532480XADS0702_3\">10.1207/S1532480XADS0702_3</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Applied+Developmental+Science&amp;rft.atitle=Child+and+Family+Program+Evaluation%3A+Learning+to+Enjoy+Complexity&amp;rft.volume=7&amp;rft.issue=2&amp;rft.pages=62-75&amp;rft.date=2003&amp;rft_id=info%3Adoi%2F10.1207%2FS1532480XADS0702_3&amp;rft.aulast=Jacobs&amp;rft.aufirst=F.+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-W&amp;G-28\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-W&amp;G_28-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-W&amp;G_28-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">Weiss, H.B., &amp; Greene, J.C. (1992). An empowerment partnership for family support and education programs and evaluations. Family Science Review, 5(1,2): 131-148.</span>\n</li>\n<li id=\"cite_note-NPEN-29\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-NPEN_29-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-NPEN_29-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.npen.org/parent-educators/evaluating/approach.html\">\"Five-tiered approach to evaluation\"</a>. National Parenting Education Network (n.d.)<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 18,</span> 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Five-tiered+approach+to+evaluation&amp;rft.pub=National+Parenting+Education+Network+%28n.d.%29&amp;rft_id=http%3A%2F%2Fwww.npen.org%2Fparent-educators%2Fevaluating%2Fapproach.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-B&amp;D-30\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-B&amp;D_30-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-B&amp;D_30-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation web\">Bailey, S. J.; Deen, M. Y. (2002). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.joe.org/joe/2002april/iw1.php\">\"A Framework for introducing program evaluation to extension faculty and staff\"</a>. <i>Journal of Extension</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">September 18,</span> 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Journal+of+Extension&amp;rft.atitle=A+Framework+for+introducing+program+evaluation+to+extension+faculty+and+staff&amp;rft.date=2002&amp;rft.aulast=Bailey&amp;rft.aufirst=S.+J.&amp;rft.au=Deen%2C+M.+Y.&amp;rft_id=http%3A%2F%2Fwww.joe.org%2Fjoe%2F2002april%2Fiw1.php&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-CYF-31\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-CYF_31-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-CYF_31-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-CYF_31-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-CYF_31-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://ag.arizona.edu/sfcs/cyfernet/cyfar/5_apprch.pdf\">\"Five-tiered approach to program evaluation\"</a> <span style=\"font-size:85%;\">(PdF)</span>. <i>ag.arizona.edu</i><span class=\"reference-accessdate\">. Retrieved September 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=ag.arizona.edu&amp;rft.atitle=Five-tiered+approach+to+program+evaluation&amp;rft_id=http%3A%2F%2Fag.arizona.edu%2Fsfcs%2Fcyfernet%2Fcyfar%2F5_apprch.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> <span style=\"font-size:100%\" class=\"error citation-comment\">Check date values in: <code style=\"color:inherit; border:inherit; padding:inherit;\">&#124;accessdate=</code> (<a href=\"/wiki/Help:CS1_errors#bad_date\" title=\"Help:CS1 errors\">help</a>)</span></span>\n</li>\n<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Bamberger, M. (2000). \"The Evaluation of International Development Programs: A View from the Front\". <i>American Journal of Evaluation</i>. <b>21</b> (1): 95\u2013102. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1177/109821400002100108\">10.1177/109821400002100108</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=American+Journal+of+Evaluation&amp;rft.atitle=The+Evaluation+of+International+Development+Programs%3A+A+View+from+the+Front&amp;rft.volume=21&amp;rft.issue=1&amp;rft.pages=95-102&amp;rft.date=2000&amp;rft_id=info%3Adoi%2F10.1177%2F109821400002100108&amp;rft.aulast=Bamberger&amp;rft.aufirst=M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Smith,_1990-33\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Smith,_1990_33-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Smith,_1990_33-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Smith, T. (1990). \"Policy evaluation in third world countries: some issues and problems\". <i>Asian Journal of Public Administration</i>. <b>12</b>: 55\u201368. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1080/02598272.1990.10800228\">10.1080/02598272.1990.10800228</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Asian+Journal+of+Public+Administration&amp;rft.atitle=Policy+evaluation+in+third+world+countries%3A+some+issues+and+problems&amp;rft.volume=12&amp;rft.pages=55-68&amp;rft.date=1990&amp;rft_id=info%3Adoi%2F10.1080%2F02598272.1990.10800228&amp;rft.aulast=Smith&amp;rft.aufirst=T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Ebbutt,_1998-34\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Ebbutt,_1998_34-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Ebbutt,_1998_34-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Ebbutt,_1998_34-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Ebbutt,_1998_34-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-Ebbutt,_1998_34-4\"><sup><i><b>e</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Ebbutt, D. (1998). \"Evaluation of projects in the developing world: some cultural and methodological issues\". <i>International Journal of Educational Development</i>. <b>18</b> (5): 415\u2013424. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/S0738-0593%2898%2900038-8\">10.1016/S0738-0593(98)00038-8</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Educational+Development&amp;rft.atitle=Evaluation+of+projects+in+the+developing+world%3A+some+cultural+and+methodological+issues&amp;rft.volume=18&amp;rft.issue=5&amp;rft.pages=415-424&amp;rft.date=1998&amp;rft_id=info%3Adoi%2F10.1016%2FS0738-0593%2898%2900038-8&amp;rft.aulast=Ebbutt&amp;rft.aufirst=D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Bulmer_&amp;_Warwick,_1993-35\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Bulmer_&amp;_Warwick,_1993_35-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Bulmer_&amp;_Warwick,_1993_35-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Bulmer_&amp;_Warwick,_1993_35-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Bulmer_&amp;_Warwick,_1993_35-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-Bulmer_&amp;_Warwick,_1993_35-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-Bulmer_&amp;_Warwick,_1993_35-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-Bulmer_&amp;_Warwick,_1993_35-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-Bulmer_&amp;_Warwick,_1993_35-7\"><sup><i><b>h</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Bulmer, M.; Warwick, D. (1993). <i>Social research in developing countries: surveys and censuses in the Third World</i>. London: Routledge. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/0-471-10352-7\" title=\"Special:BookSources/0-471-10352-7\">0-471-10352-7</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Social+research+in+developing+countries%3A+surveys+and+censuses+in+the+Third+World&amp;rft.place=London&amp;rft.pub=Routledge&amp;rft.date=1993&amp;rft.isbn=0-471-10352-7&amp;rft.aulast=Bulmer&amp;rft.aufirst=M.&amp;rft.au=Warwick%2C+D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-36\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-36\">^</a></b></span> <span class=\"reference-text\">Potter, C. (2006). Program Evaluation. In M. Terre Blanche, K. Durrheim &amp; D. Painter (Eds.), <i>Research in practice: Applied methods for the social sciences</i> (2nd ed.) (pp. 410-428). Cape Town: UCT Press.</span>\n</li>\n<li id=\"cite_note-37\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-37\">^</a></b></span> <span class=\"reference-text\">Rossi, P., Lipsey, M.W., &amp; Freeman, H.E. (2004). <i>Evaluation: a systematic approach</i> (7th ed.). Thousand Oaks: Sage.</span>\n</li>\n<li id=\"cite_note-38\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-38\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Gill, Andrew. <a rel=\"nofollow\" class=\"external text\" href=\"http://www.ppic.org/main/publication.asp?i=322\">\"Evaluating Academic Programs in California's Community Colleges\"</a>. PPIC.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Evaluating+Academic+Programs+in+California%27s+Community+Colleges&amp;rft.pub=PPIC&amp;rft.aulast=Gill&amp;rft.aufirst=Andrew&amp;rft_id=http%3A%2F%2Fwww.ppic.org%2Fmain%2Fpublication.asp%3Fi%3D322&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-39\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-39\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Woolcock, Michael. <a rel=\"nofollow\" class=\"external text\" href=\"http://siteresources.worldbank.org/SOCIALANALYSIS/1104890-1120158274352/20566665/Integratingqualitativeandquantapproachesraoandwoolcock.pdf\">\"Integrating Qualitative and Quantitative Approaches in Program Evaluation\"</a> <span style=\"font-size:85%;\">(PDF)</span><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">21 September</span> 2011</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Integrating+Qualitative+and+Quantitative+Approaches+in+Program+Evaluation&amp;rft.aulast=Woolcock&amp;rft.aufirst=Michael&amp;rft_id=http%3A%2F%2Fsiteresources.worldbank.org%2FSOCIALANALYSIS%2F1104890-1120158274352%2F20566665%2FIntegratingqualitativeandquantapproachesraoandwoolcock.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-40\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-40\">^</a></b></span> <span class=\"reference-text\">Weiss, C.H. (1999). Research-policy linkages: How much influence does social science research have? World Social Science Report, pp. 194-205.</span>\n</li>\n<li id=\"cite_note-41\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-41\">^</a></b></span> <span class=\"reference-text\">Louw, J. (1999). Improving practice through evaluation. In D. Donald, A. Dawes &amp; J. Louw (Eds.), <i>Addressing childhood adversity</i> (pp. 60-73). Cape Town: David Philip.</span>\n</li>\n<li id=\"cite_note-F.D.M-42\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-F.D.M_42-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-F.D.M_42-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Fetterman, D.M. (2002). \"Empowerment evaluation: Building communities of practice and a culture of learning\". <i>American Journal of Community Psychology</i>. <b>30</b> (1): 81\u2013102. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1023/a%3A1014324218388\">10.1023/a:1014324218388</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=American+Journal+of+Community+Psychology&amp;rft.atitle=Empowerment+evaluation%3A+Building+communities+of+practice+and+a+culture+of+learning&amp;rft.volume=30&amp;rft.issue=1&amp;rft.pages=81-102&amp;rft.date=2002&amp;rft_id=info%3Adoi%2F10.1023%2Fa%3A1014324218388&amp;rft.aulast=Fetterman&amp;rft.aufirst=D.M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-F.D.M2-43\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-F.D.M2_43-0\">^</a></b></span> <span class=\"reference-text\">Fetterman, D.M. (2005). Empowerment evaluation principles in action: Assessing levels of commitment. In D.M. Fetterman &amp; A. Vandersman (Eds.), <i>Empowerment evaluation principles in practice</i>(pp.27-41). New York: Guilford Press</span>\n</li>\n<li id=\"cite_note-Mertens_(2012)-44\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Mertens_(2012)_44-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Mertens_(2012)_44-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Mertens_(2012)_44-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Mertens_(2012)_44-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-Mertens_(2012)_44-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-Mertens_(2012)_44-5\"><sup><i><b>f</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Mertens, D and Wilson, A. (2012). <i>Program Evaluation Theory and Practice: A Comprehensive Guide</i>. New York, NY: The Guilford Press. pp.&#160;168\u2013180.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Program+Evaluation+Theory+and+Practice%3A+A+Comprehensive+Guide&amp;rft.place=New+York%2C+NY&amp;rft.pages=168-180&amp;rft.pub=The+Guilford+Press&amp;rft.date=2012&amp;rft.aulast=Mertens&amp;rft.aufirst=D+and+Wilson%2C+A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-SenGupta-45\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-SenGupta_45-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">SenGupta, S., Hopson, R., &amp; Thompson-Robinson, M. (2004). <i>Cultural competence in evaluation:an overview. New Directions in Evaluation, 102</i>. pp.&#160;5\u201319.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Cultural+competence+in+evaluation%3Aan+overview.+New+Directions+in+Evaluation%2C+102.&amp;rft.pages=5-19&amp;rft.date=2004&amp;rft.aulast=SenGupta&amp;rft.aufirst=S.%2C+Hopson%2C+R.%2C+%26+Thompson-Robinson%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Endo-46\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Endo_46-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Endo, T., Joh, T., &amp; Yu, H. (2003). <i>Voices from the field: Health evaluation leaders in multicultural evaluation</i>. Oakland, CA: Policy Research Associates. p.&#160;5.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Voices+from+the+field%3A+Health+evaluation+leaders+in+multicultural+evaluation&amp;rft.place=Oakland%2C+CA&amp;rft.pages=5&amp;rft.pub=Policy+Research+Associates&amp;rft.date=2003&amp;rft.aulast=Endo&amp;rft.aufirst=T.%2C+Joh%2C+T.%2C+%26+Yu%2C+H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Mertens_2010-47\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Mertens_2010_47-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Mertens, D., Yamashita, M. (2010). <i>Mission statement for the American Evaluation Association's Topical Interest Group: Mixed Methods in Evaluation</i>. Washington DC. p.&#160;48.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Mission+statement+for+the+American+Evaluation+Association%27s+Topical+Interest+Group%3A+Mixed+Methods+in+Evaluation.&amp;rft.place=Washington+DC&amp;rft.pages=48&amp;rft.date=2010&amp;rft.aulast=Mertens&amp;rft.aufirst=D.%2C+Yamashita%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Solorzano-48\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Solorzano_48-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Solorzano, D. (1997). \"Images and Words that wound: Critical race theory, racial stereotyping and teacher education\". <i>Teacher Education Quarterly</i>. <b>24</b>: 5\u201319.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Teacher+Education+Quarterly&amp;rft.atitle=Images+and+Words+that+wound%3A+Critical+race+theory%2C+racial+stereotyping+and+teacher+education.&amp;rft.volume=24&amp;rft.pages=5-19&amp;rft.date=1997&amp;rft.aulast=Solorzano&amp;rft.aufirst=D.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Brabeck-49\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Brabeck_49-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Brabeck, M. &amp; Brabeck, K. (2009). <i>Feminist Perspective on Research Ethics</i>. Thousand Oaks, CA: Sage. p.&#160;39.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Feminist+Perspective+on+Research+Ethics&amp;rft.place=Thousand+Oaks%2C+CA&amp;rft.pages=39&amp;rft.pub=Sage&amp;rft.date=2009&amp;rft.aulast=Brabeck&amp;rft.aufirst=M.+%26+Brabeck%2C+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-50\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-50\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.whitehouse.gov/sites/default/files/omb/performance/chapter8-2012.pdf\">\"Program Evaluation\"</a> <span style=\"font-size:85%;\">(PDF)</span>. White House<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">20 September</span> 2011</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Program+Evaluation&amp;rft.pub=White+House&amp;rft_id=http%3A%2F%2Fwww.whitehouse.gov%2Fsites%2Fdefault%2Ffiles%2Fomb%2Fperformance%2Fchapter8-2012.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Robinson-51\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Robinson_51-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Robinson_51-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Robinson_51-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Robinson_51-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-Robinson_51-4\"><sup><i><b>e</b></i></sup></a> <a href=\"#cite_ref-Robinson_51-5\"><sup><i><b>f</b></i></sup></a> <a href=\"#cite_ref-Robinson_51-6\"><sup><i><b>g</b></i></sup></a> <a href=\"#cite_ref-Robinson_51-7\"><sup><i><b>h</b></i></sup></a></span> <span class=\"reference-text\">Robinson, B. (2002).The CIPP has its formation from the earlier stages where there were no paragraphs or any acronyms for any product or stanzas. The CIPP approach to evaluation.Collit project: A background note from Bernadette</span>\n</li>\n</ol></div>\n<h2><span class=\"mw-headline\" id=\"Further_reading\">Further reading</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=59\" title=\"Edit section: Further reading\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<style data-mw-deduplicate=\"TemplateStyles:r853264625\">.mw-parser-output .refbegin{font-size:90%;margin-bottom:0.5em}.mw-parser-output .refbegin-hanging-indents>ul{list-style-type:none;margin-left:0}.mw-parser-output .refbegin-hanging-indents>ul>li,.mw-parser-output .refbegin-hanging-indents>dl>dd{margin-left:0;padding-left:3.2em;text-indent:-3.2em;list-style:none}.mw-parser-output .refbegin-100{font-size:100%}</style><div class=\"refbegin columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em;\">\n<ul><li>Suchman, Edward A. <i>Evaluative Research: Principles and Practice in Public Service &amp; Social Action Programs</i> (1967)</li>\n<li>Rivlin, Alice M. <i>Systematic Thinking for Social Action</i> (1971)</li>\n<li>Weiss, Carol H. <i>Evaluative Research: Methods of Assessing Program Effectiveness</i> (1972)</li>\n<li>Cook, Thomas D. and Campbell, Donald T. <i>Quasi-Experimentation: Design &amp; Analysis for Field Settings</i> (1979)</li>\n<li>Boulmetis, John and Dutwin, Phyllis. <i>The ABCs of Evaluation</i> (2005)</li>\n<li>Rossi, Peter H., Freeman, Howard A. and Lipsey, Mark W.. <i>Evaluation.  A Systematic Approach</i> (1999)</li>\n<li>Preskill, Hallie, Parkhurst, Marcie, and Juster, Jennifer Splansky. \"Guide to Evaluating Collective Impact\" (2014)</li>\n<li>Borland, J., &amp; Tseng, Y. P. (2011). A Primer on Doing Evaluation of Social Programs. Parity, 24(7), 8.</li>\n<li><cite class=\"citation book\">Wholey Joseph S, Hatry Harry P, Newcomer Kathryn E,  et al. (2010). <i>Handbook of Practical Program Evaluation Third Edition</i>. San Francisco: Jossey-Bass. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-0-470-52247-9\" title=\"Special:BookSources/978-0-470-52247-9\">978-0-470-52247-9</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Handbook+of+Practical+Program+Evaluation+Third+Edition&amp;rft.place=San+Francisco&amp;rft.pub=Jossey-Bass&amp;rft.date=2010&amp;rft.isbn=978-0-470-52247-9&amp;rft.aulast=Wholey+Joseph&amp;rft.aufirst=S&amp;rft.au=Hatry+Harry%2C+P&amp;rft.au=Newcomer+Kathryn%2C+E&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AProgram+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li>Khaidukov, Danil; Tasalov, Kirill; Schepetina, Ekaterina; Chueva, Ekaterina (2016). Improvement of methodological approaches to enhancing the efficiency of state programs of the Russian Federation // Lomonosov Moscow State University Science Conference \u00abEffective management\u00bb, Poligraf Service, Moscow, pp.&#160;65\u201373</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://www.acf.hhs.gov/sites/default/files/opre/program_managers_guide_to_eval2010.pdf\">The Program Manager's Guide to Evaluation, Second Edition</a>, Office of Planning, Research &amp; Evaluation, Administration for Children and Families.</li></ul>\n</div>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Program_evaluation&amp;action=edit&amp;section=60\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.acf.hhs.gov/programs/opre/other_resrch/pm_guide_eval/reports/pmguide/pmguide_toc.html\">Administration for Children and Families</a> The Program Manager's Guide to Evaluation. Discussion of evaluation, includes chapters on Why evaluate, What is evaluation.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.betterevaluation.org/\">BetterEvaluation</a> BetterEvaluation: Sharing information to improve evaluation</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.eval.org/\">American Evaluation Association</a> Includes a link to <a rel=\"nofollow\" class=\"external text\" href=\"http://www.eval.org/resources.asp\">Evaluation resources</a> such as organizations, links sites, email discussion lists, consultants and more</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.evaluationcanada.ca/\">Canadian Evaluation Society</a> Includes a link to <a rel=\"nofollow\" class=\"external text\" href=\"http://evaluationcanada.ca/site.cgi?section=1&amp;ssection=1&amp;_lang=an\">Evaluation information</a> such as services, professional development, resources, organizations, regional chapters</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://www.cdc.gov/mmwr/preview/mmwrhtml/rr4811a1.htm\">CDC six-step framework</a>. Also available here  <a rel=\"nofollow\" class=\"external text\" href=\"https://www.cdc.gov/eval/framework.htm\">Centers for Disease Control and Prevention.  Framework for Program Evaluation in Public Health. MMWR 1999;48(No. RR-11)</a>. Includes a description of logic models in the Steps section.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/books?id=sgoHv5ZP6dcC&amp;pg=PA83&amp;lpg=PA83&amp;dq=self+selection++experimental+design&amp;source=web&amp;ots=VlCu1pcpbr&amp;sig=VjRJgw5ASJUmRpAVi-sa_t2P5_w&amp;hl=en&amp;sa=X&amp;oi=book_result&amp;resnum=4&amp;ct=result#PPA82,M1\">Handbook of Research Design &amp; Social Measurement</a>. Delbert Charles Miller, Neil J. Salkind (2002) Edition: 6, revised. Published by SAGE.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.evalpartners.org/\">EvalPartners (EvalYouth)</a> International Evaluation Network and Training tools</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20070330104852/http://www.evaluationwiki.org/\">The EvaluationWiki</a> - The mission of EvaluationWiki is to make freely available a compendium of up-to-date information and resources to everyone involved in the science and practice of evaluation. The EvaluationWiki is presented by the non-profit <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20070311150127/http://www.evaluationwiki.org/wiki/index.php/Evaluation_Resource_Institute\">Evaluation Resource Institute</a>.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://sites.google.com/site/gsocialchange/\">Free Resources for Program Evaluation and Social Research Methods</a> This is a gateway to resources on program evaluation, how to, online guides, manuals, books on methods of evaluation and free software related to evaluation.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.innonet.org\">Innovation Network</a> A nonprofit organization working to share planning and evaluation tools and know-how. The organization <a href=\"/w/index.php?title=Provides&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Provides (page does not exist)\">provides</a> online tools, consulting, and training, for nonprofits and funders.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20060222134254/http://www.education.purdue.edu/AssessmentCouncil/Links/Index.htm\">Links to Assessment and Evaluation Resources</a> List of links to resources on several topics, including: centers, community building, education and training in evaluation; Foundations; Indiana government &amp; organizations; Links collected by...; Logic models; performance assessment &amp; electronic portfolios, political &amp; private groups or companies, professional assns, orgs &amp; pubs, Purdue University, United States Government, web searches for publications by author &amp; topic, and Vivisimo topical meta searches.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.maine.gov/legis/opega/\">Maine Legislature's Office of Program Evaluation &amp; Government Accountability</a> An excellent example of a governmental Program Evaluation office with links to several detailed reports which include methodology, evaluations results, recommendations and action plans.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.ncsl.org/nlpes/\">National Legislative Program Evaluation Society</a> Includes links to state offices of program evaluation and/or performance auditing in the US.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.uwex.edu/ces/4h/evaluation/Evaluating\">4-H Youth Development Programs</a> This is a teaching website and provides help wishing to evaluate their programs.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.urban.org/sites/default/files/alfresco/publication-pdfs/310776-Key-Steps-in-Outcome-Management.PDF\">Program evaluation and Outcome management - The Urban Institute</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3060080\">What is Program Evaluation: a Beginner's Guide</a> Gene Shackman</li></ul>\n\n<!-- \nNewPP limit report\nParsed by mw2216\nCached time: 20180912205547\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.440 seconds\nReal time usage: 0.488 seconds\nPreprocessor visited node count: 2268/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 55397/2097152 bytes\nTemplate argument size: 656/2097152 bytes\nHighest expansion depth: 11/40\nExpensive parser function count: 1/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 56258/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.173/10.000 seconds\nLua memory usage: 4.16 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  391.354      1 -total\n 65.66%  256.976      1 Template:Reflist\n 25.64%  100.346     17 Template:Cite_web\n 12.62%   49.399      1 Template:Synthesis_inline\n 10.36%   40.556      9 Template:Cite_journal\n  8.64%   33.797      8 Template:Cite_book\n  8.25%   32.306      1 Template:Fix\n  5.03%   19.688      1 Template:Category_handler\n  4.43%   17.341      1 Template:Refbegin\n  2.44%    9.532      1 Template:Delink\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:619350-0!canonical and timestamp 20180912205546 and revision id 859258119\n -->\n</div>"},"langlinks":[{"lang":"fr","url":"https://fr.wikipedia.org/wiki/Mission_d%27%C3%A9valuation_des_politiques_publiques","langname":"French","autonym":"fran\u00e7ais","*":"Mission d'\u00e9valuation des politiques publiques"},{"lang":"it","url":"https://it.wikipedia.org/wiki/Valutazione_delle_politiche_pubbliche","langname":"Italian","autonym":"italiano","*":"Valutazione delle politiche pubbliche"},{"lang":"he","url":"https://he.wikipedia.org/wiki/%D7%94%D7%A2%D7%A8%D7%9B%D7%AA_%D7%91%D7%99%D7%A6%D7%95%D7%A2","langname":"Hebrew","autonym":"\u05e2\u05d1\u05e8\u05d9\u05ea","*":"\u05d4\u05e2\u05e8\u05db\u05ea \u05d1\u05d9\u05e6\u05d5\u05e2"},{"lang":"pt","url":"https://pt.wikipedia.org/wiki/Avalia%C3%A7%C3%A3o_de_programas","langname":"Portuguese","autonym":"portugu\u00eas","*":"Avalia\u00e7\u00e3o de programas"},{"lang":"ru","url":"https://ru.wikipedia.org/wiki/%D0%9E%D1%86%D0%B5%D0%BD%D0%BA%D0%B0_%D0%BF%D1%80%D0%BE%D0%B3%D1%80%D0%B0%D0%BC%D0%BC","langname":"Russian","autonym":"\u0440\u0443\u0441\u0441\u043a\u0438\u0439","*":"\u041e\u0446\u0435\u043d\u043a\u0430 \u043f\u0440\u043e\u0433\u0440\u0430\u043c\u043c"}],"categories":[{"sortkey":"","hidden":"","*":"CS1_maint:_Multiple_names:_authors_list"},{"sortkey":"","hidden":"","*":"CS1_errors:_dates"},{"sortkey":"Program Evaluation","hidden":"","*":"Articles_that_may_contain_original_research_from_February_2012"},{"sortkey":"Program Evaluation","*":"Impact_assessment"},{"sortkey":"Program Evaluation","*":"Educational_evaluation_methods"},{"sortkey":"Program Evaluation","*":"Public_policy_research"}],"links":[{"ns":14,"exists":"","*":"Category:Articles that may contain original research from February 2012"},{"ns":14,"exists":"","*":"Category:CS1 maint: Multiple names: authors list"},{"ns":0,"exists":"","*":"Action research"},{"ns":0,"exists":"","*":"American Evaluation Association"},{"ns":0,"exists":"","*":"Axiology"},{"ns":0,"exists":"","*":"CIPP evaluation model"},{"ns":0,"exists":"","*":"Centers for Disease Control and Prevention"},{"ns":0,"exists":"","*":"Collective impact"},{"ns":0,"exists":"","*":"Critical Race Theory"},{"ns":0,"exists":"","*":"Cultural competence"},{"ns":0,"exists":"","*":"Decision-making"},{"ns":0,"exists":"","*":"Digital object identifier"},{"ns":0,"exists":"","*":"Economics"},{"ns":0,"exists":"","*":"Empowerment evaluation"},{"ns":0,"exists":"","*":"Epistemology"},{"ns":0,"exists":"","*":"Evaluation"},{"ns":0,"exists":"","*":"Factor of production"},{"ns":0,"exists":"","*":"Feminist theories"},{"ns":0,"exists":"","*":"Gene V. Glass"},{"ns":0,"exists":"","*":"Great Society"},{"ns":0,"exists":"","*":"Impact assessment"},{"ns":0,"exists":"","*":"Impact evaluation"},{"ns":0,"exists":"","*":"Implementation"},{"ns":0,"exists":"","*":"International Standard Book Number"},{"ns":0,"exists":"","*":"John F. Kennedy"},{"ns":0,"exists":"","*":"Logic model"},{"ns":0,"exists":"","*":"Lyndon Johnson"},{"ns":0,"exists":"","*":"Methodology"},{"ns":0,"exists":"","*":"National Association of Social Workers"},{"ns":0,"exists":"","*":"Ontology"},{"ns":0,"exists":"","*":"Participatory Impact Pathways Analysis"},{"ns":0,"exists":"","*":"Participatory impact pathways analysis"},{"ns":0,"exists":"","*":"Policy analysis"},{"ns":0,"exists":"","*":"Policy studies"},{"ns":0,"exists":"","*":"Positivist"},{"ns":0,"exists":"","*":"Product breakdown structure"},{"ns":0,"exists":"","*":"Program (management)"},{"ns":0,"exists":"","*":"Programme management"},{"ns":0,"exists":"","*":"Project stakeholder"},{"ns":0,"exists":"","*":"Psychology"},{"ns":0,"exists":"","*":"Public policy"},{"ns":0,"exists":"","*":"Qualitative method"},{"ns":0,"exists":"","*":"Quantitative method"},{"ns":0,"exists":"","*":"Queer theory"},{"ns":0,"exists":"","*":"Self-selection"},{"ns":0,"exists":"","*":"Self-selection bias"},{"ns":0,"exists":"","*":"Social research"},{"ns":0,"exists":"","*":"Social work"},{"ns":0,"exists":"","*":"Sociology"},{"ns":0,"*":"Utilization-Focused Evaluation"},{"ns":0,"*":"Provides"},{"ns":4,"exists":"","*":"Wikipedia:No original research"},{"ns":12,"exists":"","*":"Help:CS1 errors"}],"templates":[{"ns":10,"exists":"","*":"Template:TOC limit"},{"ns":10,"exists":"","*":"Template:Synthesis inline"},{"ns":10,"exists":"","*":"Template:Fix"},{"ns":10,"exists":"","*":"Template:Category handler"},{"ns":10,"exists":"","*":"Template:Fix/category"},{"ns":10,"exists":"","*":"Template:Delink"},{"ns":10,"exists":"","*":"Template:Main"},{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Column-width"},{"ns":10,"exists":"","*":"Template:Cite web"},{"ns":10,"exists":"","*":"Template:Cite book"},{"ns":10,"exists":"","*":"Template:Cite paper"},{"ns":10,"exists":"","*":"Template:Cite journal"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":10,"exists":"","*":"Template:Refbegin"},{"ns":10,"exists":"","*":"Template:Refbegin/styles.css"},{"ns":10,"exists":"","*":"Template:Refend"},{"ns":828,"exists":"","*":"Module:Unsubst"},{"ns":828,"exists":"","*":"Module:Category handler"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Category handler/data"},{"ns":828,"exists":"","*":"Module:Category handler/config"},{"ns":828,"exists":"","*":"Module:Category handler/shared"},{"ns":828,"exists":"","*":"Module:Category handler/blacklist"},{"ns":828,"exists":"","*":"Module:Namespace detect/data"},{"ns":828,"exists":"","*":"Module:Namespace detect/config"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Delink"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:Main"},{"ns":828,"exists":"","*":"Module:Hatnote"},{"ns":828,"exists":"","*":"Module:Hatnote list"},{"ns":828,"exists":"","*":"Module:Citation/CS1"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Configuration"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Whitelist"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Utilities"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Date validation"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Identifiers"},{"ns":828,"exists":"","*":"Module:Citation/CS1/COinS"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"}],"images":[],"externallinks":["http://www.acf.hhs.gov/programs/opre/other_resrch/pm_guide_eval/index.html","https://papers.ssrn.com/sol3/papers.cfm?abstract_id=3060080","https://www.cdc.gov/eval/strongevaluations/index.htm","http://www.dol.gov/oasam/programs/history/dolchp06.htm","https://www.archives.gov/research/guide-fed-records/groups/051.html","http://media.wiley.com/product_data/excerpt/57/07879752/0787975257.pdf","http://alumnus.caltech.edu/~rouda/T2_NA.html#website=alumnus.caltech.edu","//doi.org/10.1177/2399808317720446","https://www.cdc.gov/eval/framework/index.htm","http://www.ssireview.org/articles/entry/collective_impact","http://www.fsg.org/Portals/0/Uploads/Documents/PDF/Evaluating_Collective_Impact_Assessing_Progress_2.pdf","http://tei.gwu.edu/evaluation-approaches-and-techniques#developmental-evaluation","http://evaluationtoolbox.net.au/index.php?option=com_content&view=article&id=24:formative-evaluation&catid=17:formative-evaluation&Itemid=125","http://evaluationtoolbox.net.au/index.php?option=com_content&view=article&id=40&Itemid=126","http://learningstore.uwex.edu/assets/pdfs/g3658-1.pdf","//doi.org/10.1177/109821400402500102","//doi.org/10.1207/S1532480XADS0702_3","http://www.npen.org/parent-educators/evaluating/approach.html","http://www.joe.org/joe/2002april/iw1.php","http://ag.arizona.edu/sfcs/cyfernet/cyfar/5_apprch.pdf","//doi.org/10.1177/109821400002100108","//doi.org/10.1080/02598272.1990.10800228","//doi.org/10.1016/S0738-0593(98)00038-8","http://www.ppic.org/main/publication.asp?i=322","http://siteresources.worldbank.org/SOCIALANALYSIS/1104890-1120158274352/20566665/Integratingqualitativeandquantapproachesraoandwoolcock.pdf","//doi.org/10.1023/a:1014324218388","http://www.whitehouse.gov/sites/default/files/omb/performance/chapter8-2012.pdf","https://www.acf.hhs.gov/sites/default/files/opre/program_managers_guide_to_eval2010.pdf","http://www.acf.hhs.gov/programs/opre/other_resrch/pm_guide_eval/reports/pmguide/pmguide_toc.html","http://www.betterevaluation.org/","http://www.eval.org/","http://www.eval.org/resources.asp","http://www.evaluationcanada.ca/","http://evaluationcanada.ca/site.cgi?section=1&ssection=1&_lang=an","https://www.cdc.gov/mmwr/preview/mmwrhtml/rr4811a1.htm","https://www.cdc.gov/eval/framework.htm","https://books.google.com/books?id=sgoHv5ZP6dcC&pg=PA83&lpg=PA83&dq=self+selection++experimental+design&source=web&ots=VlCu1pcpbr&sig=VjRJgw5ASJUmRpAVi-sa_t2P5_w&hl=en&sa=X&oi=book_result&resnum=4&ct=result#PPA82,M1","http://www.evalpartners.org/","https://web.archive.org/web/20070330104852/http://www.evaluationwiki.org/","https://web.archive.org/web/20070311150127/http://www.evaluationwiki.org/wiki/index.php/Evaluation_Resource_Institute","https://sites.google.com/site/gsocialchange/","http://www.innonet.org","https://web.archive.org/web/20060222134254/http://www.education.purdue.edu/AssessmentCouncil/Links/Index.htm","http://www.maine.gov/legis/opega/","http://www.ncsl.org/nlpes/","http://www.uwex.edu/ces/4h/evaluation/Evaluating","http://www.urban.org/sites/default/files/alfresco/publication-pdfs/310776-Key-Steps-in-Outcome-Management.PDF"],"sections":[{"toclevel":1,"level":"2","line":"Doing an evaluation","number":"1","index":"1","fromtitle":"Program_evaluation","byteoffset":3204,"anchor":"Doing_an_evaluation"},{"toclevel":2,"level":"3","line":"Assessing needs","number":"1.1","index":"2","fromtitle":"Program_evaluation","byteoffset":3974,"anchor":"Assessing_needs"},{"toclevel":2,"level":"3","line":"Assessing program theory","number":"1.2","index":"3","fromtitle":"Program_evaluation","byteoffset":11315,"anchor":"Assessing_program_theory"},{"toclevel":2,"level":"3","line":"Assessing implementation","number":"1.3","index":"4","fromtitle":"Program_evaluation","byteoffset":17427,"anchor":"Assessing_implementation"},{"toclevel":2,"level":"3","line":"Assessing the impact (effectiveness)","number":"1.4","index":"5","fromtitle":"Program_evaluation","byteoffset":18975,"anchor":"Assessing_the_impact_(effectiveness)"},{"toclevel":3,"level":"4","line":"Program Outcomes","number":"1.4.1","index":"6","fromtitle":"Program_evaluation","byteoffset":19193,"anchor":"Program_Outcomes"},{"toclevel":3,"level":"4","line":"Measuring Program Outcomes","number":"1.4.2","index":"7","fromtitle":"Program_evaluation","byteoffset":20119,"anchor":"Measuring_Program_Outcomes"},{"toclevel":2,"level":"3","line":"Assessing efficiency","number":"1.5","index":"8","fromtitle":"Program_evaluation","byteoffset":21824,"anchor":"Assessing_efficiency"},{"toclevel":1,"level":"2","line":"Determining causation","number":"2","index":"9","fromtitle":"Program_evaluation","byteoffset":22590,"anchor":"Determining_causation"},{"toclevel":1,"level":"2","line":"Reliability, validity and sensitivity in program evaluation","number":"3","index":"10","fromtitle":"Program_evaluation","byteoffset":24499,"anchor":"Reliability,_validity_and_sensitivity_in_program_evaluation"},{"toclevel":2,"level":"3","line":"Reliability","number":"3.1","index":"11","fromtitle":"Program_evaluation","byteoffset":25195,"anchor":"Reliability"},{"toclevel":2,"level":"3","line":"Validity","number":"3.2","index":"12","fromtitle":"Program_evaluation","byteoffset":25825,"anchor":"Validity"},{"toclevel":2,"level":"3","line":"Sensitivity","number":"3.3","index":"13","fromtitle":"Program_evaluation","byteoffset":26249,"anchor":"Sensitivity"},{"toclevel":1,"level":"2","line":"Evaluating Collective Impact","number":"4","index":"14","fromtitle":"Program_evaluation","byteoffset":28039,"anchor":"Evaluating_Collective_Impact"},{"toclevel":1,"level":"2","line":"Planning a program evaluation","number":"5","index":"15","fromtitle":"Program_evaluation","byteoffset":31226,"anchor":"Planning_a_program_evaluation"},{"toclevel":1,"level":"2","line":"Methodological constraints and challenges","number":"6","index":"16","fromtitle":"Program_evaluation","byteoffset":32309,"anchor":"Methodological_constraints_and_challenges"},{"toclevel":2,"level":"3","line":"The shoestring approach","number":"6.1","index":"17","fromtitle":"Program_evaluation","byteoffset":32356,"anchor":"The_shoestring_approach"},{"toclevel":2,"level":"3","line":"Budget constraints","number":"6.2","index":"18","fromtitle":"Program_evaluation","byteoffset":33748,"anchor":"Budget_constraints"},{"toclevel":2,"level":"3","line":"Time constraints","number":"6.3","index":"19","fromtitle":"Program_evaluation","byteoffset":34671,"anchor":"Time_constraints"},{"toclevel":2,"level":"3","line":"Data constraints","number":"6.4","index":"20","fromtitle":"Program_evaluation","byteoffset":35384,"anchor":"Data_constraints"},{"toclevel":2,"level":"3","line":"Five-tiered approach","number":"6.5","index":"21","fromtitle":"Program_evaluation","byteoffset":37093,"anchor":"Five-tiered_approach"},{"toclevel":2,"level":"3","line":"Methodological challenges presented by language and culture","number":"6.6","index":"22","fromtitle":"Program_evaluation","byteoffset":40713,"anchor":"Methodological_challenges_presented_by_language_and_culture"},{"toclevel":1,"level":"2","line":"Utilization results","number":"7","index":"23","fromtitle":"Program_evaluation","byteoffset":45976,"anchor":"Utilization_results"},{"toclevel":2,"level":"3","line":"Persuasive utilization","number":"7.1","index":"24","fromtitle":"Program_evaluation","byteoffset":46163,"anchor":"Persuasive_utilization"},{"toclevel":2,"level":"3","line":"Direct (instrumental) utilization","number":"7.2","index":"25","fromtitle":"Program_evaluation","byteoffset":46555,"anchor":"Direct_(instrumental)_utilization"},{"toclevel":2,"level":"3","line":"Conceptual utilization","number":"7.3","index":"26","fromtitle":"Program_evaluation","byteoffset":47286,"anchor":"Conceptual_utilization"},{"toclevel":2,"level":"3","line":"Variables affecting utilization","number":"7.4","index":"27","fromtitle":"Program_evaluation","byteoffset":47830,"anchor":"Variables_affecting_utilization"},{"toclevel":2,"level":"3","line":"Guidelines for maximizing utilization","number":"7.5","index":"28","fromtitle":"Program_evaluation","byteoffset":48207,"anchor":"Guidelines_for_maximizing_utilization"},{"toclevel":1,"level":"2","line":"Internal versus external program evaluators","number":"8","index":"29","fromtitle":"Program_evaluation","byteoffset":48655,"anchor":"Internal_versus_external_program_evaluators"},{"toclevel":2,"level":"3","line":"Internal evaluators","number":"8.1","index":"30","fromtitle":"Program_evaluation","byteoffset":49365,"anchor":"Internal_evaluators"},{"toclevel":2,"level":"3","line":"External evaluators","number":"8.2","index":"31","fromtitle":"Program_evaluation","byteoffset":49768,"anchor":"External_evaluators"},{"toclevel":1,"level":"2","line":"Three paradigms","number":"9","index":"32","fromtitle":"Program_evaluation","byteoffset":50336,"anchor":"Three_paradigms"},{"toclevel":2,"level":"3","line":"Positivist","number":"9.1","index":"33","fromtitle":"Program_evaluation","byteoffset":50357,"anchor":"Positivist"},{"toclevel":2,"level":"3","line":"Interpretive","number":"9.2","index":"34","fromtitle":"Program_evaluation","byteoffset":51768,"anchor":"Interpretive"},{"toclevel":2,"level":"3","line":"Critical-emancipatory","number":"9.3","index":"35","fromtitle":"Program_evaluation","byteoffset":52905,"anchor":"Critical-emancipatory"},{"toclevel":1,"level":"2","line":"Empowerment evaluation","number":"10","index":"36","fromtitle":"Program_evaluation","byteoffset":54612,"anchor":"Empowerment_evaluation"},{"toclevel":2,"level":"3","line":"Establishing a mission","number":"10.1","index":"37","fromtitle":"Program_evaluation","byteoffset":56145,"anchor":"Establishing_a_mission"},{"toclevel":2,"level":"3","line":"Taking stock","number":"10.2","index":"38","fromtitle":"Program_evaluation","byteoffset":56588,"anchor":"Taking_stock"},{"toclevel":2,"level":"3","line":"Planning for the future","number":"10.3","index":"39","fromtitle":"Program_evaluation","byteoffset":57481,"anchor":"Planning_for_the_future"},{"toclevel":1,"level":"2","line":"Transformative Paradigm","number":"11","index":"40","fromtitle":"Program_evaluation","byteoffset":59131,"anchor":"Transformative_Paradigm"},{"toclevel":2,"level":"3","line":"Paradigms","number":"11.1","index":"41","fromtitle":"Program_evaluation","byteoffset":61933,"anchor":"Paradigms"},{"toclevel":3,"level":"4","line":"Axiology (Values and Value Judgements)","number":"11.1.1","index":"42","fromtitle":"Program_evaluation","byteoffset":62214,"anchor":"Axiology_(Values_and_Value_Judgements)"},{"toclevel":3,"level":"4","line":"Ontology (Reality)","number":"11.1.2","index":"43","fromtitle":"Program_evaluation","byteoffset":62518,"anchor":"Ontology_(Reality)"},{"toclevel":3,"level":"4","line":"Epistemology (Knowledge)","number":"11.1.3","index":"44","fromtitle":"Program_evaluation","byteoffset":62976,"anchor":"Epistemology_(Knowledge)"},{"toclevel":3,"level":"4","line":"Methodology (Systematic Inquiry)","number":"11.1.4","index":"45","fromtitle":"Program_evaluation","byteoffset":63509,"anchor":"Methodology_(Systematic_Inquiry)"},{"toclevel":2,"level":"3","line":"Lenses","number":"11.2","index":"46","fromtitle":"Program_evaluation","byteoffset":63940,"anchor":"Lenses"},{"toclevel":3,"level":"4","line":"Critical Race Theory","number":"11.2.1","index":"47","fromtitle":"Program_evaluation","byteoffset":64336,"anchor":"Critical_Race_Theory"},{"toclevel":3,"level":"4","line":"Feminist Theory","number":"11.2.2","index":"48","fromtitle":"Program_evaluation","byteoffset":64937,"anchor":"Feminist_Theory"},{"toclevel":3,"level":"4","line":"Queer/LGBTQ Theory","number":"11.2.3","index":"49","fromtitle":"Program_evaluation","byteoffset":65332,"anchor":"Queer/LGBTQ_Theory"},{"toclevel":1,"level":"2","line":"Government requirements","number":"12","index":"50","fromtitle":"Program_evaluation","byteoffset":65813,"anchor":"Government_requirements"},{"toclevel":1,"level":"2","line":"Types of Evaluation","number":"13","index":"51","fromtitle":"Program_evaluation","byteoffset":67014,"anchor":"Types_of_Evaluation"},{"toclevel":1,"level":"2","line":"CIPP Model of evaluation","number":"14","index":"52","fromtitle":"Program_evaluation","byteoffset":67357,"anchor":"CIPP_Model_of_evaluation"},{"toclevel":2,"level":"3","line":"History of the CIPP model","number":"14.1","index":"53","fromtitle":"Program_evaluation","byteoffset":67418,"anchor":"History_of_the_CIPP_model"},{"toclevel":2,"level":"3","line":"CIPP model","number":"14.2","index":"54","fromtitle":"Program_evaluation","byteoffset":68161,"anchor":"CIPP_model"},{"toclevel":2,"level":"3","line":"Four aspects of CIPP evaluation","number":"14.3","index":"55","fromtitle":"Program_evaluation","byteoffset":69142,"anchor":"Four_aspects_of_CIPP_evaluation"},{"toclevel":2,"level":"3","line":"Using CIPP in the different stages of the evaluation","number":"14.4","index":"56","fromtitle":"Program_evaluation","byteoffset":70690,"anchor":"Using_CIPP_in_the_different_stages_of_the_evaluation"},{"toclevel":1,"level":"2","line":"See also","number":"15","index":"57","fromtitle":"Program_evaluation","byteoffset":71536,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"References","number":"16","index":"58","fromtitle":"Program_evaluation","byteoffset":71686,"anchor":"References"},{"toclevel":1,"level":"2","line":"Further reading","number":"17","index":"59","fromtitle":"Program_evaluation","byteoffset":71719,"anchor":"Further_reading"},{"toclevel":1,"level":"2","line":"External links","number":"18","index":"60","fromtitle":"Program_evaluation","byteoffset":73349,"anchor":"External_links"}],"parsewarnings":[],"displaytitle":"Program evaluation","iwlinks":[],"properties":[{"name":"defaultsort","*":"Program Evaluation"},{"name":"wikibase_item","*":"Q1061410"}]}}