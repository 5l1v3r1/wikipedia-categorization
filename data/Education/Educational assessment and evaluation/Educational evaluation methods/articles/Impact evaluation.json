{"parse":{"title":"Impact evaluation","pageid":15092946,"revid":843969668,"text":{"*":"<div class=\"mw-parser-output\"><table class=\"plainlinks metadata ambox ambox-style ambox-external_links\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/40px-Edit-clear.svg.png\" width=\"40\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/60px-Edit-clear.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/f2/Edit-clear.svg/80px-Edit-clear.svg.png 2x\" data-file-width=\"48\" data-file-height=\"48\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article's <b>use of <a href=\"/wiki/Wikipedia:External_links\" title=\"Wikipedia:External links\">external links</a> may not follow Wikipedia's policies or guidelines</b>.<span class=\"hide-when-compact\"> Please <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Impact_evaluation&amp;action=edit\">improve this article</a> by removing <a href=\"/wiki/Wikipedia:What_Wikipedia_is_not#Wikipedia_is_not_a_mirror_or_a_repository_of_links,_images,_or_media_files\" title=\"Wikipedia:What Wikipedia is not\">excessive</a> or <a href=\"/wiki/Wikipedia:External_links\" title=\"Wikipedia:External links\">inappropriate</a> external links, and converting useful links where appropriate into <a href=\"/wiki/Wikipedia:Citing_sources\" title=\"Wikipedia:Citing sources\">footnote references</a>.</span>  <small><i>(June 2017)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<p><b>Impact evaluation</b> assesses the changes that can be attributed to a particular intervention, such as a project, program or policy, both the intended ones, as well as ideally the unintended ones.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup> In contrast to outcome monitoring, which examines whether targets have been achieved, impact evaluation is structured to answer the question: how would outcomes such as participants' well-being have changed if the intervention had not been undertaken? This involves counterfactual analysis, that is, \"a comparison between what actually happened and what would have happened in the absence of the intervention.\"<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup> Impact evaluations seek to answer cause-and-effect questions. In other words, they look for the changes in outcome that are directly attributable to a program.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup>\n</p><p>Impact evaluation helps people answer key questions for evidence-based policy making: what works, what doesn't, where, why and for how much? It has received increasing attention in policy making in recent years in the context of both Western and developing countries.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\">&#91;4&#93;</a></sup> It is an important component of the armory of <a href=\"/wiki/Evaluation\" title=\"Evaluation\">evaluation</a> tools and approaches and integral to global efforts to improve the effectiveness of aid delivery and public spending more generally in improving living standards.<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\">&#91;5&#93;</a></sup> Originally more oriented towards evaluation of social sector programs in developing countries, notably <a href=\"/wiki/Conditional_Cash_Transfer\" class=\"mw-redirect\" title=\"Conditional Cash Transfer\">conditional cash transfers</a>, impact evaluation is now being increasingly applied in other areas such as the agriculture, energy and transport.\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Counterfactual_evaluation_designs\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Counterfactual evaluation designs</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-2\"><a href=\"#Experimental_design\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\">Experimental design</span></a></li>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"#Quasi-experimental_design\"><span class=\"tocnumber\">1.2</span> <span class=\"toctext\">Quasi-experimental design</span></a></li>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Non-experimental_design\"><span class=\"tocnumber\">1.3</span> <span class=\"toctext\">Non-experimental design</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Biases_in_estimating_programme_effects\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Biases in estimating programme effects</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-6\"><a href=\"#Selection_bias\"><span class=\"tocnumber\">2.1</span> <span class=\"toctext\">Selection bias</span></a></li>\n<li class=\"toclevel-2 tocsection-7\"><a href=\"#Other_forms_of_bias\"><span class=\"tocnumber\">2.2</span> <span class=\"toctext\">Other forms of bias</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-8\"><a href=\"#Secular_trends_or_secular_drift\"><span class=\"tocnumber\">2.2.1</span> <span class=\"toctext\">Secular trends or secular drift</span></a></li>\n<li class=\"toclevel-3 tocsection-9\"><a href=\"#Interfering_events\"><span class=\"tocnumber\">2.2.2</span> <span class=\"toctext\">Interfering events</span></a></li>\n<li class=\"toclevel-3 tocsection-10\"><a href=\"#Maturation\"><span class=\"tocnumber\">2.2.3</span> <span class=\"toctext\">Maturation</span></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-11\"><a href=\"#Estimation_methods\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Estimation methods</span></a></li>\n<li class=\"toclevel-1 tocsection-12\"><a href=\"#Debates\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Debates</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-13\"><a href=\"#Definitions\"><span class=\"tocnumber\">4.1</span> <span class=\"toctext\">Definitions</span></a></li>\n<li class=\"toclevel-2 tocsection-14\"><a href=\"#Methodological_debates\"><span class=\"tocnumber\">4.2</span> <span class=\"toctext\">Methodological debates</span></a></li>\n<li class=\"toclevel-2 tocsection-15\"><a href=\"#Theory-based_impact_evaluation\"><span class=\"tocnumber\">4.3</span> <span class=\"toctext\">Theory-based impact evaluation</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-16\"><a href=\"#Examples\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Examples</span></a></li>\n<li class=\"toclevel-1 tocsection-17\"><a href=\"#Organizations_promoting_impact_evaluation_of_development_interventions\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Organizations promoting impact evaluation of development interventions</span></a></li>\n<li class=\"toclevel-1 tocsection-18\"><a href=\"#Systematic_reviews_of_impact_evidence\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Systematic reviews of impact evidence</span></a></li>\n<li class=\"toclevel-1 tocsection-19\"><a href=\"#See_also\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-20\"><a href=\"#References\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-21\"><a href=\"#Sources_and_external_links\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">Sources and external links</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Counterfactual_evaluation_designs\">Counterfactual evaluation designs</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=1\" title=\"Edit section: Counterfactual evaluation designs\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p><a href=\"/wiki/Counterfactual_conditional\" title=\"Counterfactual conditional\">Counterfactual</a> analysis enables evaluators to attribute cause and effect between interventions and outcomes. The 'counterfactual' measures what would have happened to beneficiaries in the absence of the intervention, and impact is estimated by comparing counterfactual outcomes to those observed under the intervention. The key challenge in impact evaluation is that the counterfactual cannot be directly observed and must be approximated with reference to a comparison group. There are a range of accepted approaches to determining an appropriate comparison group for counterfactual analysis, using either prospective (ex ante) or retrospective (ex post) evaluation design. Prospective evaluations begin during the design phase of the intervention, involving collection of baseline and end-line data from intervention beneficiaries (the 'treatment group') and non-beneficiaries (the 'comparison group'); they may involve selection of individuals or communities into treatment and comparison groups. Retrospective evaluations are usually conducted after the implementation phase and may exploit existing survey data, although the best evaluations will collect data as close to baseline as possible, to ensure comparability of intervention and comparison groups.\n</p><p>There are five key principles relating to internal validity (study design) and external validity (generalizability) which rigorous impact evaluations should address: confounding factors, <a href=\"/wiki/Selection_bias\" title=\"Selection bias\">selection bias</a>, spillover effects, contamination, and impact heterogeneity.<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup>\n</p>\n<ul><li><b>Confounding</b> occurs where certain factors, typically relating to socio-economic status, are correlated with exposure to the intervention and, independent of exposure, are causally related to the outcome of interest. Confounding factors are therefore alternate explanations for an observed (possibly spurious) relationship between intervention and outcome.</li>\n<li><b>Selection bias</b>, a special case of confounding, occurs where intervention participants are non-randomly drawn from the beneficiary population, and the criteria determining selection are correlated with outcomes. <a href=\"/wiki/Unobserved_heterogeneity\" class=\"mw-redirect\" title=\"Unobserved heterogeneity\">Unobserved factors</a>, which are associated with access to or participation in the intervention, and are causally related to the outcome of interest, may lead to a spurious relationship between intervention and outcome if unaccounted for. Self-selection occurs where, for example, more able or organized individuals or communities, who are more likely to have better outcomes of interest, are also more likely to participate in the intervention. Endogenous program selection occurs where individuals or communities are chosen to participate because they are seen to be more likely to benefit from the intervention. Ignoring confounding factors can lead to a problem of omitted variable bias. In the special case of selection bias, the endogeneity of the selection variables can cause simultaneity bias.</li>\n<li><b>Spillover</b> (referred to as contagion in the case of experimental evaluations) occurs when members of the comparison (control) group are affected by the intervention.</li>\n<li><b>Contamination</b> occurs when members of treatment and/or comparison groups have access to another intervention which also affects the outcome of interest.</li>\n<li><b>Impact heterogeneity</b> refers to differences in impact due by beneficiary type and context. High quality impact evaluations will assess the extent to which different groups (e.g., the disadvantaged) benefit from an intervention as well as the potential effect of context on impact. The degree that results are generalizable will determine the applicability of lessons learned for interventions in other contexts.</li></ul>\n<p>Impact evaluation designs are identified by the type of methods used to generate the counterfactual and can be broadly classified into three categories \u2013 experimental, quasi-experimental and non-experimental designs \u2013 that vary in feasibility, cost, involvement during design or after implementation phase of the intervention, and degree of selection bias. White (2006)<sup id=\"cite_ref-worldbank.org_7-0\" class=\"reference\"><a href=\"#cite_note-worldbank.org-7\">&#91;7&#93;</a></sup> and Ravallion (2008)<sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;8&#93;</a></sup> discuss alternate Impact Evaluation approaches.\n</p>\n<h3><span class=\"mw-headline\" id=\"Experimental_design\">Experimental design</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=2\" title=\"Edit section: Experimental design\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Further information: <a href=\"/wiki/Experimental_design\" class=\"mw-redirect\" title=\"Experimental design\">Experimental design</a></div>\n<p>Under experimental evaluations the treatment and comparison groups are selected randomly and isolated both from the intervention, as well as any interventions which may affect the outcome of interest. These evaluation designs are referred to as <a href=\"/wiki/Randomized_controlled_trial\" title=\"Randomized controlled trial\">randomized control trials</a> (RCTs). In experimental evaluations the comparison group is called a <a href=\"/wiki/Control_group\" class=\"mw-redirect\" title=\"Control group\">control group</a>. When randomization is implemented over a sufficiently large sample with no contagion by the intervention, the only difference between treatment and control groups on average is that the latter does not receive the intervention. Random sample surveys, in which the sample for the evaluation is chosen randomly, should not be confused with experimental evaluation designs, which require the random assignment of the treatment.\n</p><p>The experimental approach is often held up as the 'gold standard' of evaluation. It is the only evaluation design which can conclusively account for selection bias in demonstrating a causal relationship between intervention and outcomes. Randomization and isolation from interventions might not be practicable in the realm of social policy and may be ethically difficult to defend,<sup id=\"cite_ref-auto_9-0\" class=\"reference\"><a href=\"#cite_note-auto-9\">&#91;9&#93;</a></sup> although there may be opportunities to use natural experiments. Bamberger and White (2007)<sup id=\"cite_ref-ed.gov_10-0\" class=\"reference\"><a href=\"#cite_note-ed.gov-10\">&#91;10&#93;</a></sup> highlight some of the limitations to applying RCTs to development interventions. Methodological critiques have been made by Scriven (2008)<sup id=\"cite_ref-11\" class=\"reference\"><a href=\"#cite_note-11\">&#91;11&#93;</a></sup> on account of the biases introduced since social interventions cannot be triple blinded, and Deaton (2009)<sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup> has pointed out that in practice analysis of RCTs falls back on the regression-based approaches they seek to avoid and so are subject to the same potential biases. Other problems include the often heterogeneous and changing contexts of interventions, logistical and practical challenges, difficulties with monitoring service delivery, access to the intervention by the comparison group and changes in selection criteria and/or intervention over time. Thus, it is estimated that RCTs are only applicable to 5 percent of development finance.<sup id=\"cite_ref-ed.gov_10-1\" class=\"reference\"><a href=\"#cite_note-ed.gov-10\">&#91;10&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Quasi-experimental_design\">Quasi-experimental design</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=3\" title=\"Edit section: Quasi-experimental design\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p><a href=\"/wiki/Quasi-experiment\" title=\"Quasi-experiment\">Quasi-experimental</a> approaches can remove bias arising from selection on observables and, where panel data are available, time invariant unobservables. Quasi-experimental methods include matching, differencing, instrumental variables and the pipeline approach; they are usually carried out by multivariate <a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">regression analysis</a>.\n</p><p>If selection characteristics are known and observed, they can be controlled for to remove the bias. Matching involves comparing program participants with non-participants based on observed selection characteristics. <a href=\"/wiki/Propensity_score_matching\" title=\"Propensity score matching\">Propensity score matching</a> (PSM) uses a statistical model to calculate the probability of participating on the basis of a set of observable characteristics and matches participants and non-participants with similar probability scores. <a href=\"/wiki/Regression_discontinuity_design\" title=\"Regression discontinuity design\">Regression discontinuity design</a> exploits a decision rule as to who does and does not get the intervention to compare outcomes for those just either side of this cut-off.\n</p><p>Difference-in-differences or double differences, which use data collected at baseline and end-line for intervention and comparison groups, can be used to account for selection bias under the assumption that unobservable factors determining selection are fixed over time (time invariant).\n</p><p><a href=\"/wiki/Instrumental_Variable\" class=\"mw-redirect\" title=\"Instrumental Variable\">Instrumental variables</a> estimation accounts for selection bias by modelling participation using factors ('instruments') that are correlated with selection but not the outcome, thus isolating the aspects of program participation which can be treated as exogenous.\n</p><p>The pipeline approach (<a href=\"/wiki/Stepped-wedge_trial\" title=\"Stepped-wedge trial\">stepped-wedge design</a>) uses beneficiaries already chosen to participate in a project at a later stage as the comparison group. The assumption is that as they have been selected to receive the intervention in the future they are similar to the treatment group, and therefore comparable in terms of outcome variables of interest. However, in practice, it cannot be guaranteed that treatment and comparison groups are comparable and some method of matching will need to be applied to verify comparability.\n</p>\n<h3><span class=\"mw-headline\" id=\"Non-experimental_design\">Non-experimental design</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=4\" title=\"Edit section: Non-experimental design\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Non-experimental impact evaluations are so-called because they do not involve a comparison group that does not have access to the intervention. The method used in non-experimental evaluation is to compare intervention groups before and after implementation of the intervention. Intervention <a href=\"/wiki/Interrupted_time-series\" class=\"mw-redirect\" title=\"Interrupted time-series\">interrupted time-series</a> (ITS) evaluations require multiple data points on treated individuals before and after the intervention, while before versus after (or pre-test post-test) designs simply require a single data point before and after. Post-test analyses include data after the intervention from the intervention group only. Non-experimental designs are the weakest evaluation design, because to show a causal relationship between intervention and outcomes convincingly, the evaluation must demonstrate that any likely alternate explanations for the outcomes are irrelevant. However, there remain applications to which this design is relevant, for example, in calculating time-savings from an intervention which improves access to amenities. In addition, there may be cases where non-experimental designs are the only feasible impact evaluation design, such as universally implemented programmes or national policy reforms in which no isolated comparison groups are likely to exist.\n</p>\n<h2><span class=\"mw-headline\" id=\"Biases_in_estimating_programme_effects\">Biases in estimating programme effects</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=5\" title=\"Edit section: Biases in estimating programme effects\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Randomized field experiments are the strongest research designs for assessing program impact. This particular research design is said to generally be the design of choice when it is feasible as it allows for a fair and accurate estimate of the program's actual effects (Rossi, Lipsey &amp; Freeman, 2004).\n</p><p>With that said, randomized field experiments are not always feasible to carry out and in these situations there are alternative research designs that are at the disposal of an evaluator. The main problem though is that regardless of which design an evaluator chooses, they are prone to a common problem: Regardless of how well thought through or well implemented the design is, each design is subject to yielding biased estimates of the program effects. These biases play the role of exaggerating or diminishing program effects. Not only that, but the direction the bias may take cannot usually be known in advance (Rossi et al., 2004). These biases affect the interest of the stakeholder. Furthermore, it is possible that program participants are disadvantaged if the bias is in such a way that it contributes to making an ineffective or harmful program seem effective. There is also the possibility that a bias can make an effective program seem ineffective or even as far as harmful. This could possibly make the accomplishments of program seem small or even insignificant therefore forcing the personnel and even cause the program's sponsors to reduce or eliminate the funding for the program (Rossi et al., 2004).\n</p><p>It is safe to say that if an inadequate design yields bias, the stakeholders who are largely responsible for the funding of the program will be the ones most concerned; the results of the evaluation help the stakeholders decide whether or not to continue funding the program because the final decision lies with the funders and the sponsors. Not only are the stakeholders mostly concerned, but those taking part in the program or those the program is intended to positively affect will be affected by the design chosen and the outcome rendered by that chosen design. Therefore, the evaluator's concern is to minimize the amount of bias in the estimation of program effects (Rossi et al., 2004).\n</p><p>Biases are normally visible in two situations: when the measurement of the outcome with program exposure or the estimate of what the outcome would have been without the program exposure is higher or lower than the corresponding \"true\" value (p267). Unfortunately, not all forms of bias that may compromise impact assessment are obvious (Rossi et al., 2004).\n</p><p>The most common form of impact evaluation design is comparing two groups of individuals or other units, an intervention group that receives the program and a control group that does not. The estimate of program effect is then based on the difference between the groups on a suitable outcome measure (Rossi et al., 2004). The random assignment of individuals to program and control groups allows for making the assumption of continuing equivalence. Group comparisons that have not been formed through randomization are known as non-equivalent comparison designs (Rossi et al., 2004).\n</p>\n<h3><span class=\"mw-headline\" id=\"Selection_bias\">Selection bias</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=6\" title=\"Edit section: Selection bias\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>When there is an absence of the assumption of equivalence, the difference in outcome between the groups that would have occurred regardless creates a form of bias in the estimate of program effects. This is known as selection bias (Rossi et al., 2004). It creates a threat to the validity of the program effect estimate in any impact assessment using a non-equivalent group comparison design and appears in situations where some process responsible for influences that are not fully known selects which individuals will be in which group instead of the assignment to groups being determined by pure chance (Rossi et al., 2004).  This may be because of participant self-selection, or it may be because of program placement (placement bias).<sup id=\"cite_ref-:0_13-0\" class=\"reference\"><a href=\"#cite_note-:0-13\">&#91;13&#93;</a></sup>\n</p><p>Selection bias can occur through natural or deliberate processes that cause a loss of outcome data for members of the intervention and control groups that have already been formed. This is known as attrition and it can come about in two ways (Rossi et al., 2004): targets drop out of the intervention or control group cannot be reached or targets refuse to co-operate in outcome measurement. Differential attrition is assumed when attrition occurs as a result of something either than explicit chance process (Rossi et al., 2004). This means that \"those individuals that were from the intervention group whose outcome data are missing cannot be assumed to have the same outcome-relevant characteristics as those from the control group whose outcome data are missing\" (Rossi et al., 2004, p271). However, random assignment designs are not safe from selection bias which is induced by attrition (Rossi et al., 2004).\n</p>\n<h3><span class=\"mw-headline\" id=\"Other_forms_of_bias\">Other forms of bias</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=7\" title=\"Edit section: Other forms of bias\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>There are other factors that can be responsible for bias in the results of an impact assessment. These generally have to do with events or experiences other than receiving the program that occur during the intervention. These biases include secular trends, interfering events and maturation (Rossi et al., 2004).\n</p>\n<h4><span class=\"mw-headline\" id=\"Secular_trends_or_secular_drift\">Secular trends or secular drift</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=8\" title=\"Edit section: Secular trends or secular drift\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Secular trends can be defined as being relatively long-term trends in the community, region or country. These are also termed secular drift and may produce changes that enhance or mask the apparent effects of a (Rossi et al., 2004). For example, when a community's birth rate is declining, a program to reduce fertility may appear effective because of bias stemming from that downward trend (Rossi et al., 2004, p273).\n</p>\n<h4><span class=\"mw-headline\" id=\"Interfering_events\">Interfering events</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=9\" title=\"Edit section: Interfering events\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Interfering events are similar to secular trends; in this case it is the short-term events that can produce changes that may introduce bias into estimates of program effect, such as a power outage disrupting communications or hampering the delivery of food supplements may interfere with a nutrition program (Rossi et al., 2004, p273).\n</p>\n<h4><span class=\"mw-headline\" id=\"Maturation\">Maturation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=10\" title=\"Edit section: Maturation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Impact evaluation needs to accommodate the fact that natural maturational and developmental processes can produce considerable change independently of the program. Including these changes in the estimates of program effects would result in bias estimates. An example of this form of bias would be a program to improve preventative health practices among adults may seem ineffective because health generally declines with age (Rossi et al., 2004, p273).\n</p><p>\"Careful maintenance of comparable circumstances for program and control groups between random assignment and outcome measurement should prevent bias from the influence of other differential experiences or events on the groups. If either of these conditions is absent from the design, there is potential for bias in the estimates of program effect\" (Rossi et al., 2004, p274).\n</p>\n<h2><span class=\"mw-headline\" id=\"Estimation_methods\">Estimation methods</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=11\" title=\"Edit section: Estimation methods\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Estimation methods broadly follow evaluation designs. Different designs require different estimation methods to measure changes in well-being from the counterfactual. In experimental and quasi-experimental evaluation, the estimated impact of the intervention is calculated as the difference in mean outcomes between the treatment group (those receiving the intervention) and the control or comparison group (those who don't). This method is also called randomised control trials (RCT). According to an interview with Jim Rough, former representant of the American Evaluation Assosiciation, in the magazine <a rel=\"nofollow\" class=\"external text\" href=\"http://www.dandc.eu/articles/220609/index.en.shtml\">D+C Development and Cooperation</a> this method doesn't work for complex, multilayer matters. The single difference estimator compares mean outcomes at end-line and is valid where treatment and control groups have the same outcome values at baseline. The difference-in-difference (or double difference) estimator calculates the difference in the change in the outcome over time for treatment and comparison groups, thus utilizing data collected at baseline for both groups and a second round of data collected at end-line, after implementation of the intervention, which may be years later.\n</p><p>Impact Evaluations which have to compare average outcomes in the treatment group, irrespective of beneficiary participation (also referred to as 'compliance' or 'adherence'), to outcomes in the comparison group are referred to as intention-to-treat (ITT) analyses. Impact Evaluations which compare outcomes among beneficiaries who comply or adhere to the intervention in the treatment group to outcomes in the control group are referred to as treatment-on-the-treated (TOT) analyses. ITT therefore provides a lower-bound estimate of impact, but is arguably of greater policy relevance than TOT in the analysis of voluntary programs.<sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\">&#91;14&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Debates\">Debates</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=12\" title=\"Edit section: Debates\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>While there is agreement on the importance of impact evaluation, and a consensus is emerging around the use of counterfactual evaluation methods, there has also been widespread debate in recent years on both the definition of impact evaluation and the use of appropriate methods (see White 2009<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\">&#91;15&#93;</a></sup> for an overview).\n</p>\n<h3><span class=\"mw-headline\" id=\"Definitions\">Definitions</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=13\" title=\"Edit section: Definitions\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The International Initiative for Impact Evaluation (3ie) defines rigorous impact evaluations as: \"analyses that measure the net change in outcomes for a particular group of people that can be attributed to a specific program using the best methodology available, feasible and appropriate to the evaluation question that is being investigated and to the specific context\".<sup id=\"cite_ref-16\" class=\"reference\"><a href=\"#cite_note-16\">&#91;16&#93;</a></sup>\n</p><p>According to the World Bank's DIME Initiative, \"Impact evaluations compare the outcomes of a program against a counterfactual that shows what would have happened to beneficiaries without the program. Unlike other forms of evaluation, they permit the attribution of observed changes in outcomes to the program being evaluated by following experimental and quasi-experimental designs\".<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup>\n</p><p>Similarly, according to the US <a href=\"/wiki/United_States_Environmental_Protection_Agency\" title=\"United States Environmental Protection Agency\">Environmental Protection Agency</a> impact evaluation is a form of evaluation that assesses the net effect of a program by comparing program outcomes with an estimate of what would have happened in the absence of a program.<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup>\n</p><p>According to the World Bank's <a href=\"/wiki/Independent_Evaluation_Group\" title=\"Independent Evaluation Group\">Independent Evaluation Group</a> (IEG), impact evaluation is the systematic identification of the effects positive or negative, intended or not on individual households, institutions, and the environment caused by a given development activity such as a program or project.<sup id=\"cite_ref-19\" class=\"reference\"><a href=\"#cite_note-19\">&#91;19&#93;</a></sup>\n</p><p>Impact evaluation has been defined differently over the past few decades.<sup id=\"cite_ref-worldbank.org_7-1\" class=\"reference\"><a href=\"#cite_note-worldbank.org-7\">&#91;7&#93;</a></sup> Other interpretations of impact evaluation include:\n</p>\n<ul><li>An evaluation which looks at the impact of an intervention on final welfare outcomes, rather than only at project outputs, or a process evaluation which focuses on implementation;</li>\n<li>An evaluation carried out some time (five to ten years) after the intervention has been completed so as to allow time for impact to appear; and</li>\n<li>An evaluation considering all interventions within a given sector or geographical area.</li></ul>\n<p>Other authors make a distinction between \"impact evaluation\" and \"impact assessment.\" \"Impact evaluation\" uses empirical techniques to estimate the effects of interventions and their statistical significance, whereas \"impact assessment\" includes a broader set of methods, including structural simulations and other approaches that cannot test for statistical significance.<sup id=\"cite_ref-:0_13-1\" class=\"reference\"><a href=\"#cite_note-:0-13\">&#91;13&#93;</a></sup>\n</p><p>Common definitions of 'impact' used in evaluation generally refer to the totality of longer-term consequences associated with an intervention on quality-of-life outcomes. For example, the Organization for Economic Cooperation and Development's Development Assistance Committee (OECD-DAC) defines impact as the \"positive and negative, primary and secondary long-term effects produced by a development intervention, directly or indirectly, intended or unintended\".<sup id=\"cite_ref-20\" class=\"reference\"><a href=\"#cite_note-20\">&#91;20&#93;</a></sup> A number of international agencies have also adopted this definition of impact. For example, UNICEF defines impact as \"The longer term results of a program \u2013 technical, economic, socio-cultural, institutional, environmental or other \u2013 whether intended or unintended. The intended impact should correspond to the program goal.\"<sup id=\"cite_ref-21\" class=\"reference\"><a href=\"#cite_note-21\">&#91;21&#93;</a></sup> Similarly, Evaluationwiki.org defines impact evaluation as an evaluation that looks beyond the immediate results of policies, instruction, or services to identify longer-term as well as unintended program effects.<sup id=\"cite_ref-22\" class=\"reference\"><a href=\"#cite_note-22\">&#91;22&#93;</a></sup>\n</p><p>Technically, an evaluation could be conducted to assess 'impact' as defined here without reference to a counterfactual. However, much of the existing literature (e.g. NONIE Guidelines on Impact Evaluation<sup id=\"cite_ref-worldbank.org1_23-0\" class=\"reference\"><a href=\"#cite_note-worldbank.org1-23\">&#91;23&#93;</a></sup> adopts the OECD-DAC definition of impact while referring to the techniques used to attribute impact to an intervention as necessarily based on counterfactual analysis.\n</p><p>What is missing from the term 'impact' evaluation is the way 'impact' shows up long-term. For instance, most Monitoring and Evaluation 'logical framework' plans have inputs-outputs-outcomes and... impacts. While the first three appear during the project duration itself, impact takes far longer to take place. For instance, in a 5-year agricultural project, seeds are inputs, farmers trained in using them our outputs, changes in crop yields as a result of the seeds being planted properly in an outcome and families being more sustainably food secure over time is an impact.  Such <a rel=\"nofollow\" class=\"external text\" href=\"http://valuingvoices.com/sustained-impact-post-project-ex-post-little-proof-at-3ie/\">post-project impact evaluations</a> are very rare. They are also called ex-post evaluations or we are coining the term <a rel=\"nofollow\" class=\"external text\" href=\"http://valuingvoices.com/what-happens-after-the-project-ends-lessons-from-post-project-sustained-impact-evaluations-part-1/\">sustained impact evaluations</a>. While hundreds of thousands of documents call for them, rarely do donors have the funding flexibility - or interest - to return to see how sustained, and durable our interventions remained after project close out, after resources were withdrawn. There are many <a rel=\"nofollow\" class=\"external text\" href=\"http://valuingvoices.com/what-happens-after-the-project-ends-lessons-from-post-project-sustained-impact-evaluations-part-1/\">lessons to be learned for design, implementation, M&amp;E</a> and how to foster <a rel=\"nofollow\" class=\"external text\" href=\"http://valuingvoices.com/what-happens-after-the-project-ends-country-national-ownership-lessons-from-post-project-sustained-impact-evaluations-part-2/\">country-ownership</a>.\n</p>\n<h3><span class=\"mw-headline\" id=\"Methodological_debates\">Methodological debates</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=14\" title=\"Edit section: Methodological debates\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>There is intensive debate in academic circles around the appropriate methodologies for impact evaluation, between proponents of experimental methods on the one hand and proponents of more general methodologies on the other. William Easterly has referred to this as <a rel=\"nofollow\" class=\"external text\" href=\"http://aidwatchers.com/2009/12/the-civil-war-in-development-economics/\">'The Civil War in Development economics'</a>. Proponents of experimental designs, sometimes referred to as 'randomistas',<sup id=\"cite_ref-auto_9-1\" class=\"reference\"><a href=\"#cite_note-auto-9\">&#91;9&#93;</a></sup> argue randomization is the only means to ensure unobservable selection bias is accounted for, and that building up the flimsy experimental evidence base should be developed as a matter of priority.<sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\">&#91;24&#93;</a></sup> In contrast, others argue that randomized assignment is seldom appropriate to development interventions and even when it is, experiments provide us with information on the results of a specific intervention applied to a specific context, and little of external relevance.<sup id=\"cite_ref-25\" class=\"reference\"><a href=\"#cite_note-25\">&#91;25&#93;</a></sup> There has been criticism from evaluation bodies and others that some donors and academics over-emphasize favoured methods for impact evaluation,<sup id=\"cite_ref-26\" class=\"reference\"><a href=\"#cite_note-26\">&#91;26&#93;</a></sup> and that this may in fact hinder learning and accountability.<sup id=\"cite_ref-27\" class=\"reference\"><a href=\"#cite_note-27\">&#91;27&#93;</a></sup> In addition, there has been a debate around the appropriate role for qualitative methods within impact evaluations.<sup id=\"cite_ref-28\" class=\"reference\"><a href=\"#cite_note-28\">&#91;28&#93;</a></sup><sup id=\"cite_ref-29\" class=\"reference\"><a href=\"#cite_note-29\">&#91;29&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Theory-based_impact_evaluation\">Theory-based impact evaluation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=15\" title=\"Edit section: Theory-based impact evaluation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>While knowledge of effectiveness is vital, it is also important to understand the reasons for effectiveness and the circumstances under which results are likely to be replicated. In contrast with 'black box' impact evaluation approaches, which only report mean differences in outcomes between treatment and comparison groups, theory-based impact evaluation involves mapping out the causal chain from inputs to outcomes and impact and testing the underlying assumptions.<sup id=\"cite_ref-3ieimpact.org_30-0\" class=\"reference\"><a href=\"#cite_note-3ieimpact.org-30\">&#91;30&#93;</a></sup><sup id=\"cite_ref-worldbank.org1_23-1\" class=\"reference\"><a href=\"#cite_note-worldbank.org1-23\">&#91;23&#93;</a></sup> Most interventions within the realm of public policy are of a voluntary, rather than coercive (legally required) nature. In addition, interventions are often active rather than passive, requiring a greater rather than lesser degree of participation among beneficiaries and therefore behavior change as a pre-requisite for effectiveness. Public policy will therefore be successful to the extent that people are incentivized to change their behaviour favourably. A theory-based approach enables policy-makers to understand the reasons for differing levels of program participation (referred to as 'compliance' or 'adherence') and the processes determining behavior change. Theory-Based approaches use both quantitative and qualitative data collection, and the latter can be particularly useful in understanding the reasons for compliance and therefore whether and how the intervention may be replicated in other settings. Methods of qualitative data collection include focus groups, in-depth interviews, participatory rural appraisal (PRA) and field visits, as well as reading of anthropological and political literature.\n</p><p>White (2009b)<sup id=\"cite_ref-3ieimpact.org_30-1\" class=\"reference\"><a href=\"#cite_note-3ieimpact.org-30\">&#91;30&#93;</a></sup> advocates more widespread application of a theory-based approach to impact evaluation as a means to improve policy relevance of impact evaluations, outlining six key principles of the theory-based approach:\n</p>\n<ol><li>Map out the causal chain (program theory) which explains how the intervention is expected to lead to the intended outcomes, and collect data to test the underlying assumptions of the causal links.</li>\n<li>Understand context, including the social, political and economic setting of the intervention.</li>\n<li>Anticipate heterogeneity to help in identifying sub-groups and adjusting the sample size to account for the levels of disaggregation to be used in the analysis.</li>\n<li>Rigorous evaluation of impact using a credible counterfactual (as discussed above).</li>\n<li>Rigorous factual analysis of links in the causal chain.</li>\n<li>Use mixed methods (a combination of quantitative and qualitative methods).</li></ol>\n<h2><span class=\"mw-headline\" id=\"Examples\">Examples</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=16\" title=\"Edit section: Examples\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>While experimental impact evaluation methodologies have been used to assess nutrition and water and sanitation interventions in developing countries since the 1980s, the first, and best known, application of experimental methods to a large-scale development program is the evaluation of the <a href=\"/wiki/Conditional_Cash_Transfer\" class=\"mw-redirect\" title=\"Conditional Cash Transfer\">Conditional Cash Transfer</a> (CCT) program Progresa (now called <a href=\"/wiki/Oportunidades\" title=\"Oportunidades\">Oportunidades</a>) in Mexico, which examined a range of development outcomes, including schooling, immunization rates and child work.<sup id=\"cite_ref-31\" class=\"reference\"><a href=\"#cite_note-31\">&#91;31&#93;</a></sup><sup id=\"cite_ref-32\" class=\"reference\"><a href=\"#cite_note-32\">&#91;32&#93;</a></sup> CCT programs have since been implemented by a number of governments in Latin America and elsewhere, and a report released by the World Bank in February 2009 examines the impact of CCTs across twenty countries.<sup id=\"cite_ref-33\" class=\"reference\"><a href=\"#cite_note-33\">&#91;33&#93;</a></sup>\n</p><p>More recently, impact evaluation has been applied to a range of interventions across social and productive sectors. 3ie has launched an online <a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org/database_of_impact_evaluations.html\">database of impact evaluations</a> covering studies conducted in low- and middle income countries. Other organisations publishing Impact Evaluations include <a rel=\"nofollow\" class=\"external text\" href=\"http://poverty-action.org/work/publications\">Innovations for Poverty Action</a>, the World Bank's <a rel=\"nofollow\" class=\"external text\" href=\"http://www.worldbank.org/dime\">DIME Initiative</a> and <a rel=\"nofollow\" class=\"external text\" href=\"http://www.worldbank.org/ieg/nonie/papers.html\">NONIE</a>. The <a href=\"/wiki/Independent_Evaluation_Group\" title=\"Independent Evaluation Group\">IEG</a> of the World Bank has systematically assessed and summarized the experience of ten impact evaluation of development programs in various sectors carried out over the past 20 years.<sup id=\"cite_ref-34\" class=\"reference\"><a href=\"#cite_note-34\">&#91;34&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Organizations_promoting_impact_evaluation_of_development_interventions\">Organizations promoting impact evaluation of development interventions</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=17\" title=\"Edit section: Organizations promoting impact evaluation of development interventions\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>In 2006, the Evaluation Gap Working Group<sup id=\"cite_ref-35\" class=\"reference\"><a href=\"#cite_note-35\">&#91;35&#93;</a></sup> argued for a major gap in the evidence on development interventions, and in particular for an independent body to be set up to plug the gap by funding and advocating for rigorous impact evaluation in low- and middle-income countries. The <a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org\">International Initiative for Impact Evaluation (3ie)</a> was set up in response to this report. 3ie seeks to improve the lives of poor people in low- and middle-income countries by providing, and summarizing, evidence of what works, when, why and for how much. 3ie operates a grant program, financing impact studies in low- and middle-income countries and synthetic reviews of existing evidence updated as new evidence appears, and supports quality impact evaluation through its quality assurance services.\n</p><p>Another initiative devoted to the evaluation of impacts is the <a rel=\"nofollow\" class=\"external text\" href=\"http://sustainablecommodities.org/cosa\">Committee on Sustainability Assessment (COSA)</a>.  COSA is a non-profit global consortium of institutions, sustained in partnership with the International Institute for Sustainable Development (IISD) <a href=\"/wiki/Sustainable_Commodity_Initiative\" title=\"Sustainable Commodity Initiative\">Sustainable Commodity Initiative</a>, the <a href=\"/wiki/United_Nations_Conference_on_Trade_and_Development\" title=\"United Nations Conference on Trade and Development\">United Nations Conference on Trade and Development</a> (UNCTAD), and the United Nations <a href=\"/wiki/International_Trade_Centre\" title=\"International Trade Centre\">International Trade Centre</a> (ITC). COSA is developing and applying an independent measurement tool to analyze the distinct social, environmental and economic impacts of agricultural practices, and in particular those associated with the implementation of specific sustainability programs (Organic, <a href=\"/wiki/Fairtrade\" class=\"mw-redirect\" title=\"Fairtrade\">Fairtrade</a> etc.).  The focus of the initiative is to establish global indicators and measurement tools which farmers, policy-makers, and industry can use to understand and improve their sustainability with different crops or agricultural sectors. COSA aims to facilitate this by enabling them to accurately calculate the relative costs and benefits of becoming involved in any given sustainability initiative.\n</p><p>A number of additional organizations have been established to promote impact evaluation globally, including <a rel=\"nofollow\" class=\"external text\" href=\"http://poverty-action.org/\">Innovations for Poverty Action</a>, the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.worldbank.org/en/programs/sief-trust-fund\">World Bank's Strategic Impact Evaluation Fund (SIEF)</a>, the World Bank's Development Impact Evaluation (DIME) Initiative, the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cgiar-ilac.org\">Institutional Learning and Change (ILAC) Initiative</a> of the CGIAR, and the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.worldbank.org/ieg/nonie/\">Network of Networks on Impact Evaluation (NONIE)</a>.\n</p>\n<h2><span class=\"mw-headline\" id=\"Systematic_reviews_of_impact_evidence\">Systematic reviews of impact evidence</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=18\" title=\"Edit section: Systematic reviews of impact evidence\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>A range of organizations are working to coordinate the production of <a href=\"/wiki/Systematic_reviews\" class=\"mw-redirect\" title=\"Systematic reviews\">systematic reviews</a>. Systematic reviews aim to bridge the research-policy divide by assessing the range of existing evidence on a particular topic, and presenting the information in an accessible format. Like rigorous impact evaluations, they are developed from a study Protocol which sets out a priori the criteria for study inclusion, search and methods of synthesis. Systematic reviews involve five key steps: determination of interventions, populations, outcomes and study designs to be included; searches to identify published and unpublished literature, and application of study inclusion criteria (relating to interventions, populations, outcomes and study design), as set out in study Protocol; coding of information from studies; presentation of quantitative estimates on intervention effectiveness using forest plots and, where interventions are determined as appropriately homogeneous, calculation of a pooled summary estimate using meta-analysis; finally, systematic reviews should be updated periodically as new evidence emerges. Systematic reviews may also involve the synthesis of qualitative information, for example relating to the barriers to, or facilitators of, intervention effectiveness.\n</p><p>Organizations supporting the production of systematic reviews include the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cochrane.org/\">Cochrane Collaboration</a>, which has been coordinating systematic reviews in the medical and public health fields since 1993, and publishes the <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20090414113230/http://www.cochrane-handbook.org/\">Cochrane Handbook</a> which is definitive systematic review methodology guide. In addition, the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.campbellcollaboration.org/\">Campbell Collaboration</a> has coordinated the production of systematic reviews of social interventions since 2000, and the International Initiative for Impact Evaluation (in partnership with the Campbell Collaboration) is funding systematic reviews of social programs in developing countries. Other organizations supporting systematic reviews include the <a rel=\"nofollow\" class=\"external text\" href=\"http://eppi.ioe.ac.uk/cms/Default.aspx\">Institute of Education's EPPI-Centre</a> and the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.york.ac.uk/inst/crd/\">University of York's Centre for Reviews and Dissemination</a>.\n</p><p>The body of evidence from systematic reviews is large and available through various online portals including the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.thecochranelibrary.com/\">Cochrane library</a>, the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.campbellcollaboration.org/library.php\">Campbell library</a>, and the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.crd.york.ac.uk/crdweb/\">Centre for Reviews and Dissemination</a>. The available evidence from Reviews of development interventions in low- and middle-income countries is being built up by organisations such as the <a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org/en/evidence/systematic-reviews/\">International Initiative for Impact Evaluation's synthetic reviews programme</a>.\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=19\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Econometrics\" title=\"Econometrics\">Econometrics</a></li>\n<li><a href=\"/wiki/Impact_assessment\" title=\"Impact assessment\">Impact assessment</a></li>\n<li><a href=\"/wiki/Outcomes_theory\" title=\"Outcomes theory\">Outcomes theory</a></li>\n<li><a href=\"/wiki/Participatory_impact_pathways_analysis\" title=\"Participatory impact pathways analysis\">Participatory impact pathways analysis</a></li>\n<li><a href=\"/wiki/Policy_analysis\" title=\"Policy analysis\">Policy analysis</a></li>\n<li><a href=\"/wiki/Policy_studies\" title=\"Policy studies\">Policy studies</a></li>\n<li><a href=\"/wiki/Program_evaluation\" title=\"Program evaluation\">Program evaluation</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=20\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"mw-references-wrap mw-references-columns\"><ol class=\"references\">\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://web.worldbank.org/WBSITE/EXTERNAL/TOPICS/EXTPOVERTY/EXTISPMA/0,,menuPK:384336~pagePK:149018~piPK:149093~theSitePK:384329,00.html\">World Bank Poverty Group on Impact Evaluation</a>, accessed on January 6, 2008</span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://lnweb90.worldbank.org/oed/oeddoclib.nsf/DocUNIDViewForJavaSearch/35BC420995BF58F8852571E00068C6BD/$file/impact_evaluation.pdf\">White, H. (2006) Impact Evaluation: The Experience of the Independent Evaluation Group of the World Bank, World Bank, Washington, D.C., p. 3</a></span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://publications.worldbank.org/index.php?main_page=product_info&amp;cPath=1&amp;products_id=23915\">Gertler, Martinez, Premand, Rawlings and Vermeersch (2011) Impact Evaluation in Practice, Washington, DC:The World Bank</a></span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org/media/filer/2012/05/07/Working_Paper_8.pdf\">\"Log in\"</a> <span style=\"font-size:85%;\">(PDF)</span><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Log+in&amp;rft_id=http%3A%2F%2Fwww.3ieimpact.org%2Fmedia%2Ffiler%2F2012%2F05%2F07%2FWorking_Paper_8.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.enterprise-development.org/page/download?id=2133,\"><i> Muaz, Jalil Mohammad (2013), Practical Guidelines for conducting research. Summarising good research practice in line with the DCED Standard</i></a></span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org/media/filer/2012/04/20/principles-for-impact-evaluation.pdf\">\"Log in\"</a> <span style=\"font-size:85%;\">(PDF)</span><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Log+in&amp;rft_id=http%3A%2F%2Fwww.3ieimpact.org%2Fmedia%2Ffiler%2F2012%2F04%2F20%2Fprinciples-for-impact-evaluation.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-worldbank.org-7\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-worldbank.org_7-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-worldbank.org_7-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://lnweb90.worldbank.org/oed/oeddoclib.nsf/DocUNIDViewForJavaSearch/35BC420995BF58F8852571E00068C6BD/$file/impact_evaluation.pdf\">White, H. (2006) Impact Evaluation: The Experience of the Independent Evaluation Group of the World Bank, World Bank, Washington, D.C.</a></span>\n</li>\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://siteresources.worldbank.org/INTISPMA/Resources/383704-1153333441931/Evaluating_Antipoverty_Programs.pdf\">Ravallion, M. (2008) Evaluating Anti-Poverty Programs</a></span>\n</li>\n<li id=\"cite_note-auto-9\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-auto_9-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-auto_9-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Martin, Ravallion (1 January 2009). <a rel=\"nofollow\" class=\"external text\" href=\"http://ideas.repec.org/a/bpj/evoice/v6y2009i2n6.html\">\"Should the Randomistas Rule?\"</a>. <b>6</b> (2): 1\u20135<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>  &#8211; via RePEc - IDEAS.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Should+the+Randomistas+Rule%3F&amp;rft.volume=6&amp;rft.issue=2&amp;rft.pages=1-5&amp;rft.date=2009-01-01&amp;rft.aulast=Martin&amp;rft.aufirst=Ravallion&amp;rft_id=http%3A%2F%2Fideas.repec.org%2Fa%2Fbpj%2Fevoice%2Fv6y2009i2n6.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-ed.gov-10\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-ed.gov_10-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-ed.gov_10-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.eric.ed.gov/ERICWebPortal/custom/portlets/recordDetails/detailmini.jsp?_nfpb=true&amp;_&amp;ERICExtSearch_SearchValue_0=EJ800319&amp;ERICExtSearch_SearchType_0=no&amp;accno=EJ800319\">Bamberger, M. and White, H. (2007) Using Strong Evaluation Designs in Developing Countries: Experience and Challenges, <i>Journal of MultiDisciplinary Evaluation</i>, Volume 4, Number 8, 58-73</a></span>\n</li>\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\">Scriven (2008) A Summative Evaluation of RCT Methodology: &amp; An Alternative Approach to Causal Research, <i>Journal of MultiDisciplinary Evaluation</i>, Volume 5, Number 9, 11-24</span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Deaton, Angus (1 January 2009). \"Instruments of Development: Randomization in the Tropics, and the Search for the Elusive Keys to Economic Development\". <a href=\"/wiki/Social_Science_Research_Network\" title=\"Social Science Research Network\">SSRN</a>&#160;<span class=\"plainlinks\"><a rel=\"nofollow\" class=\"external text\" href=\"//ssrn.com/abstract=1335715\">1335715</a>&#8239;<img alt=\"Freely accessible\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\" title=\"Freely accessible\" width=\"9\" height=\"14\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x\" data-file-width=\"512\" data-file-height=\"813\" /></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Instruments+of+Development%3A+Randomization+in+the+Tropics%2C+and+the+Search+for+the+Elusive+Keys+to+Economic+Development&amp;rft.date=2009-01-01&amp;rft_id=%2F%2Fssrn.com%2Fabstract%3D1335715&amp;rft.aulast=Deaton&amp;rft.aufirst=Angus&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> <span style=\"display:none;font-size:100%\" class=\"error citation-comment\">Missing or empty <code style=\"color:inherit; border:inherit; padding:inherit;\">&#124;url=</code> (<a href=\"/wiki/Help:CS1_errors#cite_web_url\" title=\"Help:CS1 errors\">help</a>)</span></span>\n</li>\n<li id=\"cite_note-:0-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:0_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:0_13-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">White, Howard; Raitzer, David (2017). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.adb.org/sites/default/files/publication/392376/impact-evaluation-development-interventions-guide.pdf\"><i>Impact Evaluation of Development Interventions: A Practical Guide</i></a> <span style=\"font-size:85%;\">(PDF)</span>. Manila: Asian Development Bank. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/978-92-9261-059-3\" title=\"Special:BookSources/978-92-9261-059-3\">978-92-9261-059-3</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Impact+Evaluation+of+Development+Interventions%3A+A+Practical+Guide&amp;rft.place=Manila&amp;rft.pub=Asian+Development+Bank&amp;rft.date=2017&amp;rft.isbn=978-92-9261-059-3&amp;rft.aulast=White&amp;rft.aufirst=Howard&amp;rft.au=Raitzer%2C+David&amp;rft_id=https%3A%2F%2Fwww.adb.org%2Fsites%2Fdefault%2Ffiles%2Fpublication%2F392376%2Fimpact-evaluation-development-interventions-guide.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.eric.ed.gov/PDFS/ED493363.pdf\">Bloom, H. (2006) The core analytics of randomized experiments for social research. MDRC Working Papers on Research Methodology. MDRC, New York</a></span>\n</li>\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org/en/evaluation/working-papers/working-paper-1/\">White, H. (2009) Some reflections on current debates in impact evaluation, Working paper 1, International Initiative for Impact Evaluation, New Delhi</a></span>\n</li>\n<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org/media/filer/2012/04/20/principles-for-impact-evaluation.pdf\">\"Log in\"</a> <span style=\"font-size:85%;\">(PDF)</span><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Log+in&amp;rft_id=http%3A%2F%2Fwww.3ieimpact.org%2Fmedia%2Ffiler%2F2012%2F04%2F20%2Fprinciples-for-impact-evaluation.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://siteresources.worldbank.org/INTDEVIMPEVAINI/Resources/DIME_project_document-rev.pdf\">World Bank (n.d.) The Development IMpact Evaluation (DIME) Initiative, Project Document, World Bank, Washington, D.C.</a></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.epa.gov/evaluate/glossary/i-esd.htm\">US Environmental Protection Agency Program Evaluation Glossary</a>, accessed on January 6, 2008</span>\n</li>\n<li id=\"cite_note-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-19\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.worldbank.org/ieg/ie/\">World Bank Independent Evaluation Group</a>, accessed on January 6, 2008</span>\n</li>\n<li id=\"cite_note-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-20\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.oecd.org/dataoecd/8/43/40501129.pdf\">OECD-DAC (2002) Glossary of Key Terms in Evaluation and Results-Based Management Proposed Harmonized Terminology, OECD, Paris</a></span>\n</li>\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.unicef.org/evaldatabase/files/UNICEF_Eval_Report_Standards.pdf\">UNICEF (2004) UNICEF Evaluation Report Standards, Evaluation Office, UNICEF NYHQ, New York</a></span>\n</li>\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.evaluationwiki.org/index.php/Evaluation_Definition:_What_is_Evaluation%3F#Impact_Evaluations\">\"Evaluation Definition: What is Evaluation? - EvaluationWiki\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Evaluation+Definition%3A+What+is+Evaluation%3F+-+EvaluationWiki&amp;rft_id=http%3A%2F%2Fwww.evaluationwiki.org%2Findex.php%2FEvaluation_Definition%3A_What_is_Evaluation%253F%23Impact_Evaluations&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-worldbank.org1-23\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-worldbank.org1_23-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-worldbank.org1_23-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.worldbank.org/ieg/nonie/guidance.html\">\"Page Not Found\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Page+Not+Found&amp;rft_id=http%3A%2F%2Fwww.worldbank.org%2Fieg%2Fnonie%2Fguidance.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.mdgoals.net/wp-content/uploads/banerjee.pdf\">\"Banerjee, A. V. (2007) 'Making Aid Work' Cambridge, Boston Review Book, MIT Press, MA\"</a> <span style=\"font-size:85%;\">(PDF)</span><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Banerjee%2C+A.+V.+%282007%29+%27Making+Aid+Work%27+Cambridge%2C+Boston+Review+Book%2C+MIT+Press%2C+MA&amp;rft_id=http%3A%2F%2Fwww.mdgoals.net%2Fwp-content%2Fuploads%2Fbanerjee.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-25\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.eric.ed.gov/ERICWebPortal/custom/portlets/recordDetails/detailmini.jsp?_nfpb=true&amp;_&amp;ERICExtSearch_SearchValue_0=EJ800319&amp;ERICExtSearch_SearchType_0=no&amp;accno=EJ800319\">Bamberger, M. and White, H. (2007) Using Strong Evaluation Designs in Developing Countries: Experience and Challenges, Journal of MultiDisciplinary Evaluation, Volume 4, Number 8, 58-73</a></span>\n</li>\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external free\" href=\"http://www.europeanevaluation.org/download/?noGzip=1&amp;id=1969403\">http://www.europeanevaluation.org/download/?noGzip=1&amp;id=1969403</a> EES Statement on the importance of a methodologically diverse approach to impact evaluation</span>\n</li>\n<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external free\" href=\"http://www.odi.org.uk/resources/odi-publications/opinions/127-impact-evaluation.pdf\">http://www.odi.org.uk/resources/odi-publications/opinions/127-impact-evaluation.pdf</a> The 'gold standard' is not a silver bullet for evaluation</span>\n</li>\n<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external free\" href=\"https://www.odi.org/publications/430-aid-effectiveness-role-qualitative-research-impact-evaluation\">https://www.odi.org/publications/430-aid-effectiveness-role-qualitative-research-impact-evaluation</a></span>\n</li>\n<li id=\"cite_note-29\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-29\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external free\" href=\"http://journals.sagepub.com/doi/pdf/10.1177/146499341201300104\">http://journals.sagepub.com/doi/pdf/10.1177/146499341201300104</a></span>\n</li>\n<li id=\"cite_note-3ieimpact.org-30\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-3ieimpact.org_30-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-3ieimpact.org_30-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org/en/evaluation/working-papers/working-paper-3/\">White, H. (2009b) Theory-based impact evaluation: Principles and practice, Working Paper 3, International Initiative for Impact Evaluation, New Delhi</a></span>\n</li>\n<li id=\"cite_note-31\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-31\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.ifpri.org/sites/default/files/publications/gertler_health.pdf\">Gertler, P. (2000) Final Report: The Impact of PROGRESA on Health. International Food Policy Research Institute, Washington, D.C.</a></span>\n</li>\n<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://athena.sas.upenn.edu/~petra/papers/trans18.pdf\">\"Untitled Document\"</a> <span style=\"font-size:85%;\">(PDF)</span><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=Untitled+Document&amp;rft_id=http%3A%2F%2Fathena.sas.upenn.edu%2F~petra%2Fpapers%2Ftrans18.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-33\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-33\">^</a></b></span> <span class=\"reference-text\">Fiszbein, A. and Schady, N. (2009) Conditional Cash Transfers: Reducing present and future poverty: A World Bank Policy Research Report, World Bank, Washington, D.C.</span>\n</li>\n<li id=\"cite_note-34\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-34\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://lnweb18.worldbank.org/oed/oeddoclib.nsf/DocUNIDViewForJavaSearch/35BC420995BF58F8852571E00068C6BD/$file/impact_evaluation.pdf\">Impact Evaluation: The Experience of the Independent Evaluation Group of the World Bank, 2006</a></span>\n</li>\n<li id=\"cite_note-35\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-35\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.cgdev.org/content/publications/detail/7973\">\"When Will We Ever Learn? Improving Lives Through Impact Evaluation\"</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">16 January</span> 2017</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=When+Will+We+Ever+Learn%3F+Improving+Lives+Through+Impact+Evaluation&amp;rft_id=http%3A%2F%2Fwww.cgdev.org%2Fcontent%2Fpublications%2Fdetail%2F7973&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImpact+evaluation\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n</ol></div>\n<h2><span class=\"mw-headline\" id=\"Sources_and_external_links\">Sources and external links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Impact_evaluation&amp;action=edit&amp;section=21\" title=\"Edit section: Sources and external links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.worldbank.org/ieinpractice\">Gertler, Martinez, Premand, Rawlings and Vermeersch (2011) Impact Evaluation in Practice, Washington, DC:The World Bank</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://web.worldbank.org/WBSITE/EXTERNAL/TOPICS/EXTPOVERTY/EXTISPMA/0,,menuPK:384336~pagePK:149018~piPK:149093~theSitePK:384329,00.html\">World Bank Poverty Group World Bank Poverty Group</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.worldbank.org/ieg/ie/\">World Bank Independent Evaluation Group</a> or in Wikipedia <a href=\"/wiki/Independent_Evaluation_Group\" title=\"Independent Evaluation Group\">Independent Evaluation Group</a></li>\n<li>Baker, Judy. 2000. <a rel=\"nofollow\" class=\"external text\" href=\"http://web.worldbank.org/WBSITE/EXTERNAL/TOPICS/EXTPOVERTY/EXTISPMA/0,,contentMDK:20194198~menuPK:451260~pagePK:148956~piPK:216618~theSitePK:384329,00.html\">Evaluating the Impact of Development Projects on Poverty: A Handbook for Practitioners.</a> Directions in Development, World Bank, Washington, D.C.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.3ieimpact.org\">International Initiative for Impact Evaluation</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://poverty-action.org/\">Innovations for Poverty Action</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.cochrane.org/\">Cochrane Collaboration</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.campbellcollaboration.org/\">Campbell Collaboration</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.sustainablecommodities.org/cosa\">Committee on Sustainability Assessment (COSA)</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.iisd.org\">International Institute for Sustainable Development (IISD)</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.intracen.org\">UN International Trade Centre (ITC)</a></li></ul>\n\n<!-- \nNewPP limit report\nParsed by mw1261\nCached time: 20180910191441\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.188 seconds\nReal time usage: 0.225 seconds\nPreprocessor visited node count: 792/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 20405/2097152 bytes\nTemplate argument size: 9/2097152 bytes\nHighest expansion depth: 6/40\nExpensive parser function count: 2/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 24457/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.072/10.000 seconds\nLua memory usage: 3.3 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  155.405      1 -total\n 37.11%   57.672      9 Template:Cite_web\n 29.55%   45.920      1 Template:External_links\n 19.98%   31.044      1 Template:Ambox\n  6.17%    9.593      1 Template:Further\n  3.88%    6.028      1 Template:Cite_book\n  3.79%    5.897      1 Template:Cite_journal\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:15092946-0!canonical and timestamp 20180910191441 and revision id 843969668\n -->\n</div>"},"langlinks":[],"categories":[{"sortkey":"","hidden":"","*":"Pages_using_web_citations_with_no_URL"},{"sortkey":"Impact Evaluation","hidden":"","*":"Wikipedia_external_links_cleanup_from_June_2017"},{"sortkey":"Impact Evaluation","hidden":"","*":"Wikipedia_spam_cleanup_from_June_2017"},{"sortkey":"Impact Evaluation","*":"Impact_assessment"},{"sortkey":"Impact Evaluation","*":"Philosophy_of_science"},{"sortkey":"Impact Evaluation","*":"Educational_evaluation_methods"},{"sortkey":"Impact Evaluation","*":"Observational_study"}],"links":[{"ns":14,"exists":"","*":"Category:Wikipedia external links cleanup from June 2017"},{"ns":14,"exists":"","*":"Category:Wikipedia spam cleanup from June 2017"},{"ns":0,"exists":"","*":"Conditional Cash Transfer"},{"ns":0,"exists":"","*":"Control group"},{"ns":0,"exists":"","*":"Counterfactual conditional"},{"ns":0,"exists":"","*":"Econometrics"},{"ns":0,"exists":"","*":"Evaluation"},{"ns":0,"exists":"","*":"Experimental design"},{"ns":0,"exists":"","*":"Fairtrade"},{"ns":0,"exists":"","*":"Impact assessment"},{"ns":0,"exists":"","*":"Independent Evaluation Group"},{"ns":0,"exists":"","*":"Instrumental Variable"},{"ns":0,"exists":"","*":"International Standard Book Number"},{"ns":0,"exists":"","*":"International Trade Centre"},{"ns":0,"exists":"","*":"Interrupted time-series"},{"ns":0,"exists":"","*":"Oportunidades"},{"ns":0,"exists":"","*":"Outcomes theory"},{"ns":0,"exists":"","*":"Participatory impact pathways analysis"},{"ns":0,"exists":"","*":"Policy analysis"},{"ns":0,"exists":"","*":"Policy studies"},{"ns":0,"exists":"","*":"Program evaluation"},{"ns":0,"exists":"","*":"Propensity score matching"},{"ns":0,"exists":"","*":"Quasi-experiment"},{"ns":0,"exists":"","*":"Randomized controlled trial"},{"ns":0,"exists":"","*":"Regression analysis"},{"ns":0,"exists":"","*":"Regression discontinuity design"},{"ns":0,"exists":"","*":"Selection bias"},{"ns":0,"exists":"","*":"Social Science Research Network"},{"ns":0,"exists":"","*":"Stepped-wedge trial"},{"ns":0,"exists":"","*":"Sustainable Commodity Initiative"},{"ns":0,"exists":"","*":"Systematic reviews"},{"ns":0,"exists":"","*":"United Nations Conference on Trade and Development"},{"ns":0,"exists":"","*":"United States Environmental Protection Agency"},{"ns":0,"exists":"","*":"Unobserved heterogeneity"},{"ns":4,"exists":"","*":"Wikipedia:Citing sources"},{"ns":4,"exists":"","*":"Wikipedia:External links"},{"ns":4,"exists":"","*":"Wikipedia:What Wikipedia is not"},{"ns":12,"exists":"","*":"Help:CS1 errors"},{"ns":12,"exists":"","*":"Help:Maintenance template removal"}],"templates":[{"ns":10,"exists":"","*":"Template:External links"},{"ns":10,"exists":"","*":"Template:Ambox"},{"ns":10,"exists":"","*":"Template:Further"},{"ns":10,"exists":"","*":"Template:Cite web"},{"ns":10,"exists":"","*":"Template:Cite journal"},{"ns":10,"exists":"","*":"Template:Cite book"},{"ns":828,"exists":"","*":"Module:Unsubst"},{"ns":828,"exists":"","*":"Module:Message box"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Message box/configuration"},{"ns":828,"exists":"","*":"Module:Category handler"},{"ns":828,"exists":"","*":"Module:Category handler/data"},{"ns":828,"exists":"","*":"Module:Category handler/config"},{"ns":828,"exists":"","*":"Module:Category handler/shared"},{"ns":828,"exists":"","*":"Module:Category handler/blacklist"},{"ns":828,"exists":"","*":"Module:Namespace detect/data"},{"ns":828,"exists":"","*":"Module:Namespace detect/config"},{"ns":828,"exists":"","*":"Module:Labelled list hatnote"},{"ns":828,"exists":"","*":"Module:Hatnote"},{"ns":828,"exists":"","*":"Module:Hatnote list"},{"ns":828,"exists":"","*":"Module:Citation/CS1"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Configuration"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Whitelist"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Utilities"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Date validation"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Identifiers"},{"ns":828,"exists":"","*":"Module:Citation/CS1/COinS"}],"images":["Lock-green.svg","Edit-clear.svg"],"externallinks":["http://web.worldbank.org/WBSITE/EXTERNAL/TOPICS/EXTPOVERTY/EXTISPMA/0,,menuPK:384336~pagePK:149018~piPK:149093~theSitePK:384329,00.html","http://lnweb90.worldbank.org/oed/oeddoclib.nsf/DocUNIDViewForJavaSearch/35BC420995BF58F8852571E00068C6BD/$file/impact_evaluation.pdf","http://publications.worldbank.org/index.php?main_page=product_info&cPath=1&products_id=23915","http://www.3ieimpact.org/media/filer/2012/05/07/Working_Paper_8.pdf","http://www.enterprise-development.org/page/download?id=2133,","http://www.3ieimpact.org/media/filer/2012/04/20/principles-for-impact-evaluation.pdf","http://siteresources.worldbank.org/INTISPMA/Resources/383704-1153333441931/Evaluating_Antipoverty_Programs.pdf","http://ideas.repec.org/a/bpj/evoice/v6y2009i2n6.html","http://www.eric.ed.gov/ERICWebPortal/custom/portlets/recordDetails/detailmini.jsp?_nfpb=true&_&ERICExtSearch_SearchValue_0=EJ800319&ERICExtSearch_SearchType_0=no&accno=EJ800319","//ssrn.com/abstract=1335715","https://www.adb.org/sites/default/files/publication/392376/impact-evaluation-development-interventions-guide.pdf","http://www.eric.ed.gov/PDFS/ED493363.pdf","http://www.3ieimpact.org/en/evaluation/working-papers/working-paper-1/","http://siteresources.worldbank.org/INTDEVIMPEVAINI/Resources/DIME_project_document-rev.pdf","http://www.epa.gov/evaluate/glossary/i-esd.htm","http://www.worldbank.org/ieg/ie/","http://www.oecd.org/dataoecd/8/43/40501129.pdf","http://www.unicef.org/evaldatabase/files/UNICEF_Eval_Report_Standards.pdf","http://www.evaluationwiki.org/index.php/Evaluation_Definition:_What_is_Evaluation%3F#Impact_Evaluations","http://www.worldbank.org/ieg/nonie/guidance.html","http://www.mdgoals.net/wp-content/uploads/banerjee.pdf","http://www.3ieimpact.org/en/evaluation/working-papers/working-paper-3/","http://www.ifpri.org/sites/default/files/publications/gertler_health.pdf","http://athena.sas.upenn.edu/~petra/papers/trans18.pdf","http://lnweb18.worldbank.org/oed/oeddoclib.nsf/DocUNIDViewForJavaSearch/35BC420995BF58F8852571E00068C6BD/$file/impact_evaluation.pdf","http://www.cgdev.org/content/publications/detail/7973","http://www.europeanevaluation.org/download/?noGzip=1&id=1969403","http://www.odi.org.uk/resources/odi-publications/opinions/127-impact-evaluation.pdf","https://www.odi.org/publications/430-aid-effectiveness-role-qualitative-research-impact-evaluation","http://journals.sagepub.com/doi/pdf/10.1177/146499341201300104","http://www.dandc.eu/articles/220609/index.en.shtml","http://valuingvoices.com/sustained-impact-post-project-ex-post-little-proof-at-3ie/","http://valuingvoices.com/what-happens-after-the-project-ends-lessons-from-post-project-sustained-impact-evaluations-part-1/","http://valuingvoices.com/what-happens-after-the-project-ends-country-national-ownership-lessons-from-post-project-sustained-impact-evaluations-part-2/","http://aidwatchers.com/2009/12/the-civil-war-in-development-economics/","http://www.3ieimpact.org/database_of_impact_evaluations.html","http://poverty-action.org/work/publications","http://www.worldbank.org/dime","http://www.worldbank.org/ieg/nonie/papers.html","http://www.3ieimpact.org","http://sustainablecommodities.org/cosa","http://poverty-action.org/","http://www.worldbank.org/en/programs/sief-trust-fund","http://www.cgiar-ilac.org","http://www.worldbank.org/ieg/nonie/","http://www.cochrane.org/","https://web.archive.org/web/20090414113230/http://www.cochrane-handbook.org/","http://www.campbellcollaboration.org/","http://eppi.ioe.ac.uk/cms/Default.aspx","http://www.york.ac.uk/inst/crd/","http://www.thecochranelibrary.com/","http://www.campbellcollaboration.org/library.php","http://www.crd.york.ac.uk/crdweb/","http://www.3ieimpact.org/en/evidence/systematic-reviews/","http://www.worldbank.org/ieinpractice","http://web.worldbank.org/WBSITE/EXTERNAL/TOPICS/EXTPOVERTY/EXTISPMA/0,,contentMDK:20194198~menuPK:451260~pagePK:148956~piPK:216618~theSitePK:384329,00.html","http://www.sustainablecommodities.org/cosa","http://www.iisd.org","http://www.intracen.org"],"sections":[{"toclevel":1,"level":"2","line":"Counterfactual evaluation designs","number":"1","index":"1","fromtitle":"Impact_evaluation","byteoffset":2673,"anchor":"Counterfactual_evaluation_designs"},{"toclevel":2,"level":"3","line":"Experimental design","number":"1.1","index":"2","fromtitle":"Impact_evaluation","byteoffset":7522,"anchor":"Experimental_design"},{"toclevel":2,"level":"3","line":"Quasi-experimental design","number":"1.2","index":"3","fromtitle":"Impact_evaluation","byteoffset":10733,"anchor":"Quasi-experimental_design"},{"toclevel":2,"level":"3","line":"Non-experimental design","number":"1.3","index":"4","fromtitle":"Impact_evaluation","byteoffset":12858,"anchor":"Non-experimental_design"},{"toclevel":1,"level":"2","line":"Biases in estimating programme effects","number":"2","index":"5","fromtitle":"Impact_evaluation","byteoffset":14172,"anchor":"Biases_in_estimating_programme_effects"},{"toclevel":2,"level":"3","line":"Selection bias","number":"2.1","index":"6","fromtitle":"Impact_evaluation","byteoffset":17381,"anchor":"Selection_bias"},{"toclevel":2,"level":"3","line":"Other forms of bias","number":"2.2","index":"7","fromtitle":"Impact_evaluation","byteoffset":19423,"anchor":"Other_forms_of_bias"},{"toclevel":3,"level":"4","line":"Secular trends or secular drift","number":"2.2.1","index":"8","fromtitle":"Impact_evaluation","byteoffset":19765,"anchor":"Secular_trends_or_secular_drift"},{"toclevel":3,"level":"4","line":"Interfering events","number":"2.2.2","index":"9","fromtitle":"Impact_evaluation","byteoffset":20226,"anchor":"Interfering_events"},{"toclevel":3,"level":"4","line":"Maturation","number":"2.2.3","index":"10","fromtitle":"Impact_evaluation","byteoffset":20591,"anchor":"Maturation"},{"toclevel":1,"level":"2","line":"Estimation methods","number":"3","index":"11","fromtitle":"Impact_evaluation","byteoffset":21443,"anchor":"Estimation_methods"},{"toclevel":1,"level":"2","line":"Debates","number":"4","index":"12","fromtitle":"Impact_evaluation","byteoffset":23546,"anchor":"Debates"},{"toclevel":2,"level":"3","line":"Definitions","number":"4.1","index":"13","fromtitle":"Impact_evaluation","byteoffset":24108,"anchor":"Definitions"},{"toclevel":2,"level":"3","line":"Methodological debates","number":"4.2","index":"14","fromtitle":"Impact_evaluation","byteoffset":30805,"anchor":"Methodological_debates"},{"toclevel":2,"level":"3","line":"Theory-based impact evaluation","number":"4.3","index":"15","fromtitle":"Impact_evaluation","byteoffset":33192,"anchor":"Theory-based_impact_evaluation"},{"toclevel":1,"level":"2","line":"Examples","number":"5","index":"16","fromtitle":"Impact_evaluation","byteoffset":36033,"anchor":"Examples"},{"toclevel":1,"level":"2","line":"Organizations promoting impact evaluation of development interventions","number":"6","index":"17","fromtitle":"Impact_evaluation","byteoffset":38307,"anchor":"Organizations_promoting_impact_evaluation_of_development_interventions"},{"toclevel":1,"level":"2","line":"Systematic reviews of impact evidence","number":"7","index":"18","fromtitle":"Impact_evaluation","byteoffset":41077,"anchor":"Systematic_reviews_of_impact_evidence"},{"toclevel":1,"level":"2","line":"See also","number":"8","index":"19","fromtitle":"Impact_evaluation","byteoffset":43999,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"References","number":"9","index":"20","fromtitle":"Impact_evaluation","byteoffset":44193,"anchor":"References"},{"toclevel":1,"level":"2","line":"Sources and external links","number":"10","index":"21","fromtitle":"Impact_evaluation","byteoffset":44225,"anchor":"Sources_and_external_links"}],"parsewarnings":[],"displaytitle":"Impact evaluation","iwlinks":[],"properties":[{"name":"defaultsort","*":"Impact Evaluation"},{"name":"wikibase_item","*":"Q17055852"}]}}