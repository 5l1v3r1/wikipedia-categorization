{"parse":{"title":"ImageNet","pageid":50896194,"revid":854252429,"text":{"*":"<div class=\"mw-parser-output\"><p>The <b>ImageNet</b> project is a large visual <a href=\"/wiki/Database\" title=\"Database\">database</a> designed for use in <a href=\"/wiki/Outline_of_object_recognition\" title=\"Outline of object recognition\">visual object recognition software</a> research. Over 14 million<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup><sup id=\"cite_ref-nytimes_2012_2-0\" class=\"reference\"><a href=\"#cite_note-nytimes_2012-2\">&#91;2&#93;</a></sup> URLs of images have been hand-annotated by ImageNet to indicate what objects are pictured; in at least one million of the images, bounding boxes are also provided.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup> ImageNet contains over 20 thousand categories;<sup id=\"cite_ref-nytimes_2012_2-1\" class=\"reference\"><a href=\"#cite_note-nytimes_2012-2\">&#91;2&#93;</a></sup> a typical category, such as \"balloon\" or \"strawberry\", contains several hundred images.<sup id=\"cite_ref-economist_4-0\" class=\"reference\"><a href=\"#cite_note-economist-4\">&#91;4&#93;</a></sup> The database of annotations of third-party image URL's is freely available directly from ImageNet; however, the actual images are not owned by ImageNet.<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\">&#91;5&#93;</a></sup> Since 2010, the ImageNet project runs an annual software contest, the ImageNet Large Scale Visual Recognition Challenge (<a href=\"#ImageNet_Challenge\">ILSVRC</a>), where software programs compete to correctly classify and detect objects and scenes. The ImageNet Challenge uses a \"trimmed\" list of one thousand non-overlapping classes.<sup id=\"cite_ref-ILJVRC-2015_6-0\" class=\"reference\"><a href=\"#cite_note-ILJVRC-2015-6\">&#91;6&#93;</a></sup>\n</p><p>A dramatic 2012 breakthrough in solving the ImageNet Challenge is widely considered to be the beginning of the <a href=\"/wiki/Deep_learning\" title=\"Deep learning\">deep learning</a> revolution of the 2010s: \"Suddenly people started to pay attention, not just within the AI community but across the technology industry as a whole.\"<sup id=\"cite_ref-economist_4-1\" class=\"reference\"><a href=\"#cite_note-economist-4\">&#91;4&#93;</a></sup><sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;7&#93;</a></sup>\n</p><p><br />\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#History\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">History</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Dataset\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Dataset</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#ImageNet_Challenge\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">ImageNet Challenge</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Non-competition_results\"><span class=\"tocnumber\">3.1</span> <span class=\"toctext\">Non-competition results</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#See_also\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#References\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#External_links\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"History\">History</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=ImageNet&amp;action=edit&amp;section=1\" title=\"Edit section: History\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The database was presented for the first time as a poster at the 2009 <a href=\"/wiki/Conference_on_Computer_Vision_and_Pattern_Recognition\" title=\"Conference on Computer Vision and Pattern Recognition\">Conference on Computer Vision and Pattern Recognition</a> (CVPR) in Florida by researchers from the Computer Science department at <a href=\"/wiki/Princeton_University\" title=\"Princeton University\">Princeton University</a>.<sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;8&#93;</a></sup><sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup>\n</p>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a href=\"/wiki/File:ImageNet_error_rate_history_(just_systems).svg\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ImageNet_error_rate_history_%28just_systems%29.svg/220px-ImageNet_error_rate_history_%28just_systems%29.svg.png\" width=\"220\" height=\"269\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ImageNet_error_rate_history_%28just_systems%29.svg/330px-ImageNet_error_rate_history_%28just_systems%29.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/4/4f/ImageNet_error_rate_history_%28just_systems%29.svg/440px-ImageNet_error_rate_history_%28just_systems%29.svg.png 2x\" data-file-width=\"810\" data-file-height=\"990\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:ImageNet_error_rate_history_(just_systems).svg\" class=\"internal\" title=\"Enlarge\"></a></div>Error rate history on ImageNet (showing best result per team and up to 10 entries per year)</div></div></div>\n<h2><span class=\"mw-headline\" id=\"Dataset\">Dataset</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=ImageNet&amp;action=edit&amp;section=2\" title=\"Edit section: Dataset\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>ImageNet <a href=\"/wiki/Crowdsources\" class=\"mw-redirect\" title=\"Crowdsources\">crowdsources</a> its annotation process. Image-level annotations indicate the presence or absence of an object class in an image, such as \"there are tigers in this image\" or \"there are no tigers in this image\". Object-level annotations provide a bounding box around the (visible part of the) indicated object. ImageNet uses a variant of the broad <a href=\"/wiki/WordNet\" title=\"WordNet\">WordNet</a> schema to categorize objects, augmented with 120 categories of <a href=\"/wiki/Dog_breeds\" class=\"mw-redirect\" title=\"Dog breeds\">dog breeds</a> to showcase fine-grained classification.<sup id=\"cite_ref-ILJVRC-2015_6-1\" class=\"reference\"><a href=\"#cite_note-ILJVRC-2015-6\">&#91;6&#93;</a></sup> One downside of WordNet use is the categories may be more \"elevated\" than would be optimal for ImageNet: \"Most people are more interested in Lady Gaga or the iPod Mini than in this rare kind of <a href=\"/wiki/Diplodocus\" title=\"Diplodocus\">diplodocus</a>.\" In 2012 ImageNet was the world's largest academic user of <a href=\"/wiki/Amazon_Mechanical_Turk\" title=\"Amazon Mechanical Turk\">Mechanical Turk</a>. The average worker identified 50 images per minute.<sup id=\"cite_ref-nytimes_2012_2-2\" class=\"reference\"><a href=\"#cite_note-nytimes_2012-2\">&#91;2&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"ImageNet_Challenge\">ImageNet Challenge</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=ImageNet&amp;action=edit&amp;section=3\" title=\"Edit section: ImageNet Challenge\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Since 2010, the annual ImageNet Large Scale Visual Recognition Challenge (ILSVRC) is a competition where research teams evaluate their algorithms on the given data set, and compete to achieve higher accuracy on several visual recognition tasks. The ILSVRC aims to \"follow in the footsteps\" of the smaller-scale PASCAL VOC challenge, established in 2005, which contained only about 20,000 images and twenty object classes.<sup id=\"cite_ref-ILJVRC-2015_6-2\" class=\"reference\"><a href=\"#cite_note-ILJVRC-2015-6\">&#91;6&#93;</a></sup> The ILSVRC uses a \"trimmed\" list of only 1000 image categories or \"classes\", including 90 of the 120 dog breeds classified by the full ImageNet schema.<sup id=\"cite_ref-ILJVRC-2015_6-3\" class=\"reference\"><a href=\"#cite_note-ILJVRC-2015-6\">&#91;6&#93;</a></sup> The 2010s saw dramatic progress in image processing. Around 2011, a good ILSVRC classification error rate was 25%. In 2012, a <a href=\"/wiki/Deep_learning#Convolutional_neural_networks\" title=\"Deep learning\">deep convolutional neural net</a> achieved 16%; in the next couple of years, error rates fell to a few percent.<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\">&#91;10&#93;</a></sup> While the 2012 breakthrough \"combined pieces that were all there before\", the dramatic quantitative improvement marked the start of an industry-wide artificial intelligence boom.<sup id=\"cite_ref-economist_4-2\" class=\"reference\"><a href=\"#cite_note-economist-4\">&#91;4&#93;</a></sup> By 2015, researchers reported that software exceeded human ability at the narrow ILSVRC tasks.<sup id=\"cite_ref-11\" class=\"reference\"><a href=\"#cite_note-11\">&#91;11&#93;</a></sup> However, as one of the challenge's organisers, Olga Russakovsky, pointed out in 2015, the programs only have to identify images as belonging to one of a thousand categories; humans can recognize a larger number of categories, and also (unlike the programs) can judge the context of an image.<sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup>\n</p><p>By 2014, over fifty institutions participated in the ILSVRC.<sup id=\"cite_ref-ILJVRC-2015_6-4\" class=\"reference\"><a href=\"#cite_note-ILJVRC-2015-6\">&#91;6&#93;</a></sup> In 2015, <a href=\"/wiki/Baidu\" title=\"Baidu\">Baidu</a> scientists were banned for a year for using different accounts to greatly exceed the specified limit of two submissions per week.<sup id=\"cite_ref-13\" class=\"reference\"><a href=\"#cite_note-13\">&#91;13&#93;</a></sup><sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\">&#91;14&#93;</a></sup> Baidu later stated that it fired the team leader involved and that it would establish a scientific advisory panel.<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\">&#91;15&#93;</a></sup>\n</p><p>In 2017, 29 of 38 competing teams got less than 5% wrong.<sup id=\"cite_ref-16\" class=\"reference\"><a href=\"#cite_note-16\">&#91;16&#93;</a></sup> In 2017 ImageNet stated it would roll out a new, much more difficult, challenge in 2018 that involves classifying 3D objects using natural language. Because creating 3D data is more costly than annotating a pre-existing 2D image, the dataset is expected to be smaller. The applications of progress in this area would range from robotic navigation to <a href=\"/wiki/Augmented_reality\" title=\"Augmented reality\">augmented reality</a>.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Non-competition_results\">Non-competition results</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=ImageNet&amp;action=edit&amp;section=4\" title=\"Edit section: Non-competition results\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Around November 2017, Google's AutoML project to evolve new neural net topologies created NASNet, a system optimized for ImageNet and COCO. According to Google, NASNet's performance exceeded all previously published ImageNet performance.<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=ImageNet&amp;action=edit&amp;section=5\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Computer_vision\" title=\"Computer vision\">Computer vision</a></li>\n<li><a href=\"/wiki/List_of_datasets_for_machine_learning_research\" title=\"List of datasets for machine learning research\">List of datasets for machine learning research</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=ImageNet&amp;action=edit&amp;section=6\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\n<ol class=\"references\">\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.newscientist.com/article/2127131-new-computer-vision-challenge-wants-to-teach-robots-to-see-in-3d/\">\"New computer vision challenge wants to teach robots to see in 3D\"</a>. <i>New Scientist</i>. 7 April 2017<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">3 February</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+Scientist&amp;rft.atitle=New+computer+vision+challenge+wants+to+teach+robots+to+see+in+3D&amp;rft.date=2017-04-07&amp;rft_id=https%3A%2F%2Fwww.newscientist.com%2Farticle%2F2127131-new-computer-vision-challenge-wants-to-teach-robots-to-see-in-3d%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-nytimes_2012-2\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-nytimes_2012_2-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-nytimes_2012_2-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-nytimes_2012_2-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation news\">Markoff, John (19 November 2012). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.nytimes.com/2012/11/20/science/for-web-images-creating-new-technology-to-seek-and-find.html\">\"For Web Images, Creating New Technology to Seek and Find\"</a>. <i>The New York Times</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">3 February</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=For+Web+Images%2C+Creating+New+Technology+to+Seek+and+Find&amp;rft.date=2012-11-19&amp;rft.aulast=Markoff&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2012%2F11%2F20%2Fscience%2Ffor-web-images-creating-new-technology-to-seek-and-find.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://image-net.org/about-stats\">\"ImageNet Summary and Statistics\"</a>. ImageNet<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">22 June</span> 2016</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=ImageNet+Summary+and+Statistics&amp;rft.pub=ImageNet&amp;rft_id=http%3A%2F%2Fimage-net.org%2Fabout-stats&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-economist-4\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-economist_4-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-economist_4-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-economist_4-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation news\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.economist.com/news/special-report/21700756-artificial-intelligence-boom-based-old-idea-modern-twist-not\">\"From not working to neural networking\"</a>. <i>The Economist</i>. 25 June 2016<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">3 February</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Economist&amp;rft.atitle=From+not+working+to+neural+networking&amp;rft.date=2016-06-25&amp;rft_id=https%3A%2F%2Fwww.economist.com%2Fnews%2Fspecial-report%2F21700756-artificial-intelligence-boom-based-old-idea-modern-twist-not&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"http://image-net.org/about-overview\">\"ImageNet Overview\"</a>. ImageNet<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">22 June</span> 2016</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=ImageNet+Overview&amp;rft.pub=ImageNet&amp;rft_id=http%3A%2F%2Fimage-net.org%2Fabout-overview&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-ILJVRC-2015-6\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-ILJVRC-2015_6-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-ILJVRC-2015_6-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-ILJVRC-2015_6-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-ILJVRC-2015_6-3\"><sup><i><b>d</b></i></sup></a> <a href=\"#cite_ref-ILJVRC-2015_6-4\"><sup><i><b>e</b></i></sup></a></span> <span class=\"reference-text\">Olga Russakovsky*, Jia Deng*, Hao Su, Jonathan Krause, Sanjeev Satheesh, Sean Ma, Zhiheng Huang, <a href=\"/wiki/Andrej_Karpathy\" title=\"Andrej Karpathy\">Andrej Karpathy</a>, Aditya Khosla, Michael Bernstein, Alexander C. Berg and Li Fei-Fei. (* = equal contribution) ImageNet Large Scale Visual Recognition Challenge. IJCV, 2015.</span>\n</li>\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.ft.com/content/4cc048f6-d5f4-11e7-a303-9060cb1e5f44\">\"Machines 'beat humans' for a growing number of tasks\"</a>. <i>Financial Times</i>. 30 November 2017<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">3 February</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Financial+Times&amp;rft.atitle=Machines+%27beat+humans%27+for+a+growing+number+of+tasks&amp;rft.date=2017-11-30&amp;rft_id=https%3A%2F%2Fwww.ft.com%2Fcontent%2F4cc048f6-d5f4-11e7-a303-9060cb1e5f44&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Gershgorn, Dave (2017-07-26). <a rel=\"nofollow\" class=\"external text\" href=\"https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/\">\"The data that transformed AI research\u2014and possibly the world\"</a>. <i>Quartz</i>. Atlantic Media Co<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2017-07-26</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Quartz&amp;rft.atitle=The+data+that+transformed+AI+research%E2%80%94and+possibly+the+world&amp;rft.date=2017-07-26&amp;rft.aulast=Gershgorn&amp;rft.aufirst=Dave&amp;rft_id=https%3A%2F%2Fqz.com%2F1034972%2Fthe-data-that-changed-the-direction-of-ai-research-and-possibly-the-world%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite id=\"CITEREFDengDongSocherLi2009\" class=\"citation\">Deng, Jia; Dong, Wei; Socher, Richard; Li, Li-Jia; Li, Kai; Fei-Fei, Li (2009), <a rel=\"nofollow\" class=\"external text\" href=\"http://www.image-net.org/papers/imagenet_cvpr09.pdf\">\"ImageNet: A Large-Scale Hierarchical Image Database\"</a> <span style=\"font-size:85%;\">(PDF)</span>, <i>2009 conference on Computer Vision and Pattern Recognition</i></cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=ImageNet%3A+A+Large-Scale+Hierarchical+Image+Database&amp;rft.btitle=2009+conference+on+Computer+Vision+and+Pattern+Recognition&amp;rft.date=2009&amp;rft.aulast=Deng&amp;rft.aufirst=Jia&amp;rft.au=Dong%2C+Wei&amp;rft.au=Socher%2C+Richard&amp;rft.au=Li%2C+Li-Jia&amp;rft.au=Li%2C+Kai&amp;rft.au=Fei-Fei%2C+Li&amp;rft_id=http%3A%2F%2Fwww.image-net.org%2Fpapers%2Fimagenet_cvpr09.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\">Robbins, Martin (6 May 2016). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.theguardian.com/science/2016/may/06/does-an-ai-need-to-make-love-to-rembrandts-girlfriend-to-make-art\">\"Does an AI need to make love to Rembrandt's girlfriend to make art?\"</a>. <i>The Guardian</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">22 June</span> 2016</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Guardian&amp;rft.atitle=Does+an+AI+need+to+make+love+to+Rembrandt%27s+girlfriend+to+make+art%3F&amp;rft.date=2016-05-06&amp;rft.aulast=Robbins&amp;rft.aufirst=Martin&amp;rft_id=https%3A%2F%2Fwww.theguardian.com%2Fscience%2F2016%2Fmay%2F06%2Fdoes-an-ai-need-to-make-love-to-rembrandts-girlfriend-to-make-art&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\">Markoff, John (10 December 2015). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.nytimes.com/2015/12/11/science/an-advance-in-artificial-intelligence-rivals-human-vision-abilities.html\">\"A Learning Advance in Artificial Intelligence Rivals Human Abilities\"</a>. <i>The New York Times</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">22 June</span> 2016</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=A+Learning+Advance+in+Artificial+Intelligence+Rivals+Human+Abilities&amp;rft.date=2015-12-10&amp;rft.aulast=Markoff&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2015%2F12%2F11%2Fscience%2Fan-advance-in-artificial-intelligence-rivals-human-vision-abilities.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\">Aron, Jacob (21 September 2015). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.newscientist.com/article/dn28206-forget-the-turing-test-there-are-better-ways-of-judging-ai/\">\"Forget the Turing test \u2013 there are better ways of judging AI\"</a>. <i>New Scientist</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">22 June</span> 2016</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+Scientist&amp;rft.atitle=Forget+the+Turing+test+%E2%80%93+there+are+better+ways+of+judging+AI&amp;rft.date=2015-09-21&amp;rft.aulast=Aron&amp;rft.aufirst=Jacob&amp;rft_id=https%3A%2F%2Fwww.newscientist.com%2Farticle%2Fdn28206-forget-the-turing-test-there-are-better-ways-of-judging-ai%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-13\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\">Markoff, John (3 June 2015). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.nytimes.com/2015/06/04/technology/computer-scientists-are-astir-after-baidu-team-is-barred-from-ai-competition.html\">\"Computer Scientists Are Astir After Baidu Team Is Barred From A.I. Competition\"</a>. <i>The New York Times</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">22 June</span> 2016</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+New+York+Times&amp;rft.atitle=Computer+Scientists+Are+Astir+After+Baidu+Team+Is+Barred+From+A.I.+Competition&amp;rft.date=2015-06-03&amp;rft.aulast=Markoff&amp;rft.aufirst=John&amp;rft_id=https%3A%2F%2Fwww.nytimes.com%2F2015%2F06%2F04%2Ftechnology%2Fcomputer-scientists-are-astir-after-baidu-team-is-barred-from-ai-competition.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.bbc.com/news/technology-33005728\">\"Chinese search giant Baidu disqualified from AI test\"</a>. <i>BBC News</i>. 14 June 2015<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">22 June</span> 2016</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=BBC+News&amp;rft.atitle=Chinese+search+giant+Baidu+disqualified+from+AI+test&amp;rft.date=2015-06-14&amp;rft_id=https%3A%2F%2Fwww.bbc.com%2Fnews%2Ftechnology-33005728&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\"><a rel=\"nofollow\" class=\"external text\" href=\"http://www.pcworld.com/article/2935232/baidu-fires-researcher-involved-in-ai-contest-flap.html\">\"Baidu fires researcher involved in AI contest flap\"</a>. <i>PCWorld</i>. 11 June 2015<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">22 June</span> 2016</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=PCWorld&amp;rft.atitle=Baidu+fires+researcher+involved+in+AI+contest+flap&amp;rft.date=2015-06-11&amp;rft_id=http%3A%2F%2Fwww.pcworld.com%2Farticle%2F2935232%2Fbaidu-fires-researcher-involved-in-ai-contest-flap.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-16\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-16\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\">Gershgorn, Dave (10 September 2017). <a rel=\"nofollow\" class=\"external text\" href=\"https://qz.com/1046350/the-quartz-guide-to-artificial-intelligence-what-is-it-why-is-it-important-and-should-we-be-afraid/\">\"The Quartz guide to artificial intelligence: What is it, why is it important, and should we be afraid?\"</a>. <i>Quartz</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">3 February</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Quartz&amp;rft.atitle=The+Quartz+guide+to+artificial+intelligence%3A+What+is+it%2C+why+is+it+important%2C+and+should+we+be+afraid%3F&amp;rft.date=2017-09-10&amp;rft.aulast=Gershgorn&amp;rft.aufirst=Dave&amp;rft_id=https%3A%2F%2Fqz.com%2F1046350%2Fthe-quartz-guide-to-artificial-intelligence-what-is-it-why-is-it-important-and-should-we-be-afraid%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.newscientist.com/article/2127131-new-computer-vision-challenge-wants-to-teach-robots-to-see-in-3d/\">\"New computer vision challenge wants to teach robots to see in 3D\"</a>. <i>New Scientist</i>. 7 April 2017<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">3 February</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=New+Scientist&amp;rft.atitle=New+computer+vision+challenge+wants+to+teach+robots+to+see+in+3D&amp;rft.date=2017-04-07&amp;rft_id=https%3A%2F%2Fwww.newscientist.com%2Farticle%2F2127131-new-computer-vision-challenge-wants-to-teach-robots-to-see-in-3d%2F&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation news\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.independent.co.uk/life-style/gadgets-and-tech/news/google-child-ai-bot-nasnet-automl-machine-learning-artificial-intelligence-a8093201.html\">\"Google AI creates its own 'child' bot\"</a>. <i>The Independent</i>. 5 December 2017<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">5 February</span> 2018</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Independent&amp;rft.atitle=Google+AI+creates+its+own+%E2%80%98child%E2%80%99+bot&amp;rft.date=2017-12-05&amp;rft_id=https%3A%2F%2Fwww.independent.co.uk%2Flife-style%2Fgadgets-and-tech%2Fnews%2Fgoogle-child-ai-bot-nasnet-automl-machine-learning-artificial-intelligence-a8093201.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AImageNet\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n</ol></div>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=ImageNet&amp;action=edit&amp;section=7\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><span class=\"official-website\"><span class=\"url\"><a rel=\"nofollow\" class=\"external text\" href=\"http://image-net.org\">Official website</a></span></span></li></ul>\n<div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Standard_test_items\" style=\"padding:3px\"><table class=\"nowraplinks collapsible autolistlapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Standard_test_item\" title=\"Template:Standard test item\"><abbr title=\"View this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Standard_test_item\" title=\"Template talk:Standard test item\"><abbr title=\"Discuss this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Template:Standard_test_item&amp;action=edit\"><abbr title=\"Edit this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">e</abbr></a></li></ul></div><div id=\"Standard_test_items\" style=\"font-size:114%;margin:0 4em\">Standard test items</div></th></tr><tr><td class=\"navbox-abovebelow hlist\" colspan=\"2\"><div id=\"*_Pangram&amp;#10;*_Reference_implementation&amp;#10;*_Standard_test_image\">\n<ul><li><a href=\"/wiki/Pangram\" title=\"Pangram\">Pangram</a></li>\n<li><a href=\"/wiki/Reference_implementation\" title=\"Reference implementation\">Reference implementation</a></li>\n<li><a href=\"/wiki/Standard_test_image\" title=\"Standard test image\">Standard test image</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Television\" title=\"Television\">Television</a> (<a href=\"/wiki/Test_card\" title=\"Test card\">test card</a>)</th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/SMPTE_color_bars\" title=\"SMPTE color bars\">SMPTE color bars</a></li>\n<li><a href=\"/wiki/Indian-head_test_pattern\" title=\"Indian-head test pattern\">Indian-head test pattern</a></li>\n<li><a href=\"/wiki/Test_Card_F\" title=\"Test Card F\">Test Card F</a></li>\n<li><a href=\"/wiki/Philips_PM5544\" title=\"Philips PM5544\">Philips PM5544</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Computer_language\" title=\"Computer language\">Computer languages</a></th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/%22Hello,_World!%22_program\" title=\"&quot;Hello, World!&quot; program\">\"Hello, World!\" program</a></li>\n<li><a href=\"/wiki/Quine_(computing)\" title=\"Quine (computing)\">Quine</a></li>\n<li><a href=\"/wiki/Trabb_Pardo%E2%80%93Knuth_algorithm\" title=\"Trabb Pardo\u2013Knuth algorithm\">Trabb Pardo\u2013Knuth algorithm</a></li>\n<li><a href=\"/wiki/Man_or_boy_test\" title=\"Man or boy test\">Man or boy test</a></li>\n<li><a href=\"/wiki/Just_another_Perl_hacker\" title=\"Just another Perl hacker\">Just another Perl hacker</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Data_compression\" title=\"Data compression\">Data compression</a></th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Calgary_corpus\" title=\"Calgary corpus\">Calgary corpus</a></li>\n<li><a href=\"/wiki/Canterbury_corpus\" title=\"Canterbury corpus\">Canterbury corpus</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/3D_computer_graphics\" title=\"3D computer graphics\">3D computer graphics</a></th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Cornell_box\" title=\"Cornell box\">Cornell box</a></li>\n<li><a href=\"/wiki/Stanford_bunny\" title=\"Stanford bunny\">Stanford bunny</a></li>\n<li><a href=\"/wiki/Stanford_dragon\" title=\"Stanford dragon\">Stanford dragon</a></li>\n<li><a href=\"/wiki/Utah_teapot\" title=\"Utah teapot\">Utah teapot</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a></th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a class=\"mw-selflink selflink\">ImageNet</a></li>\n<li><a href=\"/wiki/MNIST_database\" title=\"MNIST database\">MNIST database</a></li>\n<li><a href=\"/wiki/List_of_datasets_for_machine_learning_research\" title=\"List of datasets for machine learning research\">List</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Typography\" title=\"Typography\">Typography</a></th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Hamburgevons\" title=\"Hamburgevons\">Hamburgevons</a></li>\n<li><a href=\"/wiki/Lorem_ipsum\" title=\"Lorem ipsum\">Lorem ipsum</a></li>\n<li><a href=\"/wiki/The_quick_brown_fox_jumps_over_the_lazy_dog\" title=\"The quick brown fox jumps over the lazy dog\">The quick brown fox jumps over the lazy dog</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">Other</th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/EICAR_test_file\" title=\"EICAR test file\">EICAR test file</a></li>\n<li><a href=\"/wiki/GTUBE\" title=\"GTUBE\">GTUBE</a></li>\n<li><a href=\"/wiki/Harvard_sentences\" title=\"Harvard sentences\">Harvard sentences</a></li>\n<li><a href=\"/wiki/Lenna\" title=\"Lenna\">Lenna</a></li>\n<li><a href=\"/wiki/Tom%27s_Diner#The_&quot;Mother_of_the_MP3&quot;\" title=\"Tom&#39;s Diner\">\"Tom's Diner\"</a></li>\n<li><a href=\"/wiki/Film_leader\" title=\"Film leader\">SMPTE universal leader</a></li>\n<li><a href=\"/wiki/EURion_constellation\" title=\"EURion constellation\">EURion constellation</a></li></ul>\n</div></td></tr></tbody></table></div>\n\n<!-- \nNewPP limit report\nParsed by mw2237\nCached time: 20180913073917\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.256 seconds\nReal time usage: 0.304 seconds\nPreprocessor visited node count: 877/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 41007/2097152 bytes\nTemplate argument size: 185/2097152 bytes\nHighest expansion depth: 7/40\nExpensive parser function count: 0/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 20821/5000000 bytes\nNumber of Wikibase entities loaded: 1/400\nLua time usage: 0.125/10.000 seconds\nLua memory usage: 2.97 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  226.847      1 -total\n 76.39%  173.297      1 Template:Reflist\n 46.40%  105.260     13 Template:Cite_news\n 16.06%   36.438      1 Template:Official_website\n  7.44%   16.877      1 Template:Standard_test_item\n  6.17%   13.990      1 Template:Navbox\n  5.56%   12.612      1 Template:Citation\n  5.50%   12.475      3 Template:Cite_web\n  1.77%    4.021      1 Template:Column-width\n  1.65%    3.736      2 Template:Main_other\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:50896194-0!canonical and timestamp 20180913073917 and revision id 854252429\n -->\n</div>"},"langlinks":[{"lang":"de","url":"https://de.wikipedia.org/wiki/ImageNet","langname":"German","autonym":"Deutsch","*":"ImageNet"},{"lang":"fr","url":"https://fr.wikipedia.org/wiki/ImageNet","langname":"French","autonym":"fran\u00e7ais","*":"ImageNet"},{"lang":"ru","url":"https://ru.wikipedia.org/wiki/ImageNet","langname":"Russian","autonym":"\u0440\u0443\u0441\u0441\u043a\u0438\u0439","*":"ImageNet"}],"categories":[{"sortkey":"","hidden":"","*":"Official_website_different_in_Wikidata_and_Wikipedia"},{"sortkey":"","*":"Computer_science_competitions"},{"sortkey":"","*":"2009_in_computer_science"},{"sortkey":"","*":"Object_recognition_and_categorization"},{"sortkey":"","*":"Databases"},{"sortkey":"","*":"Datasets_in_computer_vision"}],"links":[{"ns":10,"exists":"","*":"Template:Standard test item"},{"ns":0,"exists":"","*":"\"Hello, World!\" program"},{"ns":0,"exists":"","*":"3D computer graphics"},{"ns":0,"exists":"","*":"Amazon Mechanical Turk"},{"ns":0,"exists":"","*":"Andrej Karpathy"},{"ns":0,"exists":"","*":"Augmented reality"},{"ns":0,"exists":"","*":"Baidu"},{"ns":0,"exists":"","*":"Calgary corpus"},{"ns":0,"exists":"","*":"Canterbury corpus"},{"ns":0,"exists":"","*":"Computer language"},{"ns":0,"exists":"","*":"Computer vision"},{"ns":0,"exists":"","*":"Conference on Computer Vision and Pattern Recognition"},{"ns":0,"exists":"","*":"Cornell box"},{"ns":0,"exists":"","*":"Crowdsources"},{"ns":0,"exists":"","*":"Data compression"},{"ns":0,"exists":"","*":"Database"},{"ns":0,"exists":"","*":"Deep learning"},{"ns":0,"exists":"","*":"Diplodocus"},{"ns":0,"exists":"","*":"Dog breeds"},{"ns":0,"exists":"","*":"EICAR test file"},{"ns":0,"exists":"","*":"EURion constellation"},{"ns":0,"exists":"","*":"Film leader"},{"ns":0,"exists":"","*":"GTUBE"},{"ns":0,"exists":"","*":"Hamburgevons"},{"ns":0,"exists":"","*":"Harvard sentences"},{"ns":0,"exists":"","*":"Indian-head test pattern"},{"ns":0,"exists":"","*":"Just another Perl hacker"},{"ns":0,"exists":"","*":"Lenna"},{"ns":0,"exists":"","*":"List of datasets for machine learning research"},{"ns":0,"exists":"","*":"Lorem ipsum"},{"ns":0,"exists":"","*":"MNIST database"},{"ns":0,"exists":"","*":"Machine learning"},{"ns":0,"exists":"","*":"Man or boy test"},{"ns":0,"exists":"","*":"Outline of object recognition"},{"ns":0,"exists":"","*":"Pangram"},{"ns":0,"exists":"","*":"Philips PM5544"},{"ns":0,"exists":"","*":"Princeton University"},{"ns":0,"exists":"","*":"Quine (computing)"},{"ns":0,"exists":"","*":"Reference implementation"},{"ns":0,"exists":"","*":"SMPTE color bars"},{"ns":0,"exists":"","*":"Standard test image"},{"ns":0,"exists":"","*":"Stanford bunny"},{"ns":0,"exists":"","*":"Stanford dragon"},{"ns":0,"exists":"","*":"Television"},{"ns":0,"exists":"","*":"Test Card F"},{"ns":0,"exists":"","*":"Test card"},{"ns":0,"exists":"","*":"The quick brown fox jumps over the lazy dog"},{"ns":0,"exists":"","*":"Tom's Diner"},{"ns":0,"exists":"","*":"Trabb Pardo\u2013Knuth algorithm"},{"ns":0,"exists":"","*":"Typography"},{"ns":0,"exists":"","*":"Utah teapot"},{"ns":0,"exists":"","*":"WordNet"},{"ns":11,"exists":"","*":"Template talk:Standard test item"}],"templates":[{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Column-width"},{"ns":10,"exists":"","*":"Template:Cite news"},{"ns":10,"exists":"","*":"Template:Cite web"},{"ns":10,"exists":"","*":"Template:Citation"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":10,"exists":"","*":"Template:Official website"},{"ns":10,"exists":"","*":"Template:Standard test item"},{"ns":10,"exists":"","*":"Template:Navbox"},{"ns":828,"exists":"","*":"Module:Citation/CS1"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Configuration"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Whitelist"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Utilities"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Date validation"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Identifiers"},{"ns":828,"exists":"","*":"Module:Citation/CS1/COinS"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"},{"ns":828,"exists":"","*":"Module:Official website"},{"ns":828,"exists":"","*":"Module:URL"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Navbox"},{"ns":828,"exists":"","*":"Module:Navbar"}],"images":["ImageNet_error_rate_history_(just_systems).svg"],"externallinks":["https://www.newscientist.com/article/2127131-new-computer-vision-challenge-wants-to-teach-robots-to-see-in-3d/","https://www.nytimes.com/2012/11/20/science/for-web-images-creating-new-technology-to-seek-and-find.html","http://image-net.org/about-stats","https://www.economist.com/news/special-report/21700756-artificial-intelligence-boom-based-old-idea-modern-twist-not","http://image-net.org/about-overview","https://www.ft.com/content/4cc048f6-d5f4-11e7-a303-9060cb1e5f44","https://qz.com/1034972/the-data-that-changed-the-direction-of-ai-research-and-possibly-the-world/","http://www.image-net.org/papers/imagenet_cvpr09.pdf","https://www.theguardian.com/science/2016/may/06/does-an-ai-need-to-make-love-to-rembrandts-girlfriend-to-make-art","https://www.nytimes.com/2015/12/11/science/an-advance-in-artificial-intelligence-rivals-human-vision-abilities.html","https://www.newscientist.com/article/dn28206-forget-the-turing-test-there-are-better-ways-of-judging-ai/","https://www.nytimes.com/2015/06/04/technology/computer-scientists-are-astir-after-baidu-team-is-barred-from-ai-competition.html","https://www.bbc.com/news/technology-33005728","http://www.pcworld.com/article/2935232/baidu-fires-researcher-involved-in-ai-contest-flap.html","https://qz.com/1046350/the-quartz-guide-to-artificial-intelligence-what-is-it-why-is-it-important-and-should-we-be-afraid/","https://www.independent.co.uk/life-style/gadgets-and-tech/news/google-child-ai-bot-nasnet-automl-machine-learning-artificial-intelligence-a8093201.html","http://image-net.org"],"sections":[{"toclevel":1,"level":"2","line":"History","number":"1","index":"1","fromtitle":"ImageNet","byteoffset":2660,"anchor":"History"},{"toclevel":1,"level":"2","line":"Dataset","number":"2","index":"2","fromtitle":"ImageNet","byteoffset":3752,"anchor":"Dataset"},{"toclevel":1,"level":"2","line":"ImageNet Challenge","number":"3","index":"3","fromtitle":"ImageNet","byteoffset":4944,"anchor":"ImageNet_Challenge"},{"toclevel":2,"level":"3","line":"Non-competition results","number":"3.1","index":"4","fromtitle":"ImageNet","byteoffset":9526,"anchor":"Non-competition_results"},{"toclevel":1,"level":"2","line":"See also","number":"4","index":"5","fromtitle":"ImageNet","byteoffset":10090,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"References","number":"5","index":"6","fromtitle":"ImageNet","byteoffset":10181,"anchor":"References"},{"toclevel":1,"level":"2","line":"External links","number":"6","index":"7","fromtitle":"ImageNet","byteoffset":10216,"anchor":"External_links"}],"parsewarnings":[],"displaytitle":"ImageNet","iwlinks":[],"properties":[{"name":"wikibase_item","*":"Q24901201"}]}}