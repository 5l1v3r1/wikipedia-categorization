{"parse":{"title":"Speech perception","pageid":5366050,"revid":859692764,"text":{"*":"<div class=\"mw-parser-output\"><p><b>Speech perception</b> is the process by which the sounds of <a href=\"/wiki/Language\" title=\"Language\">language</a> are heard, interpreted and understood. The study of <a href=\"/wiki/Speech\" title=\"Speech\">speech</a> perception is closely linked to the fields of <a href=\"/wiki/Phonology\" title=\"Phonology\">phonology</a> and <a href=\"/wiki/Phonetics\" title=\"Phonetics\">phonetics</a> in <a href=\"/wiki/Linguistics\" title=\"Linguistics\">linguistics</a> and <a href=\"/wiki/Cognitive_psychology\" title=\"Cognitive psychology\">cognitive psychology</a> and <a href=\"/wiki/Perception\" title=\"Perception\">perception</a> in <a href=\"/wiki/Psychology\" title=\"Psychology\">psychology</a>. Research in speech perception seeks to understand how human listeners recognize speech sounds and use this information to understand spoken language. Speech perception research has applications in building <a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">computer systems that can recognize speech</a>, in improving speech recognition for hearing- and language-impaired listeners, and in foreign-language teaching.\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Basics\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Basics</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-2\"><a href=\"#Acoustic_cues\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\">Acoustic cues</span></a></li>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"#Linearity_and_the_segmentation_problem\"><span class=\"tocnumber\">1.2</span> <span class=\"toctext\">Linearity and the segmentation problem</span></a></li>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Lack_of_invariance\"><span class=\"tocnumber\">1.3</span> <span class=\"toctext\">Lack of invariance</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-5\"><a href=\"#Context-induced_variation\"><span class=\"tocnumber\">1.3.1</span> <span class=\"toctext\">Context-induced variation</span></a></li>\n<li class=\"toclevel-3 tocsection-6\"><a href=\"#Variation_due_to_differing_speech_conditions\"><span class=\"tocnumber\">1.3.2</span> <span class=\"toctext\">Variation due to differing speech conditions</span></a></li>\n<li class=\"toclevel-3 tocsection-7\"><a href=\"#Variation_due_to_different_speaker_identity\"><span class=\"tocnumber\">1.3.3</span> <span class=\"toctext\">Variation due to different speaker identity</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Perceptual_constancy_and_normalization\"><span class=\"tocnumber\">1.4</span> <span class=\"toctext\">Perceptual constancy and normalization</span></a></li>\n<li class=\"toclevel-2 tocsection-9\"><a href=\"#Categorical_perception\"><span class=\"tocnumber\">1.5</span> <span class=\"toctext\">Categorical perception</span></a></li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"#Top-down_influences\"><span class=\"tocnumber\">1.6</span> <span class=\"toctext\">Top-down influences</span></a></li>\n<li class=\"toclevel-2 tocsection-11\"><a href=\"#Acquired_brain_disabilities\"><span class=\"tocnumber\">1.7</span> <span class=\"toctext\">Acquired brain disabilities</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-12\"><a href=\"#Aphasia\"><span class=\"tocnumber\">1.7.1</span> <span class=\"toctext\">Aphasia</span></a></li>\n<li class=\"toclevel-3 tocsection-13\"><a href=\"#Parkinson&#39;s_disease\"><span class=\"tocnumber\">1.7.2</span> <span class=\"toctext\">Parkinson's disease</span></a></li>\n<li class=\"toclevel-3 tocsection-14\"><a href=\"#Agnosia\"><span class=\"tocnumber\">1.7.3</span> <span class=\"toctext\">Agnosia</span></a>\n<ul>\n<li class=\"toclevel-4 tocsection-15\"><a href=\"#Speech_agnosia\"><span class=\"tocnumber\">1.7.3.1</span> <span class=\"toctext\">Speech agnosia</span></a></li>\n<li class=\"toclevel-4 tocsection-16\"><a href=\"#Phonagnosia\"><span class=\"tocnumber\">1.7.3.2</span> <span class=\"toctext\">Phonagnosia</span></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li class=\"toclevel-2 tocsection-17\"><a href=\"#Treatments\"><span class=\"tocnumber\">1.8</span> <span class=\"toctext\">Treatments</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-18\"><a href=\"#Aphasia_2\"><span class=\"tocnumber\">1.8.1</span> <span class=\"toctext\">Aphasia</span></a></li>\n<li class=\"toclevel-3 tocsection-19\"><a href=\"#Parkinson&#39;s_disease_2\"><span class=\"tocnumber\">1.8.2</span> <span class=\"toctext\">Parkinson's disease</span></a></li>\n<li class=\"toclevel-3 tocsection-20\"><a href=\"#Speech_agnosia_2\"><span class=\"tocnumber\">1.8.3</span> <span class=\"toctext\">Speech agnosia</span></a></li>\n<li class=\"toclevel-3 tocsection-21\"><a href=\"#Phonagnosia_2\"><span class=\"tocnumber\">1.8.4</span> <span class=\"toctext\">Phonagnosia</span></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-22\"><a href=\"#Research_topics\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Research topics</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-23\"><a href=\"#Infant_speech_perception\"><span class=\"tocnumber\">2.1</span> <span class=\"toctext\">Infant speech perception</span></a></li>\n<li class=\"toclevel-2 tocsection-24\"><a href=\"#Cross-language_and_second-language\"><span class=\"tocnumber\">2.2</span> <span class=\"toctext\">Cross-language and second-language</span></a></li>\n<li class=\"toclevel-2 tocsection-25\"><a href=\"#In_language_or_hearing_impairment\"><span class=\"tocnumber\">2.3</span> <span class=\"toctext\">In language or hearing impairment</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-26\"><a href=\"#Listeners_with_aphasia\"><span class=\"tocnumber\">2.3.1</span> <span class=\"toctext\">Listeners with aphasia</span></a></li>\n<li class=\"toclevel-3 tocsection-27\"><a href=\"#Listeners_with_cochlear_implants\"><span class=\"tocnumber\">2.3.2</span> <span class=\"toctext\">Listeners with cochlear implants</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-2 tocsection-28\"><a href=\"#Noise\"><span class=\"tocnumber\">2.4</span> <span class=\"toctext\">Noise</span></a></li>\n<li class=\"toclevel-2 tocsection-29\"><a href=\"#Music-language_connection\"><span class=\"tocnumber\">2.5</span> <span class=\"toctext\">Music-language connection</span></a></li>\n<li class=\"toclevel-2 tocsection-30\"><a href=\"#Speech_phenomenology\"><span class=\"tocnumber\">2.6</span> <span class=\"toctext\">Speech phenomenology</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-31\"><a href=\"#The_experience_of_speech\"><span class=\"tocnumber\">2.6.1</span> <span class=\"toctext\">The experience of speech</span></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-32\"><a href=\"#Research_methods\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Research methods</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-33\"><a href=\"#Behavioral_methods\"><span class=\"tocnumber\">3.1</span> <span class=\"toctext\">Behavioral methods</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-34\"><a href=\"#Sinewave_Speech\"><span class=\"tocnumber\">3.1.1</span> <span class=\"toctext\">Sinewave Speech</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-2 tocsection-35\"><a href=\"#Computational_methods\"><span class=\"tocnumber\">3.2</span> <span class=\"toctext\">Computational methods</span></a></li>\n<li class=\"toclevel-2 tocsection-36\"><a href=\"#Neurophysiological_methods\"><span class=\"tocnumber\">3.3</span> <span class=\"toctext\">Neurophysiological methods</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-37\"><a href=\"#Theories\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Theories</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-38\"><a href=\"#Speech_mode_hypothesis\"><span class=\"tocnumber\">4.1</span> <span class=\"toctext\">Speech mode hypothesis</span></a></li>\n<li class=\"toclevel-2 tocsection-39\"><a href=\"#Motor_theory\"><span class=\"tocnumber\">4.2</span> <span class=\"toctext\">Motor theory</span></a></li>\n<li class=\"toclevel-2 tocsection-40\"><a href=\"#Direct_realist_theory\"><span class=\"tocnumber\">4.3</span> <span class=\"toctext\">Direct realist theory</span></a></li>\n<li class=\"toclevel-2 tocsection-41\"><a href=\"#Fuzzy-logical_model\"><span class=\"tocnumber\">4.4</span> <span class=\"toctext\">Fuzzy-logical model</span></a></li>\n<li class=\"toclevel-2 tocsection-42\"><a href=\"#Acoustic_landmarks_and_distinctive_features\"><span class=\"tocnumber\">4.5</span> <span class=\"toctext\">Acoustic landmarks and distinctive features</span></a></li>\n<li class=\"toclevel-2 tocsection-43\"><a href=\"#Exemplar_theory\"><span class=\"tocnumber\">4.6</span> <span class=\"toctext\">Exemplar theory</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-44\"><a href=\"#See_also\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-45\"><a href=\"#References\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-46\"><a href=\"#External_links\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Basics\">Basics</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=1\" title=\"Edit section: Basics\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The process of perceiving speech begins at the level of the sound signal and the process of audition. (For a complete description of the process of audition see <a href=\"/wiki/Hearing_(sense)\" class=\"mw-redirect\" title=\"Hearing (sense)\">Hearing</a>.) After processing the initial auditory signal, speech sounds are further processed to extract acoustic cues and phonetic information. This speech information can then be used for higher-level language processes, such as word recognition.\n</p>\n<h3><span class=\"mw-headline\" id=\"Acoustic_cues\">Acoustic cues</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=2\" title=\"Edit section: Acoustic cues\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:252px;\"><a href=\"/wiki/File:Spectrograms_of_syllables_dee_dah_doo.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/75/Spectrograms_of_syllables_dee_dah_doo.png/250px-Spectrograms_of_syllables_dee_dah_doo.png\" width=\"250\" height=\"293\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/75/Spectrograms_of_syllables_dee_dah_doo.png/375px-Spectrograms_of_syllables_dee_dah_doo.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/75/Spectrograms_of_syllables_dee_dah_doo.png/500px-Spectrograms_of_syllables_dee_dah_doo.png 2x\" data-file-width=\"790\" data-file-height=\"926\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Spectrograms_of_syllables_dee_dah_doo.png\" class=\"internal\" title=\"Enlarge\"></a></div>Figure 1: Spectrograms of syllables \"dee\" (top), \"dah\" (middle), and \"doo\" (bottom) showing how the onset <a href=\"/w/index.php?title=Formant_transition&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Formant transition (page does not exist)\">formant transitions</a> that define perceptually the consonant <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">[d]</span> differ depending on the identity of the following vowel. (<a href=\"/wiki/Formant\" title=\"Formant\">Formants</a> are highlighted by red dotted lines; transitions are the bending beginnings of the formant trajectories.)</div></div></div>\n<p>The speech sound signal contains a number of <a href=\"/w/index.php?title=Acoustic_cues&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Acoustic cues (page does not exist)\">acoustic cues</a> that are used in speech perception. The cues differentiate speech sounds belonging to different <a href=\"/wiki/Phonetic\" class=\"mw-redirect\" title=\"Phonetic\">phonetic</a> categories. For example, one of the most studied cues in speech is <a href=\"/wiki/Voice_onset_time\" title=\"Voice onset time\">voice onset time</a> or VOT. VOT is a primary cue signaling the difference between voiced and voiceless plosives, such as \"b\" and \"p\". Other cues differentiate sounds that are produced at different <a href=\"/wiki/Place_of_articulation\" title=\"Place of articulation\">places of articulation</a> or <a href=\"/wiki/Manner_of_articulation\" title=\"Manner of articulation\">manners of articulation</a>. The speech system must also combine these cues to determine the category of a specific speech sound. This is often thought of in terms of abstract representations of <a href=\"/wiki/Phonemes\" class=\"mw-redirect\" title=\"Phonemes\">phonemes</a>. These representations can then be combined for use in word recognition and other language processes.\n</p><p>It is not easy to identify what acoustic cues listeners are sensitive to when perceiving a particular speech sound:\n</p>\n<blockquote><p><i>At first glance, the solution to the problem of how we perceive speech seems deceptively simple. If one could identify stretches of the acoustic waveform that correspond to units of perception, then the path from sound to meaning would be clear. However, this correspondence or mapping has proven extremely difficult to find, even after some forty-five years of research on the problem.</i><sup id=\"cite_ref-np_1-0\" class=\"reference\"><a href=\"#cite_note-np-1\">&#91;1&#93;</a></sup></p></blockquote>\n<p>If a specific aspect of the acoustic waveform indicated one linguistic unit, a series of tests using speech synthesizers would be sufficient to determine such a cue or cues. However, there are two significant obstacles:\n</p>\n<ol><li>One acoustic aspect of the speech signal may cue different linguistically relevant dimensions. For example, the duration of a vowel in English can indicate whether or not the vowel is stressed, or whether it is in a syllable closed by a voiced or a voiceless consonant, and in some cases (like American English <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/\u025b/</span> and <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/\u00e6/</span>) it can distinguish the identity of vowels.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup> Some experts even argue that duration can help in distinguishing of what is traditionally called short and long vowels in English.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup></li>\n<li>One linguistic unit can be cued by several acoustic properties. For example, in a classic experiment, <a href=\"/wiki/Alvin_Liberman\" title=\"Alvin Liberman\">Alvin Liberman</a> (1957) showed that the onset <a href=\"/w/index.php?title=Formant_transitions&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Formant transitions (page does not exist)\">formant transitions</a> of <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/d/</span> differ depending on the following vowel (see Figure 1) but they are all interpreted as the phoneme <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/d/</span> by listeners.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\">&#91;4&#93;</a></sup></li></ol>\n<h3><span class=\"mw-headline\" id=\"Linearity_and_the_segmentation_problem\">Linearity and the segmentation problem <span id=\"segmentation\"></span></span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=3\" title=\"Edit section: Linearity and the segmentation problem\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Speech_segmentation\" title=\"Speech segmentation\">Speech segmentation</a></div>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:302px;\"><a href=\"/wiki/File:Spectrogram_of_I_owe_you.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/5/5c/Spectrogram_of_I_owe_you.png/300px-Spectrogram_of_I_owe_you.png\" width=\"300\" height=\"156\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/5/5c/Spectrogram_of_I_owe_you.png 1.5x\" data-file-width=\"436\" data-file-height=\"227\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Spectrogram_of_I_owe_you.png\" class=\"internal\" title=\"Enlarge\"></a></div>Figure 2: A spectrogram of the phrase \"I owe you\". There are no clearly distinguishable boundaries between speech sounds.</div></div></div>\n<p>Although listeners perceive speech as a stream of discrete units<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (July 2010)\">citation needed</span></a></i>&#93;</sup> (<a href=\"/wiki/Phonemes\" class=\"mw-redirect\" title=\"Phonemes\">phonemes</a>, <a href=\"/wiki/Syllables\" class=\"mw-redirect\" title=\"Syllables\">syllables</a>, and <a href=\"/wiki/Words\" class=\"mw-redirect\" title=\"Words\">words</a>), this linearity is difficult to see in the physical speech signal (see Figure 2 for an example). Speech sounds do not strictly follow one another, rather, they overlap.<sup id=\"cite_ref-fow_5-0\" class=\"reference\"><a href=\"#cite_note-fow-5\">&#91;5&#93;</a></sup> A speech sound is influenced by the ones that precede and the ones that follow. This influence can even be exerted at a distance of two or more segments (and across syllable- and word-boundaries).<sup id=\"cite_ref-fow_5-1\" class=\"reference\"><a href=\"#cite_note-fow-5\">&#91;5&#93;</a></sup>\n</p><p>Because the speech signal is not linear, there is a problem of segmentation. It is difficult to delimit a stretch of speech signal as belonging to a single perceptual unit. As an example, the acoustic properties of the phoneme <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/d/</span> will depend on the production of the following vowel (because of <a href=\"/wiki/Coarticulation\" title=\"Coarticulation\">coarticulation</a>).\n</p>\n<h3><span class=\"mw-headline\" id=\"Lack_of_invariance\">Lack of invariance</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=4\" title=\"Edit section: Lack of invariance\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The research and application of speech perception must deal with several problems which result from what has been termed the lack of invariance. Reliable constant relations between a phoneme of a language and its acoustic manifestation in speech are difficult to find. There are several reasons for this:\n</p>\n<h4><span class=\"mw-headline\" id=\"Context-induced_variation\">Context-induced variation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=5\" title=\"Edit section: Context-induced variation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Phonetic environment affects the acoustic properties of speech sounds. For example, <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/u/</span> in English is fronted when surrounded by <a href=\"/wiki/Coronal_consonant\" title=\"Coronal consonant\">coronal consonants</a>.<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup> Or, the <a href=\"/wiki/Voice_onset_time\" title=\"Voice onset time\">voice onset time</a> marking the boundary between voiced and voiceless plosives are different for labial, alveolar and velar plosives and they shift under stress or depending on the position within a syllable.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;7&#93;</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"Variation_due_to_differing_speech_conditions\">Variation due to differing speech conditions</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=6\" title=\"Edit section: Variation due to differing speech conditions\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>One important factor that causes variation is differing speech rate. Many phonemic contrasts are constituted by temporal characteristics (short vs. long vowels or consonants, affricates vs. fricatives, plosives vs. glides, voiced vs. voiceless plosives, etc.) and they are certainly affected by changes in speaking tempo.<sup id=\"cite_ref-np_1-1\" class=\"reference\"><a href=\"#cite_note-np-1\">&#91;1&#93;</a></sup> Another major source of variation is articulatory carefulness vs. sloppiness which is typical for connected speech (articulatory \"undershoot\" is obviously reflected in the acoustic properties of the sounds produced).\n</p>\n<h4><span class=\"mw-headline\" id=\"Variation_due_to_different_speaker_identity\">Variation due to different speaker identity</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=7\" title=\"Edit section: Variation due to different speaker identity\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>The resulting acoustic structure of concrete speech productions depends on the physical and psychological properties of individual speakers. Men, women, and children generally produce voices having different pitch. Because speakers have vocal tracts of different sizes (due to sex and age especially) the resonant frequencies (<a href=\"/wiki/Formants\" class=\"mw-redirect\" title=\"Formants\">formants</a>), which are important for recognition of speech sounds, will vary in their absolute values across individuals<sup id=\"cite_ref-hill_8-0\" class=\"reference\"><a href=\"#cite_note-hill-8\">&#91;8&#93;</a></sup> (see Figure 3 for an illustration of this). Research shows that infants at the age of 7.5 months cannot recognize information presented by speakers of different genders; however by the age of 10.5 months, they can detect the similarities.<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup> Dialect and foreign accent can also cause variation, as can the social characteristics of the speaker and listener.<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\">&#91;10&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Perceptual_constancy_and_normalization\">Perceptual constancy and normalization</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=8\" title=\"Edit section: Perceptual constancy and normalization\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:302px;\"><a href=\"/wiki/File:Standard_and_normalized_vowel_space2.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Standard_and_normalized_vowel_space2.png/300px-Standard_and_normalized_vowel_space2.png\" width=\"300\" height=\"122\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Standard_and_normalized_vowel_space2.png/450px-Standard_and_normalized_vowel_space2.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fa/Standard_and_normalized_vowel_space2.png/600px-Standard_and_normalized_vowel_space2.png 2x\" data-file-width=\"845\" data-file-height=\"343\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Standard_and_normalized_vowel_space2.png\" class=\"internal\" title=\"Enlarge\"></a></div>Figure 3: The left panel shows the 3 peripheral American English vowels <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/i/</span>, <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/\u0251/</span>, and <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/u/</span> in a standard F1 by F2 plot (in Hz). The mismatch between male, female, and child values is apparent. In the right panel formant distances (in <a href=\"/wiki/Bark_scale\" title=\"Bark scale\">Bark</a>) rather than absolute values are plotted using the normalization procedure proposed by Syrdal and Gopal in 1986.<sup id=\"cite_ref-sg_11-0\" class=\"reference\"><a href=\"#cite_note-sg-11\">&#91;11&#93;</a></sup> Formant values are taken from Hillenbrand et al. (1995)<sup id=\"cite_ref-hill_8-1\" class=\"reference\"><a href=\"#cite_note-hill-8\">&#91;8&#93;</a></sup></div></div></div>\n<p>Despite the great variety of different speakers and different conditions, listeners perceive vowels and consonants as constant categories. It has been proposed that this is achieved by means of the perceptual normalization process in which listeners filter out the noise (i.e. variation) to arrive at the underlying category. Vocal-tract-size differences result in formant-frequency variation across speakers; therefore a listener has to adjust his/her perceptual system to the acoustic characteristics of a particular speaker. This may be accomplished by considering the ratios of formants rather than their absolute values.<sup id=\"cite_ref-sg_11-1\" class=\"reference\"><a href=\"#cite_note-sg-11\">&#91;11&#93;</a></sup><sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup><sup id=\"cite_ref-john_13-0\" class=\"reference\"><a href=\"#cite_note-john-13\">&#91;13&#93;</a></sup> This process has been called vocal tract normalization (see Figure 3 for an example). Similarly, listeners are believed to adjust the perception of duration to the current tempo of the speech they are listening to \u2013 this has been referred to as speech rate normalization.\n</p><p>Whether or not normalization actually takes place and what is its exact nature is a matter of theoretical controversy (see <a href=\"#Theories\">theories</a> below). <a href=\"/wiki/Perceptual_constancy\" class=\"mw-redirect\" title=\"Perceptual constancy\">Perceptual constancy</a> is a phenomenon not specific to speech perception only; it exists in other types of perception too.\n</p>\n<h3><span class=\"mw-headline\" id=\"Categorical_perception\">Categorical perception</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=9\" title=\"Edit section: Categorical perception\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Categorical_perception\" title=\"Categorical perception\">Categorical perception</a></div>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:302px;\"><a href=\"/wiki/File:Categorization-and-discrimination-curves.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Categorization-and-discrimination-curves.png/300px-Categorization-and-discrimination-curves.png\" width=\"300\" height=\"200\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/1/1c/Categorization-and-discrimination-curves.png 1.5x\" data-file-width=\"436\" data-file-height=\"290\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Categorization-and-discrimination-curves.png\" class=\"internal\" title=\"Enlarge\"></a></div>Figure 4: Example identification (red) and discrimination (blue) functions</div></div></div>\n<p>Categorical perception is involved in processes of perceptual differentiation. People perceive speech sounds categorically, that is to say, they are more likely to notice the differences <i>between</i> categories (phonemes) than <i>within</i> categories. The perceptual space between categories is therefore warped, the centers of categories (or \"prototypes\") working like a sieve<sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\">&#91;14&#93;</a></sup> or like magnets<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\">&#91;15&#93;</a></sup> for incoming speech sounds.\n</p><p>In an artificial continuum between a voiceless and a voiced <a href=\"/wiki/Bilabial_plosive\" class=\"mw-redirect\" title=\"Bilabial plosive\">bilabial plosive</a>, each new step differs from the preceding one in the amount of <a href=\"/wiki/Voice_onset_time\" title=\"Voice onset time\">VOT</a>. The first sound is a <a href=\"/wiki/Pre-voicing_(phonetics)\" class=\"mw-redirect\" title=\"Pre-voicing (phonetics)\">pre-voiced</a> <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">[b]</span>, i.e. it has a negative VOT. Then, increasing the VOT, it reaches zero, i.e. the plosive is a plain <a href=\"/wiki/Aspiration_(phonetics)\" class=\"mw-redirect\" title=\"Aspiration (phonetics)\">unaspirated</a> voiceless <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">[p]</span>. Gradually, adding the same amount of VOT at a time, the plosive is eventually a strongly aspirated voiceless bilabial <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">[p\u02b0]</span>. (Such a continuum was used in an experiment by <a href=\"/wiki/Leigh_Lisker\" title=\"Leigh Lisker\">Lisker</a> and <a href=\"/wiki/Arthur_S._Abramson\" title=\"Arthur S. Abramson\">Abramson</a> in 1970.<sup id=\"cite_ref-la_16-0\" class=\"reference\"><a href=\"#cite_note-la-16\">&#91;16&#93;</a></sup> The sounds they used are <a rel=\"nofollow\" class=\"external text\" href=\"http://www.haskins.yale.edu/featured/demo-liskabram/index.html\">available online</a>.) In this continuum of, for example, seven sounds, native English listeners will identify the first three sounds as <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/b/</span> and the last three sounds as <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/p/</span> with a clear boundary between the two categories.<sup id=\"cite_ref-la_16-1\" class=\"reference\"><a href=\"#cite_note-la-16\">&#91;16&#93;</a></sup> A two-alternative identification (or categorization) test will yield a discontinuous categorization function (see red curve in Figure 4).\n</p><p>In tests of the ability to discriminate between two sounds with varying VOT values but having a constant VOT distance from each other (20 ms for instance), listeners are likely to perform at chance level if both sounds fall within the same category and at nearly 100% level if each sound falls in a different category (see the blue discrimination curve in Figure 4).\n</p><p>The conclusion to make from both the identification and the discrimination test is that listeners will have different sensitivity to the same relative increase in VOT depending on whether or not the boundary between categories was crossed. Similar perceptual adjustment is attested for other acoustic cues as well.\n</p>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">See also: <a href=\"/wiki/Auditory_processing_disorder\" title=\"Auditory processing disorder\">Auditory processing disorder</a></div>\n<h3><span class=\"mw-headline\" id=\"Top-down_influences\">Top-down influences</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=10\" title=\"Edit section: Top-down influences\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The process of speech perception is not necessarily uni-directional. That is, higher-level language processes connected with <a href=\"/wiki/Morphology_(linguistics)\" title=\"Morphology (linguistics)\">morphology</a>, <a href=\"/wiki/Syntax\" title=\"Syntax\">syntax</a>, or <a href=\"/wiki/Semantics\" title=\"Semantics\">semantics</a> may interact with basic speech perception processes to aid in recognition of speech sounds. It may be the case that it is not necessary and maybe even not possible for a listener to recognize phonemes before recognizing higher units, like words for example. After obtaining at least a fundamental piece of information about phonemic structure of the perceived entity from the acoustic signal, listeners can compensate for missing or noise-masked phonemes using their knowledge of the spoken language.\n</p><p>In a classic experiment, Richard M. Warren (1970) replaced one phoneme of a word with a cough-like sound. His subjects restored the missing speech sound perceptually without any difficulty and could not accurately identify which phoneme had been disturbed.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup> This is known as the <a href=\"/wiki/Phonemic_restoration_effect\" title=\"Phonemic restoration effect\">phonemic restoration effect</a>. Another basic experiment compares recognition of naturally spoken words presented in a sentence (or at least a phrase) and the same words presented in isolation. Perception accuracy usually drops in the latter condition. Garnes and Bond (1976) also used carrier sentences when researching the influence of semantic knowledge on perception. They created series of words differing in one phoneme (bay/day/gay, for example). The quality of the first phoneme changed along a continuum. All these stimuli were put into different sentences each of which made sense with one of the words only. Listeners had a tendency to judge the ambiguous words (when the first segment was at the boundary between categories) according to the meaning of the whole sentence.<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup><sup id=\"cite_ref-19\" class=\"reference\"><a href=\"#cite_note-19\">&#91;19&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Acquired_brain_disabilities\">Acquired brain disabilities</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=11\" title=\"Edit section: Acquired brain disabilities\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The first ever hypothesis of speech perception was used with patients who acquired an auditory comprehension deficit, also known as <a href=\"/wiki/Receptive_aphasia\" title=\"Receptive aphasia\">receptive aphasia</a>. Since then there have been many disabilities that have been classified, which resulted in a true definition of \"speech perception\".<sup id=\"cite_ref-Poeppel,_Monahan,_2008_20-0\" class=\"reference\"><a href=\"#cite_note-Poeppel,_Monahan,_2008-20\">&#91;20&#93;</a></sup> The term 'speech perception' describes the process of interest that employs sub lexical contexts to the probe process. It consists of many different language and grammatical functions, such as: features, segments (phonemes), syllabic structure (unit of pronunciation), phonological word forms (how sounds are grouped together), grammatical features, morphemic (prefixes and suffixes), and semantic information (the meaning of the words).\nIn the early years, they were more interested in the acoustics of speech. For instance, they were looking at the differences between /ba/ or /da/, but now research has been directed to the response in the brain from the stimuli. In recent years, there has been a model developed to create a sense of how speech perception works; this model is known as the dual stream model. This model has drastically changed from how psychologists look at perception. The first section of the dual stream model is the ventral pathway. This pathway incorporates middle temporal gyrus, inferior temporal sulcus and perhaps the inferior temporal gyrus. The ventral pathway shows phonological representations to the lexical or conceptual representations, which is the meaning of the words. The second section of the dual stream model is the dorsal pathway. This pathway includes the sylvian parietotemporal, inferior frontal gyrus, anterior insula, and premotor cortex. Its primary function is to take the sensory or phonological stimuli and transfer it into an articulatory-motor representation (formation of speech).<sup id=\"cite_ref-21\" class=\"reference\"><a href=\"#cite_note-21\">&#91;21&#93;</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"Aphasia\">Aphasia</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=12\" title=\"Edit section: Aphasia\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>There are two different kinds of <a href=\"/wiki/Aphasia\" title=\"Aphasia\">aphasic</a> patients: <a href=\"/wiki/Expressive_aphasia\" title=\"Expressive aphasia\">expressive aphasia</a> (also known as Broca's aphasia) and <a href=\"/wiki/Receptive_aphasia\" title=\"Receptive aphasia\">receptive aphasia</a> (also known as Wernicke's aphasia). There are three distinctive dimensions to phonetics: manner of articulation, place of articulation, and voicing.<sup id=\"cite_ref-hd_22-0\" class=\"reference\"><a href=\"#cite_note-hd-22\">&#91;22&#93;</a></sup>\n</p><p><b>Expressive aphasia</b>: Patients who suffer from this condition typically have lesions on their left inferior frontal cortex. These patients are described with having severe syntactical deficits, which means that they have extreme difficulty in forming sentences correctly. Expressive aphasic patients suffer from more regular rule governed principles in forming sentences, which is closely related to Alzheimer patients. For instance instead of saying the red ball bounced, both of these patients would say bounced ball the red. This is just one example of what a person might say; there are of course many possibilities.<sup id=\"cite_ref-hd_22-1\" class=\"reference\"><a href=\"#cite_note-hd-22\">&#91;22&#93;</a></sup>\n</p><p><b>Receptive aphasia</b>: The patients suffer from lesions or damage located in the left temporoparietal lobe. Receptive Aphasic patients mostly suffer from lexical-semantic difficulties, but also have difficulties in comprehension tasks. Though they have difficulty saying things or describing things, these people showed that they could do well in online comprehension tasks. This is closely related to Parkinson's disease because both of the diseases have trouble in distinguishing irregular verbs. For instance, a person suffering from expressive aphasia or Parkinson's disease would say \"the dog goed home\" instead of \"the dog went home\".<sup id=\"cite_ref-hd_22-2\" class=\"reference\"><a href=\"#cite_note-hd-22\">&#91;22&#93;</a></sup>\n</p>\n<h4><span id=\"Parkinson.27s_disease\"></span><span class=\"mw-headline\" id=\"Parkinson's_disease\">Parkinson's disease</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=13\" title=\"Edit section: Parkinson&#039;s disease\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>This disease attacks the brain and makes the patients unable to stop shaking. The effects could be difficulty in walking, communicating, or functioning. Over time the symptoms go from mild to severe, which can cause extreme difficulties in a person's life. Many psychologists relate Parkinson's disease to <a href=\"/wiki/Progressive_nonfluent_aphasia\" title=\"Progressive nonfluent aphasia\">progressive nonfluent aphasia</a>, which would cause a person to have comprehension deficits and being able to recognize irregular verbs. For instance, a person suffering from expressive aphasia or Parkinson's disease would say \"the dog goed home\" instead of \"the dog went home\".<sup id=\"cite_ref-ftp_23-0\" class=\"reference\"><a href=\"#cite_note-ftp-23\">&#91;23&#93;</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"Agnosia\">Agnosia</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=14\" title=\"Edit section: Agnosia\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p><a href=\"/wiki/Agnosia\" title=\"Agnosia\">Agnosia</a> is \"the loss or diminution of the ability to recognize familiar objects or stimuli usually as a result of brain damage\".<sup id=\"cite_ref-24\" class=\"reference\"><a href=\"#cite_note-24\">&#91;24&#93;</a></sup> There are several different kinds of agnosia that affect every one of our senses, but the two most common related to speech are <a href=\"/w/index.php?title=Speech_agnosia&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Speech agnosia (page does not exist)\">speech agnosia</a> and <a href=\"/wiki/Phonagnosia\" title=\"Phonagnosia\">phonagnosia</a>.\n</p>\n<h5><span class=\"mw-headline\" id=\"Speech_agnosia\">Speech agnosia</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=15\" title=\"Edit section: Speech agnosia\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h5>\n<p>Pure word deafness, or speech agnosia, is an impairment in which a person maintains the ability to hear, produce speech, and even read speech, yet they are unable to understand or properly perceive speech.  These patients seem to have all of the skills necessary in order to properly process speech, yet they appear to have no experience associated with speech stimuli. Patients have reported, \"I can hear you talking, but I can't translate it\".<sup id=\"cite_ref-25\" class=\"reference\"><a href=\"#cite_note-25\">&#91;25&#93;</a></sup> Even though they are physically receiving and processing the stimuli of speech, without the ability to determine the meaning of the speech, they essentially are unable to perceive the speech at all.\n</p>\n<h5><span class=\"mw-headline\" id=\"Phonagnosia\">Phonagnosia</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=16\" title=\"Edit section: Phonagnosia\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h5>\n<p>is associated with the inability to recognize any familiar voices. In these cases, speech stimuli can be heard and even understood but the association of the speech to a certain voice is lost. This can be due to \"abnormal processing of complex vocal properties (timbre, articulation, and prosody\u2014elements that distinguish an individual voice\".<sup id=\"cite_ref-26\" class=\"reference\"><a href=\"#cite_note-26\">&#91;26&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Treatments\">Treatments</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=17\" title=\"Edit section: Treatments\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<h4><span class=\"mw-headline\" id=\"Aphasia_2\">Aphasia</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=18\" title=\"Edit section: Aphasia\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>A group of psychologists conducted a study to test the McGurk effect with aphasia patients and speech reading.<sup id=\"cite_ref-hd_22-3\" class=\"reference\"><a href=\"#cite_note-hd-22\">&#91;22&#93;</a></sup> The subjects watched dubbed videos in which the audio and visual did not match. Then after they completed the first part of the experiment, the experimenters taught the aphasic patients to speech read, which is the ability to read lips. The experimenters then conducted the same test and found that the people still had more of an advantage of audio only over visual only, but they also found that the subjects did better in audio-visual than audio alone. The patients also did improve their place of articulation and their manner of articulation. This all means that aphasic patients might benefit from learning how to speech read (lip reading).\n</p>\n<h4><span id=\"Parkinson.27s_disease_2\"></span><span class=\"mw-headline\" id=\"Parkinson's_disease_2\">Parkinson's disease</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=19\" title=\"Edit section: Parkinson&#039;s disease\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>There are quite a few drug therapies that are possible for Parkinson's disease (ex. Sinemet). Since there is no cure for it, the patient will probably end up having to have surgery done to relieve some of the symptoms. When a patient has this procedure done, they are most likely going to receive a deep brain stimulation, so it will keep the brain stimulated even though the disease tries to disable it. A study was performed to test if surgery helps the patients discover their symptoms post surgery than pre-surgery. They found that the symptoms were still present but the patients were more aware of their difficulties than before they had surgery.<sup id=\"cite_ref-ftp_23-1\" class=\"reference\"><a href=\"#cite_note-ftp-23\">&#91;23&#93;</a></sup> This shows that surgery does improve a patient's speech perception, even though it might not cure their disease.\n</p>\n<h4><span class=\"mw-headline\" id=\"Speech_agnosia_2\">Speech agnosia</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=20\" title=\"Edit section: Speech agnosia\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Again, there are no known treatments that have been found, but from case studies and experiments it is known that speech agnosia is related to lesions in the left hemisphere or both, specifically right temporoparietal dysfunctions.<sup id=\"cite_ref-27\" class=\"reference\"><a href=\"#cite_note-27\">&#91;27&#93;</a></sup> \n</p>\n<h4><span class=\"mw-headline\" id=\"Phonagnosia_2\">Phonagnosia</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=21\" title=\"Edit section: Phonagnosia\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>There is no known treatment; however, there is a case report of an epileptic woman who began to experience phonagnosia along with other impairments. Her EEG and  MRI results showed \"a right cortical parietal T2-hyperintense lesion without gadolinium enhancement and with discrete impairment of water molecule diffusion\".<sup id=\"cite_ref-28\" class=\"reference\"><a href=\"#cite_note-28\">&#91;28&#93;</a></sup> So although no treatment has been discovered, phonagnosia can be correlated to postictal parietal cortical dysfunction.\n</p>\n<h2><span class=\"mw-headline\" id=\"Research_topics\">Research topics</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=22\" title=\"Edit section: Research topics\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<h3><span class=\"mw-headline\" id=\"Infant_speech_perception\">Infant speech perception</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=23\" title=\"Edit section: Infant speech perception\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Infants begin the process of <a href=\"/wiki/Language_acquisition\" title=\"Language acquisition\">language acquisition</a> by being able to detect very small differences between speech sounds. They can discriminate all possible speech contrasts (phonemes). Gradually, as they are exposed to their native language, their perception becomes language-specific, i.e. they learn how to ignore the differences within phonemic categories of the language (differences that may well be contrastive in other languages \u2013 for example, English distinguishes two voicing categories of <a href=\"/wiki/Plosives\" class=\"mw-redirect\" title=\"Plosives\">plosives</a>, whereas <a href=\"/wiki/Thai_language#Consonants\" title=\"Thai language\">Thai has three categories</a>; infants must learn which differences are distinctive in their native language uses, and which are not). As infants learn how to sort incoming speech sounds into categories, ignoring irrelevant differences and reinforcing the contrastive ones, their perception becomes <a href=\"#Categorical_perception\">categorical</a>. Infants learn to contrast different vowel phonemes of their native language by approximately 6 months of age. The native consonantal contrasts are acquired by 11 or 12 months of age.<sup id=\"cite_ref-kawai_29-0\" class=\"reference\"><a href=\"#cite_note-kawai-29\">&#91;29&#93;</a></sup> Some researchers have proposed that infants may be able to learn the sound categories of their native language through passive listening, using a process called <a href=\"/wiki/Statistical_learning_in_language_acquisition\" title=\"Statistical learning in language acquisition\">statistical learning</a>. Others even claim that certain sound categories are innate, that is, they are genetically specified (see discussion about <a href=\"/wiki/Categorical_perception#Acquired_distinctiveness\" title=\"Categorical perception\">innate vs. acquired categorical distinctiveness</a>).\n</p><p>If day-old babies are presented with their mother's voice speaking normally, abnormally (in monotone), and a stranger's voice, they react only to their mother's voice speaking normally. When a human and a non-human sound is played, babies turn their head only to the source of human sound. It has been suggested that auditory learning begins already in the pre-natal period.<sup id=\"cite_ref-cd_30-0\" class=\"reference\"><a href=\"#cite_note-cd-30\">&#91;30&#93;</a></sup>\n</p><p>One of the techniques used to examine how infants perceive speech, besides the head-turn procedure mentioned above, is measuring their sucking rate. In such an experiment, a baby is sucking a special nipple while presented with sounds. First, the baby's normal sucking rate is established. Then a stimulus is played repeatedly. When the baby hears the stimulus for the first time the sucking rate increases but as the baby becomes <a href=\"/wiki/Habituate\" class=\"mw-redirect\" title=\"Habituate\">habituated</a> to the stimulation the sucking rate decreases and levels off. Then, a new stimulus is played to the baby. If the baby perceives the newly introduced stimulus as different from the background stimulus the sucking rate will show an increase.<sup id=\"cite_ref-cd_30-1\" class=\"reference\"><a href=\"#cite_note-cd-30\">&#91;30&#93;</a></sup> The sucking-rate and the head-turn method are some of the more traditional, behavioral methods for studying speech perception. Among the new methods (see <a href=\"#Research_methods\">Research methods</a> below) that help us to study speech perception, <a href=\"/wiki/Near-infrared_spectroscopy\" title=\"Near-infrared spectroscopy\">near-infrared spectroscopy</a> is widely used in infants.<sup id=\"cite_ref-kawai_29-1\" class=\"reference\"><a href=\"#cite_note-kawai-29\">&#91;29&#93;</a></sup>\n</p><p>It has also been discovered that even though infants' ability to distinguish between the different phonetic properties of various languages begins to decline around the age of nine months, it is possible to reverse this process by exposing them to a new language in a sufficient way. In a research study by Patricia K. Kuhl, Feng-Ming Tsao, and Huei-Mei Liu, it was discovered that if infants are spoken to and interacted with by a native speaker of Mandarin Chinese, they can actually be conditioned to retain their ability to distinguish different speech sounds within Mandarin that are very different from speech sounds found within the English language. Thus proving that given the right conditions, it is possible to prevent infants' loss of the ability to distinguish speech sounds in languages other than those found in the native language.<sup id=\"cite_ref-31\" class=\"reference\"><a href=\"#cite_note-31\">&#91;31&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Cross-language_and_second-language\">Cross-language and second-language</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=24\" title=\"Edit section: Cross-language and second-language\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>A large amount of research has studied how users of a language perceive <a href=\"/wiki/Foreign_language\" title=\"Foreign language\">foreign</a> speech (referred to as cross-language speech perception) or <a href=\"/wiki/Second_language\" title=\"Second language\">second-language</a> speech (second-language speech perception). The latter falls within the domain of <a href=\"/wiki/Second_language_acquisition\" class=\"mw-redirect\" title=\"Second language acquisition\">second language acquisition</a>.\n</p><p>Languages differ in their phonemic inventories. Naturally, this creates difficulties when a foreign language is encountered. For example, if two foreign-language sounds are assimilated to a single mother-tongue category the difference between them will be very difficult to discern. A classic example of this situation is the observation that Japanese learners of English will have problems with identifying or distinguishing English <a href=\"/wiki/Liquid_consonant\" title=\"Liquid consonant\">liquid consonants</a> <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/l/</span> and <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/r/</span> (see <a href=\"/wiki/Perception_of_English_/r/_and_/l/_by_Japanese_speakers\" title=\"Perception of English /r/ and /l/ by Japanese speakers\">Perception of English /r/ and /l/ by Japanese speakers</a>).<sup id=\"cite_ref-32\" class=\"reference\"><a href=\"#cite_note-32\">&#91;32&#93;</a></sup>\n</p><p>Best (1995) proposed a Perceptual Assimilation Model which describes possible cross-language category assimilation patterns and predicts their consequences.<sup id=\"cite_ref-33\" class=\"reference\"><a href=\"#cite_note-33\">&#91;33&#93;</a></sup> Flege (1995) formulated a Speech Learning Model which combines several hypotheses about second-language (L2) speech acquisition and which predicts, in simple words, that an L2 sound that is not too similar to a native-language (L1) sound will be easier to acquire than an L2 sound that is relatively similar to an L1 sound (because it will be perceived as more obviously \"different\" by the learner).<sup id=\"cite_ref-34\" class=\"reference\"><a href=\"#cite_note-34\">&#91;34&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"In_language_or_hearing_impairment\">In language or hearing impairment</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=25\" title=\"Edit section: In language or hearing impairment\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Research in how people with language or hearing impairment perceive speech is not only intended to discover possible treatments. It can provide insight into the principles underlying non-impaired speech perception.<sup id=\"cite_ref-35\" class=\"reference\"><a href=\"#cite_note-35\">&#91;35&#93;</a></sup> Two areas of research can serve as an example:\n</p>\n<h4><span class=\"mw-headline\" id=\"Listeners_with_aphasia\">Listeners with aphasia</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=26\" title=\"Edit section: Listeners with aphasia\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p><a href=\"/wiki/Aphasia\" title=\"Aphasia\">Aphasia</a> affects both the expression and reception of language. Both two most common types, <a href=\"/wiki/Expressive_aphasia\" title=\"Expressive aphasia\">expressive aphasia</a> and <a href=\"/wiki/Receptive_aphasia\" title=\"Receptive aphasia\">receptive aphasia</a>, affect speech perception to some extent. Expressive aphasia causes moderate difficulties for language understanding. The effect of receptive aphasia on understanding is much more severe. It is agreed upon, that aphasics suffer from perceptual deficits. They usually cannot fully distinguish place of articulation and voicing.<sup id=\"cite_ref-cse2001_36-0\" class=\"reference\"><a href=\"#cite_note-cse2001-36\">&#91;36&#93;</a></sup> As for other features, the difficulties vary. It has not yet been proven whether low-level speech-perception skills are affected in aphasia sufferers or whether their difficulties are caused by higher-level impairment alone.<sup id=\"cite_ref-cse2001_36-1\" class=\"reference\"><a href=\"#cite_note-cse2001-36\">&#91;36&#93;</a></sup>\n</p>\n<h4><span class=\"mw-headline\" id=\"Listeners_with_cochlear_implants\">Listeners with cochlear implants</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=27\" title=\"Edit section: Listeners with cochlear implants\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p><a href=\"/wiki/Cochlear_implant\" title=\"Cochlear implant\">Cochlear implantation</a> restores access to the acoustic signal in individuals with sensorineural hearing loss. The acoustic information conveyed by an implant is usually sufficient for implant users to properly recognize speech of people they know even without visual clues.<sup id=\"cite_ref-loi1998_37-0\" class=\"reference\"><a href=\"#cite_note-loi1998-37\">&#91;37&#93;</a></sup> For cochlear implant users, it is more difficult to understand unknown speakers and sounds. The perceptual abilities of children that received an implant after the age of two are significantly better than of those who were implanted in adulthood. A number of factors have been shown to influence perceptual performance, specifically: duration of deafness prior to implantation, age of onset of deafness, age at implantation (such age effects may be related to the <a href=\"/wiki/Critical_period_hypothesis\" title=\"Critical period hypothesis\">Critical period hypothesis</a>) and the duration of using an implant. There are differences between children with congenital and acquired deafness. Postlingually deaf children have better results than the prelingually deaf and adapt to a cochlear implant faster.<sup id=\"cite_ref-loi1998_37-1\" class=\"reference\"><a href=\"#cite_note-loi1998-37\">&#91;37&#93;</a></sup> In both children with cochlear implants and normal hearing, vowels and voice onset time becomes prevalent in development before the ability to discriminate the place of articulation. Several months following implantation, children with cochlear implants can normalize speech perception.\n</p>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">See also: <a href=\"/wiki/Auditory_processing_disorder\" title=\"Auditory processing disorder\">Auditory processing disorder</a></div>\n<h3><span class=\"mw-headline\" id=\"Noise\">Noise</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=28\" title=\"Edit section: Noise\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>One of the basic problems in the study of speech is how to deal with the noise in the speech signal. This is shown by the difficulty that computer speech recognition systems have with recognizing human speech. These programs can do well at recognizing speech when they have been trained on a specific speaker's voice, and under quiet conditions. However, these systems often do poorly in more realistic listening situations where humans can understand speech without difficulty.\n</p>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">See also: <a href=\"/wiki/Auditory_processing_disorder\" title=\"Auditory processing disorder\">Auditory processing disorder</a></div>\n<h3><span class=\"mw-headline\" id=\"Music-language_connection\">Music-language connection</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=29\" title=\"Edit section: Music-language connection\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">See also: <a href=\"/wiki/Cognitive_neuroscience_of_music\" title=\"Cognitive neuroscience of music\">Cognitive neuroscience of music</a></div>\n<p>Research into the relationship between <a href=\"/wiki/Music_psychology\" title=\"Music psychology\">music and cognition</a> is an emerging field related to the study of speech perception. Originally it was theorized that the neural signals for music were processed in a specialized \"module\" in the right hemisphere of the brain. Conversely, the neural signals for language were to be processed by a similar \"module\" in the left hemisphere.<sup id=\"cite_ref-Deutsch,_Diana_38-0\" class=\"reference\"><a href=\"#cite_note-Deutsch,_Diana-38\">&#91;38&#93;</a></sup> However, utilizing technologies such as fMRI machines, research has shown that two regions of the brain traditionally considered exclusively to process speech, Broca's and Wernicke's areas, also become active during musical activities such as listening to a sequence of musical chords.<sup id=\"cite_ref-Deutsch,_Diana_38-1\" class=\"reference\"><a href=\"#cite_note-Deutsch,_Diana-38\">&#91;38&#93;</a></sup> Other studies, such as one performed by Marques et al. in 2006 showed that 8-year-olds who were given six months of musical training showed an increase in both their pitch detection performance and their electrophysiological measures when made to listen to an unknown foreign language.<sup id=\"cite_ref-Portuguese_39-0\" class=\"reference\"><a href=\"#cite_note-Portuguese-39\">&#91;39&#93;</a></sup>\n</p><p>Conversely, some research has revealed that, rather than music affecting our perception of speech, our native speech can affect our perception of music. One example is the <a href=\"/wiki/Tritone_paradox\" title=\"Tritone paradox\">tritone paradox</a>. The tritone paradox is where a listener is presented with two computer-generated tones (such as C and F-Sharp) that are half an octave (or a tritone) apart and are then asked to determine whether the pitch of the sequence is descending or ascending. One such study, performed by Ms. Diana Deutsch, found that the listener's interpretation of ascending or descending pitch was influenced by the listener's language or dialect, showing variation between those raised in the south of England and those in California or from those in Vietnam and those in California whose native language was English.<sup id=\"cite_ref-Deutsch,_Diana_38-2\" class=\"reference\"><a href=\"#cite_note-Deutsch,_Diana-38\">&#91;38&#93;</a></sup> A second study, performed in 2006 on a group of English speakers and 3 groups of East Asian students at University of Southern California, discovered that English speakers who had begun musical training at or before age 5 had an 8% chance of having perfect pitch.<sup id=\"cite_ref-Deutsch,_Diana_38-3\" class=\"reference\"><a href=\"#cite_note-Deutsch,_Diana-38\">&#91;38&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Speech_phenomenology\">Speech phenomenology</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=30\" title=\"Edit section: Speech phenomenology\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<h4><span class=\"mw-headline\" id=\"The_experience_of_speech\">The experience of speech</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=31\" title=\"Edit section: The experience of speech\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Casey O'Callaghan, in his article <i>Experiencing Speech</i>, analyzes whether \"the perceptual experience of listening to speech differs in phenomenal character\"<sup id=\"cite_ref-:0_40-0\" class=\"reference\"><a href=\"#cite_note-:0-40\">&#91;40&#93;</a></sup> with regards to understanding the language being heard. He argues that an individual's experience when hearing a language they comprehend, as opposed to their experience when hearing a language they have no knowledge of, displays a difference in <i>phenomenal features</i> which he defines as \"aspects of what an experience is like\"<sup id=\"cite_ref-:0_40-1\" class=\"reference\"><a href=\"#cite_note-:0-40\">&#91;40&#93;</a></sup> for an individual.\n</p><p>If a subject who is a monolingual native English speaker is presented with a stimulus of speech in German, the string of phonemes will appear as mere sounds and will produce a very different experience than if this exact same stimulus was presented to a subject who speaks German.\n</p><p>He also examines how speech perception changes when one learning a language. If a subject with no knowledge of the Japanese language was presented with a stimulus of Japanese speech, and then was given the exact <i>same</i> stimuli after being taught Japanese, this <i>same</i> individual would have an extremely <i>different</i> experience.\n</p>\n<h2><span class=\"mw-headline\" id=\"Research_methods\">Research methods</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=32\" title=\"Edit section: Research methods\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The methods used in speech perception research can be roughly divided into three groups: behavioral, computational, and, more recently, neurophysiological methods.\n</p>\n<h3><span class=\"mw-headline\" id=\"Behavioral_methods\">Behavioral methods</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=33\" title=\"Edit section: Behavioral methods\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Behavioral experiments are based on an active role of a participant, i.e. subjects are presented with stimuli and asked to make conscious decisions about them. This can take the form of an identification test, a <a href=\"/wiki/Discrimination_test\" class=\"mw-redirect\" title=\"Discrimination test\">discrimination test</a>, similarity rating, etc. These types of experiments help to provide a basic description of how listeners perceive and categorize speech sounds.\n</p>\n<h4><span class=\"mw-headline\" id=\"Sinewave_Speech\">Sinewave Speech</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=34\" title=\"Edit section: Sinewave Speech\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>Speech perception has also been analyzed through sinewave speech, a form of synthetic speech where the human voice is replaced by sine waves that mimic the frequencies and amplitudes present in the original speech. When subjects are first presented with this speech, the sinewave speech is interpreted as random noises. But when the subjects are informed that the stimuli actually is speech and are told what is being said, \"a distinctive, nearly immediate shift occurs\"<sup id=\"cite_ref-:0_40-2\" class=\"reference\"><a href=\"#cite_note-:0-40\">&#91;40&#93;</a></sup> to how the sinewave speech is perceived.\n</p>\n<h3><span class=\"mw-headline\" id=\"Computational_methods\">Computational methods</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=35\" title=\"Edit section: Computational methods\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Computational modeling has also been used to simulate how speech may be processed by the brain to produce behaviors that are observed. Computer models have been used to address several questions in speech perception, including how the sound signal itself is processed to extract the acoustic cues used in speech, and how speech information is used for higher-level processes, such as word recognition.<sup id=\"cite_ref-mcc1986_41-0\" class=\"reference\"><a href=\"#cite_note-mcc1986-41\">&#91;41&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Neurophysiological_methods\">Neurophysiological methods</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=36\" title=\"Edit section: Neurophysiological methods\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Neurophysiological methods rely on utilizing information stemming from more direct and not necessarily conscious (pre-attentative) processes. Subjects are presented with speech stimuli in different types of tasks and the responses of the brain are measured. The brain itself can be more sensitive than it appears to be through behavioral responses. For example, the subject may not show sensitivity to the difference between two speech sounds in a discrimination test, but brain responses may reveal sensitivity to these differences.<sup id=\"cite_ref-kawai_29-2\" class=\"reference\"><a href=\"#cite_note-kawai-29\">&#91;29&#93;</a></sup> Methods used to measure neural responses to speech include <a href=\"/wiki/Event-related_potential\" title=\"Event-related potential\">event-related potentials</a>, <a href=\"/wiki/Magnetoencephalography\" title=\"Magnetoencephalography\">magnetoencephalography</a>, and <a href=\"/wiki/Near_infrared_spectroscopy\" class=\"mw-redirect\" title=\"Near infrared spectroscopy\">near infrared spectroscopy</a>. One important response used with <a href=\"/wiki/Event-related_potential\" title=\"Event-related potential\">event-related potentials</a> is the <a href=\"/wiki/Mismatch_negativity\" title=\"Mismatch negativity\">mismatch negativity</a>, which occurs when speech stimuli are acoustically different from a stimulus that the subject heard previously.\n</p><p>Neurophysiological methods were introduced into speech perception research for several reasons:\n</p>\n<blockquote><p><i>Behavioral responses may reflect late, conscious processes and be affected by other systems such as orthography, and thus they may mask speaker's ability to recognize sounds based on lower-level acoustic distributions.</i><sup id=\"cite_ref-kaz_42-0\" class=\"reference\"><a href=\"#cite_note-kaz-42\">&#91;42&#93;</a></sup></p></blockquote>\n<p>Without the necessity of taking an active part in the test, even infants can be tested; this feature is crucial in research into acquisition processes. The possibility to observe low-level auditory processes independently from the higher-level ones makes it possible to address long-standing theoretical issues such as whether or not humans possess a specialized module for perceiving speech<sup id=\"cite_ref-43\" class=\"reference\"><a href=\"#cite_note-43\">&#91;43&#93;</a></sup><sup id=\"cite_ref-44\" class=\"reference\"><a href=\"#cite_note-44\">&#91;44&#93;</a></sup> or whether or not some complex acoustic invariance (see <a href=\"#Lack_of_invariance\">lack of invariance</a> above) underlies the recognition of a speech sound.<sup id=\"cite_ref-45\" class=\"reference\"><a href=\"#cite_note-45\">&#91;45&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Theories\">Theories</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=37\" title=\"Edit section: Theories\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Research into speech perception (SP) has by no means explained every aspect of the processes involved. A lot of what has been said about SP is a matter of theory. Several theories have been devised to develop some of the above-mentioned and other unclear issues. Not all of them give satisfactory explanations of all problems, however the research they inspired has yielded a lot of useful data.\n</p>\n<h3><span class=\"mw-headline\" id=\"Speech_mode_hypothesis\">Speech mode hypothesis</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=38\" title=\"Edit section: Speech mode hypothesis\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Speech mode hypothesis is the idea that the perception of speech requires the use of specialized mental processing.<sup id=\"cite_ref-Ingram_46-0\" class=\"reference\"><a href=\"#cite_note-Ingram-46\">&#91;46&#93;</a></sup><sup id=\"cite_ref-Parker_47-0\" class=\"reference\"><a href=\"#cite_note-Parker-47\">&#91;47&#93;</a></sup> The speech mode hypothesis is a branch off of Fodor's modularity theory (see <a href=\"/wiki/Modularity_of_mind\" title=\"Modularity of mind\">modularity of mind</a>). It utilizes a vertical processing mechanism where limited stimuli are processed by special-purpose areas of the brain that are stimuli specific.<sup id=\"cite_ref-Parker_47-1\" class=\"reference\"><a href=\"#cite_note-Parker-47\">&#91;47&#93;</a></sup>\n</p><p>Two versions of speech mode hypothesis:<sup id=\"cite_ref-Ingram_46-1\" class=\"reference\"><a href=\"#cite_note-Ingram-46\">&#91;46&#93;</a></sup>\n</p>\n<ul><li>Weak version \u2013 listening to speech engages previous knowledge of language.</li>\n<li>Strong version \u2013 listening to speech engages specialized speech mechanisms for perceiving speech.</li></ul>\n<p>Three important experimental paradigms have evolved in the search to find evidence for the speech mode hypothesis. These are <a href=\"/wiki/Dichotic_listening\" title=\"Dichotic listening\">dichotic listening</a>, <a href=\"/wiki/Categorical_perception\" title=\"Categorical perception\">categorical perception</a>, and <a href=\"/wiki/Duplex_perception\" title=\"Duplex perception\">duplex perception</a>.<sup id=\"cite_ref-Ingram_46-2\" class=\"reference\"><a href=\"#cite_note-Ingram-46\">&#91;46&#93;</a></sup> Through the research in these categories it has been found that there may not be a specific speech mode but instead one for auditory codes that require complicated auditory processing. Also it seems that modularity is learned in perceptual systems.<sup id=\"cite_ref-Ingram_46-3\" class=\"reference\"><a href=\"#cite_note-Ingram-46\">&#91;46&#93;</a></sup> Despite this the evidence and counter-evidence for the speech mode hypothesis is still unclear and needs further research.\n</p>\n<h3><span class=\"mw-headline\" id=\"Motor_theory\">Motor theory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=39\" title=\"Edit section: Motor theory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Motor_theory_of_speech_perception\" title=\"Motor theory of speech perception\">Motor theory of speech perception</a></div>\n<p>Some of the earliest work in the study of how humans perceive speech sounds was conducted by <a href=\"/wiki/Alvin_Liberman\" title=\"Alvin Liberman\">Alvin Liberman</a> and his colleagues at <a href=\"/wiki/Haskins_Laboratories\" title=\"Haskins Laboratories\">Haskins Laboratories</a>.<sup id=\"cite_ref-lib57_48-0\" class=\"reference\"><a href=\"#cite_note-lib57-48\">&#91;48&#93;</a></sup> Using a speech synthesizer, they constructed speech sounds that varied in <a href=\"/wiki/Place_of_articulation\" title=\"Place of articulation\">place of articulation</a> along a continuum from <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/b\u0251/</span> to <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/d\u0251/</span> to <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/\u0261\u0251/</span>. Listeners were asked to identify which sound they heard and to discriminate between two different sounds. The results of the experiment showed that listeners grouped sounds into discrete categories, even though the sounds they were hearing were varying continuously. Based on these results, they proposed the notion of <a href=\"/wiki/Categorical_perception\" title=\"Categorical perception\">categorical perception</a> as a mechanism by which humans can identify speech sounds.\n</p><p>More recent research using different tasks and methods suggests that listeners are highly sensitive to acoustic differences within a single phonetic category, contrary to a strict categorical account of speech perception.\n</p><p>To provide a theoretical account of the <a href=\"/wiki/Categorical_perception\" title=\"Categorical perception\">categorical perception</a> data, Liberman and colleagues<sup id=\"cite_ref-lib67_49-0\" class=\"reference\"><a href=\"#cite_note-lib67-49\">&#91;49&#93;</a></sup> worked out the motor theory of speech perception, where \"the complicated articulatory encoding was assumed to be decoded in the perception of speech by the same processes that are involved in production\"<sup id=\"cite_ref-np_1-2\" class=\"reference\"><a href=\"#cite_note-np-1\">&#91;1&#93;</a></sup> (this is referred to as analysis-by-synthesis). For instance, the English consonant <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/d/</span> may vary in its acoustic details across different phonetic contexts (see <a href=\"#Acoustic_cues\">above</a>), yet all <span title=\"Representation in the International Phonetic Alphabet (IPA)\" class=\"IPA\">/d/</span>'s as perceived by a listener fall within one category (voiced alveolar plosive) and that is because \"linguistic representations are abstract, canonical, phonetic segments or the gestures that underlie these segments\".<sup id=\"cite_ref-np_1-3\" class=\"reference\"><a href=\"#cite_note-np-1\">&#91;1&#93;</a></sup> When describing units of perception, Liberman later abandoned articulatory movements and proceeded to the neural commands to the articulators<sup id=\"cite_ref-lib70_50-0\" class=\"reference\"><a href=\"#cite_note-lib70-50\">&#91;50&#93;</a></sup> and even later to intended articulatory gestures,<sup id=\"cite_ref-lib85_51-0\" class=\"reference\"><a href=\"#cite_note-lib85-51\">&#91;51&#93;</a></sup> thus \"the neural representation of the utterance that determines the speaker's production is the distal object the listener perceives\".<sup id=\"cite_ref-lib85_51-1\" class=\"reference\"><a href=\"#cite_note-lib85-51\">&#91;51&#93;</a></sup> The theory is closely related to the <a href=\"/wiki/Modularity_of_mind\" title=\"Modularity of mind\">modularity</a> hypothesis, which proposes the existence of a special-purpose module, which is supposed to be innate and probably human-specific.\n</p><p>The theory has been criticized in terms of not being able to \"provide an account of just how acoustic signals are translated into intended gestures\"<sup id=\"cite_ref-hay_52-0\" class=\"reference\"><a href=\"#cite_note-hay-52\">&#91;52&#93;</a></sup> by listeners. Furthermore, it is unclear how indexical information (e.g. talker-identity) is encoded/decoded along with linguistically relevant information.\n</p>\n<h3><span class=\"mw-headline\" id=\"Direct_realist_theory\">Direct realist theory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=40\" title=\"Edit section: Direct realist theory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The direct realist theory of speech perception (mostly associated with <a href=\"/wiki/Carol_Fowler\" title=\"Carol Fowler\">Carol Fowler</a>) is a part of the more general theory of <a href=\"/wiki/Direct_realism\" class=\"mw-redirect\" title=\"Direct realism\">direct realism</a>, which postulates that perception allows us to have direct awareness of the world because it involves direct recovery of the <a href=\"/wiki/Distal_stimulus\" class=\"mw-redirect\" title=\"Distal stimulus\">distal source</a> of the event that is perceived. For speech perception, the theory asserts that the <a href=\"/wiki/Distal_stimulus\" class=\"mw-redirect\" title=\"Distal stimulus\">objects of perception</a> are actual vocal tract movements, or gestures, and not abstract phonemes or (as in the Motor Theory) events that are causally antecedent to these movements, i.e. intended gestures. Listeners perceive gestures not by means of a specialized decoder (as in the Motor Theory) but because information in the acoustic signal specifies the gestures that form it.<sup id=\"cite_ref-53\" class=\"reference\"><a href=\"#cite_note-53\">&#91;53&#93;</a></sup> By claiming that the actual articulatory gestures that produce different speech sounds are themselves the units of speech perception, the theory bypasses the problem of <a href=\"#Lack_of_invariance\">lack of invariance</a>.\n</p>\n<h3><span class=\"mw-headline\" id=\"Fuzzy-logical_model\">Fuzzy-logical model</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=41\" title=\"Edit section: Fuzzy-logical model\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The fuzzy logical theory of speech perception developed by <a href=\"/wiki/Dominic_W._Massaro\" title=\"Dominic W. Massaro\">Dominic Massaro</a><sup id=\"cite_ref-54\" class=\"reference\"><a href=\"#cite_note-54\">&#91;54&#93;</a></sup> proposes that people remember speech sounds in a probabilistic, or graded, way. It suggests that people remember descriptions of the perceptual units of language, called prototypes. Within each prototype various features may combine. However, features are not just binary (true or false), there is a <a href=\"/wiki/Fuzzy_logic\" title=\"Fuzzy logic\">fuzzy</a> value corresponding to how likely it is that a sound belongs to a particular speech category. Thus, when perceiving a speech signal our decision about what we actually hear is based on the relative goodness of the match between the stimulus information and values of particular prototypes. The final decision is based on multiple features or sources of information, even visual information (this explains the <a href=\"/wiki/McGurk_effect\" title=\"McGurk effect\">McGurk effect</a>).<sup id=\"cite_ref-hay_52-1\" class=\"reference\"><a href=\"#cite_note-hay-52\">&#91;52&#93;</a></sup> Computer models of the fuzzy logical theory have been used to demonstrate that the theory's predictions of how speech sounds are categorized correspond to the behavior of human listeners.<sup id=\"cite_ref-oden1978_55-0\" class=\"reference\"><a href=\"#cite_note-oden1978-55\">&#91;55&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Acoustic_landmarks_and_distinctive_features\">Acoustic landmarks and distinctive features</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=42\" title=\"Edit section: Acoustic landmarks and distinctive features\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Acoustic_landmarks_and_distinctive_features\" title=\"Acoustic landmarks and distinctive features\">Acoustic landmarks and distinctive features</a></div>\n<p>In addition to the proposals of Motor Theory and Direct Realism about the relation between phonological features and articulatory gestures, <a href=\"/wiki/Kenneth_N._Stevens\" title=\"Kenneth N. Stevens\">Kenneth N. Stevens</a> proposed another kind of relation: between phonological features and auditory properties. According to this view, listeners are inspecting the incoming signal for the so-called acoustic landmarks which are particular events in the spectrum carrying information about gestures which produced them. Since these gestures are limited by the capacities of humans' articulators and listeners are sensitive to their auditory correlates, the <a href=\"#Lack_of_invariance\">lack of invariance</a> simply does not exist in this model. The acoustic properties of the landmarks constitute the basis for establishing the distinctive features. Bundles of them uniquely specify phonetic segments (phonemes, syllables, words).<sup id=\"cite_ref-56\" class=\"reference\"><a href=\"#cite_note-56\">&#91;56&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Exemplar_theory\">Exemplar theory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=43\" title=\"Edit section: Exemplar theory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Exemplar models of speech perception differ from the four theories mentioned above which suppose that there is no connection between word- and talker-recognition and that the variation across talkers is \"noise\" to be filtered out.\n</p><p>The exemplar-based approaches claim listeners store information for both word- and talker-recognition. According to this theory, particular instances of speech sounds are stored in the memory of a listener. In the process of speech perception, the remembered instances of e.g. a syllable stored in the listener's memory are compared with the incoming stimulus so that the stimulus can be categorized. Similarly, when recognizing a talker, all the memory traces of utterances produced by that talker are activated and the talker's identity is determined. Supporting this theory are several experiments reported by Johnson<sup id=\"cite_ref-john_13-1\" class=\"reference\"><a href=\"#cite_note-john-13\">&#91;13&#93;</a></sup> that suggest that our signal identification is more accurate when we are familiar with the talker or when we have visual representation of the talker's gender. When the talker is unpredictable or the sex misidentified, the error rate in word-identification is much higher.\n</p><p>The exemplar models have to face several objections, two of which are (1) insufficient memory capacity to store every utterance ever heard and, concerning the ability to produce what was heard, (2) whether also the talker's own articulatory gestures are stored or computed when producing utterances that would sound as the auditory memories.<sup id=\"cite_ref-john_13-2\" class=\"reference\"><a href=\"#cite_note-john-13\">&#91;13&#93;</a></sup><sup id=\"cite_ref-hay_52-2\" class=\"reference\"><a href=\"#cite_note-hay-52\">&#91;52&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=44\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li>Related to the case study of <a href=\"/wiki/Genie_(feral_child)\" title=\"Genie (feral child)\">Genie (feral child)</a></li>\n<li><a href=\"/wiki/Neurocomputational_speech_processing\" title=\"Neurocomputational speech processing\">Neurocomputational speech processing</a></li>\n<li><a href=\"/wiki/Multisensory_integration\" title=\"Multisensory integration\">Multisensory integration</a></li>\n<li><a href=\"/wiki/Origin_of_speech\" title=\"Origin of speech\">Origin of speech</a></li>\n<li><a href=\"/wiki/Speech-Language_Pathology\" class=\"mw-redirect\" title=\"Speech-Language Pathology\">Speech-Language Pathology</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=45\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\n<ol class=\"references\">\n<li id=\"cite_note-np-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-np_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-np_1-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-np_1-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-np_1-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation encyclopaedia\">Nygaard, L.C., Pisoni, D.B. (1995). \"Speech Perception: New Directions in Research and Theory\".  In J.L. Miller; P.D. Eimas. <i>Handbook of Perception and Cognition: Speech, Language, and Communication</i>. San Diego: Academic Press.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Speech+Perception%3A+New+Directions+in+Research+and+Theory&amp;rft.btitle=Handbook+of+Perception+and+Cognition%3A+Speech%2C+Language%2C+and+Communication&amp;rft.place=San+Diego&amp;rft.pub=Academic+Press&amp;rft.date=1995&amp;rft.au=Nygaard%2C+L.C.%2C+Pisoni%2C+D.B.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Klatt, D.H. (1976). \"Linguistic uses of segmental duration in English: Acoustic and perceptual evidence\". <i>Journal of the Acoustical Society of America</i>. <b>59</b> (5): 1208\u20131221. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/1976ASAJ...59.1208K\">1976ASAJ...59.1208K</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1121/1.380986\">10.1121/1.380986</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/956516\">956516</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Acoustical+Society+of+America&amp;rft.atitle=Linguistic+uses+of+segmental+duration+in+English%3A+Acoustic+and+perceptual+evidence&amp;rft.volume=59&amp;rft.issue=5&amp;rft.pages=1208-1221&amp;rft.date=1976&amp;rft_id=info%3Apmid%2F956516&amp;rft_id=info%3Adoi%2F10.1121%2F1.380986&amp;rft_id=info%3Abibcode%2F1976ASAJ...59.1208K&amp;rft.au=Klatt%2C+D.H.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Halle, M., Mohanan, K.P. (1985). \"Segmental phonology of modern English\". <i>Linguistic Inquiry</i>. <b>16</b> (1): 57\u2013116.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Linguistic+Inquiry&amp;rft.atitle=Segmental+phonology+of+modern+English&amp;rft.volume=16&amp;rft.issue=1&amp;rft.pages=57-116&amp;rft.date=1985&amp;rft.au=Halle%2C+M.%2C+Mohanan%2C+K.P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Liberman, A.M. (1957). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.haskins.yale.edu/Reprints/HL0016.pdf\">\"Some results of research on speech perception\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>Journal of the Acoustical Society of America</i>. <b>29</b> (1): 117\u2013123. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/1957ASAJ...29..117L\">1957ASAJ...29..117L</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1121/1.1908635\">10.1121/1.1908635</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-17</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Acoustical+Society+of+America&amp;rft.atitle=Some+results+of+research+on+speech+perception&amp;rft.volume=29&amp;rft.issue=1&amp;rft.pages=117-123&amp;rft.date=1957&amp;rft_id=info%3Adoi%2F10.1121%2F1.1908635&amp;rft_id=info%3Abibcode%2F1957ASAJ...29..117L&amp;rft.au=Liberman%2C+A.M.&amp;rft_id=http%3A%2F%2Fwww.haskins.yale.edu%2FReprints%2FHL0016.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-fow-5\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-fow_5-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-fow_5-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation encyclopaedia\">Fowler, C.A. (1995). \"Speech production\".  In J.L. Miller; P.D. Eimas. <i>Handbook of Perception and Cognition: Speech, Language, and Communication</i>. San Diego: Academic Press.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Speech+production&amp;rft.btitle=Handbook+of+Perception+and+Cognition%3A+Speech%2C+Language%2C+and+Communication&amp;rft.place=San+Diego&amp;rft.pub=Academic+Press&amp;rft.date=1995&amp;rft.au=Fowler%2C+C.A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Hillenbrand, J.M., Clark, M.J., Nearey, T.M. (2001). \"Effects of consonant environment on vowel formant patterns\". <i>Journal of the Acoustical Society of America</i>. <b>109</b> (2): 748\u2013763. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/2001ASAJ..109..748H\">2001ASAJ..109..748H</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1121/1.1337959\">10.1121/1.1337959</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/11248979\">11248979</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Acoustical+Society+of+America&amp;rft.atitle=Effects+of+consonant+environment+on+vowel+formant+patterns&amp;rft.volume=109&amp;rft.issue=2&amp;rft.pages=748-763&amp;rft.date=2001&amp;rft_id=info%3Apmid%2F11248979&amp;rft_id=info%3Adoi%2F10.1121%2F1.1337959&amp;rft_id=info%3Abibcode%2F2001ASAJ..109..748H&amp;rft.au=Hillenbrand%2C+J.M.%2C+Clark%2C+M.J.%2C+Nearey%2C+T.M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Lisker, L., Abramson, A.S. (1967). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.haskins.yale.edu/Reprints/HL0067.pdf\">\"Some effects of context on voice onset time in English plosives\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>Language and Speech</i>. <b>10</b> (1): 1\u201328. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/6044530\">6044530</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-17</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Language+and+Speech&amp;rft.atitle=Some+effects+of+context+on+voice+onset+time+in+English+plosives&amp;rft.volume=10&amp;rft.issue=1&amp;rft.pages=1-28&amp;rft.date=1967&amp;rft_id=info%3Apmid%2F6044530&amp;rft.au=Lisker%2C+L.%2C+Abramson%2C+A.S.&amp;rft_id=http%3A%2F%2Fwww.haskins.yale.edu%2FReprints%2FHL0067.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-hill-8\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-hill_8-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-hill_8-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Hillenbrand, J., Getty, L.A., Clark, M.J., Wheeler, K. (1995). \"Acoustic characteristics of American English vowels\". <i>Journal of the Acoustical Society of America</i>. <b>97</b> (5 Pt 1): 3099\u20133111. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/1995ASAJ...97.3099H\">1995ASAJ...97.3099H</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1121/1.411872\">10.1121/1.411872</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/7759650\">7759650</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Acoustical+Society+of+America&amp;rft.atitle=Acoustic+characteristics+of+American+English+vowels&amp;rft.volume=97&amp;rft.issue=5+Pt+1&amp;rft.pages=3099-3111&amp;rft.date=1995&amp;rft_id=info%3Apmid%2F7759650&amp;rft_id=info%3Adoi%2F10.1121%2F1.411872&amp;rft_id=info%3Abibcode%2F1995ASAJ...97.3099H&amp;rft.au=Hillenbrand%2C+J.%2C+Getty%2C+L.A.%2C+Clark%2C+M.J.%2C+Wheeler%2C+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Houston, Derek M.; Juscyk, Peter W. (October 2000). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20140430034452/http://babytalk.iupui.edu/pdfs/HoustonJusczyk_2000.pdf\">\"The role of talker-specific information in word segmentation by infants\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Journal of Experimental Psychology: Human Perception and Performance</i>. <b>26</b> (5): 1570\u20131582. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1037/0096-1523.26.5.1570\">10.1037/0096-1523.26.5.1570</a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://babytalk.iupui.edu/pdfs/HoustonJusczyk_2000.pdf\">the original</a> <span style=\"font-size:85%;\">(PDF)</span> on 2014-04-30<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">1 March</span> 2012</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+Psychology%3A+Human+Perception+and+Performance&amp;rft.atitle=The+role+of+talker-specific+information+in+word+segmentation+by+infants&amp;rft.volume=26&amp;rft.issue=5&amp;rft.pages=1570-1582&amp;rft.date=2000-10&amp;rft_id=info%3Adoi%2F10.1037%2F0096-1523.26.5.1570&amp;rft.aulast=Houston&amp;rft.aufirst=Derek+M.&amp;rft.au=Juscyk%2C+Peter+W.&amp;rft_id=http%3A%2F%2Fbabytalk.iupui.edu%2Fpdfs%2FHoustonJusczyk_2000.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Hay, Jennifer; Drager, Katie (2010). \"Stuffed toys and speech perception\". <i>Linguistics</i>. <b>48</b> (4): 865\u2013892. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1515/LING.2010.027\">10.1515/LING.2010.027</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Linguistics&amp;rft.atitle=Stuffed+toys+and+speech+perception&amp;rft.volume=48&amp;rft.issue=4&amp;rft.pages=865-892&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1515%2FLING.2010.027&amp;rft.aulast=Hay&amp;rft.aufirst=Jennifer&amp;rft.au=Drager%2C+Katie&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-sg-11\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-sg_11-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-sg_11-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Syrdal, A.K., Gopal, H.S. (1986). \"A perceptual model of vowel recognition based on the auditory representation of American English vowels\". <i>Journal of the Acoustical Society of America</i>. <b>79</b> (4): 1086\u20131100. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/1986ASAJ...79.1086S\">1986ASAJ...79.1086S</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1121/1.393381\">10.1121/1.393381</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/3700864\">3700864</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Acoustical+Society+of+America&amp;rft.atitle=A+perceptual+model+of+vowel+recognition+based+on+the+auditory+representation+of+American+English+vowels&amp;rft.volume=79&amp;rft.issue=4&amp;rft.pages=1086-1100&amp;rft.date=1986&amp;rft_id=info%3Apmid%2F3700864&amp;rft_id=info%3Adoi%2F10.1121%2F1.393381&amp;rft_id=info%3Abibcode%2F1986ASAJ...79.1086S&amp;rft.au=Syrdal%2C+A.K.%2C+Gopal%2C+H.S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation encyclopaedia\">Strange, W. (1999). \"Perception of vowels: Dynamic constancy\".  In J.M. Pickett. <i>The Acoustics of Speech Communication: Fundamentals, Speech Perception Theory, and Technology</i>. Needham Heights (MA): Allyn &amp; Bacon.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Perception+of+vowels%3A+Dynamic+constancy&amp;rft.btitle=The+Acoustics+of+Speech+Communication%3A+Fundamentals%2C+Speech+Perception+Theory%2C+and+Technology&amp;rft.place=Needham+Heights+%28MA%29&amp;rft.pub=Allyn+%26+Bacon&amp;rft.date=1999&amp;rft.au=Strange%2C+W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-john-13\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-john_13-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-john_13-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-john_13-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation encyclopaedia\">Johnson, K. (2005). <a rel=\"nofollow\" class=\"external text\" href=\"http://corpus.linguistics.berkeley.edu/~kjohnson/papers/revised_chapter.pdf\">\"Speaker Normalization in speech perception\"</a> <span style=\"font-size:85%;\">(PDF)</span>.  In Pisoni, D.B.; Remez, R. <i>The Handbook of Speech Perception</i>. Oxford: Blackwell Publishers<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-17</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Speaker+Normalization+in+speech+perception&amp;rft.btitle=The+Handbook+of+Speech+Perception&amp;rft.place=Oxford&amp;rft.pub=Blackwell+Publishers&amp;rft.date=2005&amp;rft.au=Johnson%2C+K.&amp;rft_id=http%3A%2F%2Fcorpus.linguistics.berkeley.edu%2F~kjohnson%2Fpapers%2Frevised_chapter.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\"><a href=\"/wiki/Nikolai_Trubetzkoy\" title=\"Nikolai Trubetzkoy\">Trubetzkoy, Nikolay S.</a> (1969). <i>Principles of phonology</i>. Berkeley and Los Angeles: University of California Press. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/0-520-01535-5\" title=\"Special:BookSources/0-520-01535-5\">0-520-01535-5</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Principles+of+phonology&amp;rft.place=Berkeley+and+Los+Angeles&amp;rft.pub=University+of+California+Press&amp;rft.date=1969&amp;rft.isbn=0-520-01535-5&amp;rft.aulast=Trubetzkoy&amp;rft.aufirst=Nikolay+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Iverson, P., Kuhl, P.K. (1995). \"Mapping the perceptual magnet effect for speech using signal detection theory and multidimensional scaling\". <i>Journal of the Acoustical Society of America</i>. <b>97</b> (1): 553\u2013562. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/1995ASAJ...97..553I\">1995ASAJ...97..553I</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1121/1.412280\">10.1121/1.412280</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/7860832\">7860832</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Acoustical+Society+of+America&amp;rft.atitle=Mapping+the+perceptual+magnet+effect+for+speech+using+signal+detection+theory+and+multidimensional+scaling&amp;rft.volume=97&amp;rft.issue=1&amp;rft.pages=553-562&amp;rft.date=1995&amp;rft_id=info%3Apmid%2F7860832&amp;rft_id=info%3Adoi%2F10.1121%2F1.412280&amp;rft_id=info%3Abibcode%2F1995ASAJ...97..553I&amp;rft.au=Iverson%2C+P.%2C+Kuhl%2C+P.K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-la-16\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-la_16-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-la_16-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation conference\">Lisker, L., Abramson, A.S. (1970). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.haskins.yale.edu/Reprints/HL0087.pdf\">\"The voicing dimension: Some experiments in comparative phonetics\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>Proc. 6th International Congress of Phonetic Sciences</i>. Prague: Academia. pp.&#160;563\u2013567<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-17</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=The+voicing+dimension%3A+Some+experiments+in+comparative+phonetics&amp;rft.btitle=Proc.+6th+International+Congress+of+Phonetic+Sciences&amp;rft.place=Prague&amp;rft.pages=563-567&amp;rft.pub=Academia&amp;rft.date=1970&amp;rft.au=Lisker%2C+L.%2C+Abramson%2C+A.S.&amp;rft_id=http%3A%2F%2Fwww.haskins.yale.edu%2FReprints%2FHL0087.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Warren, R.M. (1970). \"Restoration of missing speech sounds\". <i>Science</i>. <b>167</b> (3917): 392\u2013393. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/1970Sci...167..392W\">1970Sci...167..392W</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1126/science.167.3917.392\">10.1126/science.167.3917.392</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/5409744\">5409744</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Science&amp;rft.atitle=Restoration+of+missing+speech+sounds&amp;rft.volume=167&amp;rft.issue=3917&amp;rft.pages=392-393&amp;rft.date=1970&amp;rft_id=info%3Apmid%2F5409744&amp;rft_id=info%3Adoi%2F10.1126%2Fscience.167.3917.392&amp;rft_id=info%3Abibcode%2F1970Sci...167..392W&amp;rft.au=Warren%2C+R.M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation conference\">Garnes, S., Bond, Z.S. (1976). \"The relationship between acoustic information and semantic expectation\". <i>Phonologica 1976</i>. Innsbruck. pp.&#160;285\u2013293.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=The+relationship+between+acoustic+information+and+semantic+expectation&amp;rft.btitle=Phonologica+1976&amp;rft.place=Innsbruck&amp;rft.pages=285-293&amp;rft.date=1976&amp;rft.au=Garnes%2C+S.%2C+Bond%2C+Z.S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-19\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Jongman A, Wang Y, Kim BH (December 2003). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20130614064103/http://www.sfu.ca/~yuew/documents/jongman%20wang%20kim%20JSLHR%20proof.pdf\">\"Contributions of semantic and facial information to perception of nonsibilant fricatives\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>J. Speech Lang. Hear. Res</i>. <b>46</b> (6): 1367\u201377. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1044/1092-4388%282003/106%29\">10.1044/1092-4388(2003/106)</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/14700361\">14700361</a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"https://www.sfu.ca/~yuew/documents/jongman%20wang%20kim%20JSLHR%20proof.pdf\">the original</a> <span style=\"font-size:85%;\">(PDF)</span> on 2013-06-14<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2017-09-14</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=J.+Speech+Lang.+Hear.+Res.&amp;rft.atitle=Contributions+of+semantic+and+facial+information+to+perception+of+nonsibilant+fricatives&amp;rft.volume=46&amp;rft.issue=6&amp;rft.pages=1367-77&amp;rft.date=2003-12&amp;rft_id=info%3Adoi%2F10.1044%2F1092-4388%282003%2F106%29&amp;rft_id=info%3Apmid%2F14700361&amp;rft.aulast=Jongman&amp;rft.aufirst=A&amp;rft.au=Wang%2C+Y&amp;rft.au=Kim%2C+BH&amp;rft_id=https%3A%2F%2Fwww.sfu.ca%2F~yuew%2Fdocuments%2Fjongman%2520wang%2520kim%2520JSLHR%2520proof.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Poeppel,_Monahan,_2008-20\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Poeppel,_Monahan,_2008_20-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Poeppel, David; Monahan, Philip J. (2008). \"Speech Perception: Cognitive Foundations and Cortical Implementation\". <i>Current Directions in Psychological Science</i>. <b>17</b> (2): 80\u201385. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1111/j.1467-8721.2008.00553.x\">10.1111/j.1467-8721.2008.00553.x</a>. <a href=\"/wiki/International_Standard_Serial_Number\" title=\"International Standard Serial Number\">ISSN</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.worldcat.org/issn/0963-7214\">0963-7214</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Current+Directions+in+Psychological+Science&amp;rft.atitle=Speech+Perception%3A+Cognitive+Foundations+and+Cortical+Implementation&amp;rft.volume=17&amp;rft.issue=2&amp;rft.pages=80-85&amp;rft.date=2008&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1467-8721.2008.00553.x&amp;rft.issn=0963-7214&amp;rft.aulast=Poeppel&amp;rft.aufirst=David&amp;rft.au=Monahan%2C+Philip+J.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-21\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Hickok G, Poeppel D (May 2007). \"The cortical organization of speech processing\". <i>Nat. Rev. Neurosci</i>. <b>8</b> (5): 393\u2013402. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1038/nrn2113\">10.1038/nrn2113</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/17431404\">17431404</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nat.+Rev.+Neurosci.&amp;rft.atitle=The+cortical+organization+of+speech+processing&amp;rft.volume=8&amp;rft.issue=5&amp;rft.pages=393-402&amp;rft.date=2007-05&amp;rft_id=info%3Adoi%2F10.1038%2Fnrn2113&amp;rft_id=info%3Apmid%2F17431404&amp;rft.aulast=Hickok&amp;rft.aufirst=G&amp;rft.au=Poeppel%2C+D&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-hd-22\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-hd_22-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-hd_22-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-hd_22-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-hd_22-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Hessler, Dorte; Jonkers, Bastiaanse (December 2010). \"The influence of phonetic dimensions on aphasic speech perception\". <i>Clinical Linguistics and Phonetics</i>. 12. <b>24</b>: 980\u2013996. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.3109/02699206.2010.507297\">10.3109/02699206.2010.507297</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Clinical+Linguistics+and+Phonetics&amp;rft.atitle=The+influence+of+phonetic+dimensions+on+aphasic+speech+perception&amp;rft.volume=24&amp;rft.pages=980-996&amp;rft.date=2010-12&amp;rft_id=info%3Adoi%2F10.3109%2F02699206.2010.507297&amp;rft.aulast=Hessler&amp;rft.aufirst=Dorte&amp;rft.au=Jonkers%2C+Bastiaanse&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-ftp-23\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-ftp_23-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-ftp_23-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Frost, Eleanor; Tripoliti, Hariz; Pring, Limousin (2010). \"Self-perception of speech changes in patients with Parkinson's disease following <a href=\"/wiki/Deep_brain_stimulation\" title=\"Deep brain stimulation\">deep brain stimulation</a> of the subthalamic nucleus\". <i>International Journal of Speech-Language Pathology</i>. <b>12</b> (5): 399\u2013404. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.3109/17549507.2010.497560\">10.3109/17549507.2010.497560</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Speech-Language+Pathology&amp;rft.atitle=Self-perception+of+speech+changes+in+patients+with+Parkinson%27s+disease+following+deep+brain+stimulation+of+the+subthalamic+nucleus&amp;rft.volume=12&amp;rft.issue=5&amp;rft.pages=399-404&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.3109%2F17549507.2010.497560&amp;rft.aulast=Frost&amp;rft.aufirst=Eleanor&amp;rft.au=Tripoliti%2C+Hariz&amp;rft.au=Pring%2C+Limousin&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-24\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-24\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\"><a rel=\"nofollow\" class=\"external text\" href=\"https://www.merriam-webster.com/dictionary/agnosia\">\"Definition of AGNOSIA\"</a>. <i>www.merriam-webster.com</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2017-12-15</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=www.merriam-webster.com&amp;rft.atitle=Definition+of+AGNOSIA&amp;rft_id=https%3A%2F%2Fwww.merriam-webster.com%2Fdictionary%2Fagnosia&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-25\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-25\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Howard, Harry (2017). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.tulane.edu/~h0Ward/BrLg/index.html\">\"Welcome to Brain and Language\"</a>. <i>Welcome to Brain and Language</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Welcome+to+Brain+and+Language&amp;rft.atitle=Welcome+to+Brain+and+Language&amp;rft.date=2017&amp;rft.aulast=Howard&amp;rft.aufirst=Harry&amp;rft_id=http%3A%2F%2Fwww.tulane.edu%2F~h0Ward%2FBrLg%2Findex.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-26\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-26\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Rocha, Sofia; Amorim, Jos\u00e9 Manuel; Machado, \u00c1lvaro Alexandre; Ferreira, Carla Maria (2015-04-01). <a rel=\"nofollow\" class=\"external text\" href=\"https://neuro.psychiatryonline.org/doi/10.1176/appi.neuropsych.14040073\">\"Phonagnosia and Inability to Perceive Time Passage in Right Parietal Lobe Epilepsy\"</a>. <i>The Journal of Neuropsychiatry and Clinical Neurosciences</i>. <b>27</b> (2): e154\u2013e155. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1176/appi.neuropsych.14040073\">10.1176/appi.neuropsych.14040073</a>. <a href=\"/wiki/International_Standard_Serial_Number\" title=\"International Standard Serial Number\">ISSN</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.worldcat.org/issn/0895-0172\">0895-0172</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Neuropsychiatry+and+Clinical+Neurosciences&amp;rft.atitle=Phonagnosia+and+Inability+to+Perceive+Time+Passage+in+Right+Parietal+Lobe+Epilepsy&amp;rft.volume=27&amp;rft.issue=2&amp;rft.pages=e154-e155&amp;rft.date=2015-04-01&amp;rft_id=info%3Adoi%2F10.1176%2Fappi.neuropsych.14040073&amp;rft.issn=0895-0172&amp;rft.aulast=Rocha&amp;rft.aufirst=Sofia&amp;rft.au=Amorim%2C+Jos%C3%A9+Manuel&amp;rft.au=Machado%2C+%C3%81lvaro+Alexandre&amp;rft.au=Ferreira%2C+Carla+Maria&amp;rft_id=https%3A%2F%2Fneuro.psychiatryonline.org%2Fdoi%2F10.1176%2Fappi.neuropsych.14040073&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-27\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-27\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Lambert, J. (1999). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.sciencedirect.com/science/article/pii/S0010945289800073\">\"Auditory Agnosia with relative sparing of speech perception\"</a>. <i>Neurocase</i>. <b>5</b>: 394. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1093/neucas/5.5.394\">10.1093/neucas/5.5.394</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neurocase&amp;rft.atitle=Auditory+Agnosia+with+relative+sparing+of+speech+perception&amp;rft.volume=5&amp;rft.pages=394&amp;rft.date=1999&amp;rft_id=info%3Adoi%2F10.1093%2Fneucas%2F5.5.394&amp;rft.aulast=Lambert&amp;rft.aufirst=J.&amp;rft_id=http%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2FS0010945289800073&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-28\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-28\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Rocha, Sofia; Amorim, Jos\u00e9 Manuel; Machado, \u00c1lvaro Alexandre; Ferreira, Carla Maria (2015-04-01). <a rel=\"nofollow\" class=\"external text\" href=\"https://neuro.psychiatryonline.org/doi/10.1176/appi.neuropsych.14040073\">\"Phonagnosia and Inability to Perceive Time Passage in Right Parietal Lobe Epilepsy\"</a>. <i>The Journal of Neuropsychiatry and Clinical Neurosciences</i>. <b>27</b> (2): e154\u2013e155. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1176/appi.neuropsych.14040073\">10.1176/appi.neuropsych.14040073</a>. <a href=\"/wiki/International_Standard_Serial_Number\" title=\"International Standard Serial Number\">ISSN</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.worldcat.org/issn/0895-0172\">0895-0172</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Neuropsychiatry+and+Clinical+Neurosciences&amp;rft.atitle=Phonagnosia+and+Inability+to+Perceive+Time+Passage+in+Right+Parietal+Lobe+Epilepsy&amp;rft.volume=27&amp;rft.issue=2&amp;rft.pages=e154-e155&amp;rft.date=2015-04-01&amp;rft_id=info%3Adoi%2F10.1176%2Fappi.neuropsych.14040073&amp;rft.issn=0895-0172&amp;rft.aulast=Rocha&amp;rft.aufirst=Sofia&amp;rft.au=Amorim%2C+Jos%C3%A9+Manuel&amp;rft.au=Machado%2C+%C3%81lvaro+Alexandre&amp;rft.au=Ferreira%2C+Carla+Maria&amp;rft_id=https%3A%2F%2Fneuro.psychiatryonline.org%2Fdoi%2F10.1176%2Fappi.neuropsych.14040073&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-kawai-29\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-kawai_29-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-kawai_29-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-kawai_29-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Minagawa-Kawai, Y., Mori, K., Naoi, N., Kojima, S. (2006). \"Neural Attunement Processes in Infants during the Acquisition of a Language-Specific Phonemic Contrast\". <i>The Journal of Neuroscience</i>. <b>27</b> (2): 315\u2013321. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1523/JNEUROSCI.1984-06.2007\">10.1523/JNEUROSCI.1984-06.2007</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/17215392\">17215392</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=The+Journal+of+Neuroscience&amp;rft.atitle=Neural+Attunement+Processes+in+Infants+during+the+Acquisition+of+a+Language-Specific+Phonemic+Contrast&amp;rft.volume=27&amp;rft.issue=2&amp;rft.pages=315-321&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1523%2FJNEUROSCI.1984-06.2007&amp;rft_id=info%3Apmid%2F17215392&amp;rft.au=Minagawa-Kawai%2C+Y.%2C+Mori%2C+K.%2C+Naoi%2C+N.%2C+Kojima%2C+S.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-cd-30\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-cd_30-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-cd_30-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Crystal, David (2005). <i>The Cambridge Encyclopedia of Language</i>. Cambridge: CUP. <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/0-521-55967-7\" title=\"Special:BookSources/0-521-55967-7\">0-521-55967-7</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=The+Cambridge+Encyclopedia+of+Language&amp;rft.place=Cambridge&amp;rft.pub=CUP&amp;rft.date=2005&amp;rft.isbn=0-521-55967-7&amp;rft.aulast=Crystal&amp;rft.aufirst=David&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-31\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-31\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Kuhl, Patricia K.; Feng-Ming Tsao; Huei-Mei Liu (July 2003). <a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC166444\">\"Foreign-language experience in infancy: Effects of short-term exposure and social interaction on phonetic learning\"</a>. <i>Proceedings of the National Academy of Sciences</i>. <b>100</b> (15): 9096\u20139101. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/2003PNAS..100.9096K\">2003PNAS..100.9096K</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1073/pnas.1532872100\">10.1073/pnas.1532872100</a>. <a href=\"/wiki/PubMed_Central\" title=\"PubMed Central\">PMC</a>&#160;<span class=\"plainlinks\"><a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pmc/articles/PMC166444\">166444</a>&#8239;<img alt=\"Freely accessible\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/9px-Lock-green.svg.png\" title=\"Freely accessible\" width=\"9\" height=\"14\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/14px-Lock-green.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/6/65/Lock-green.svg/18px-Lock-green.svg.png 2x\" data-file-width=\"512\" data-file-height=\"813\" /></span>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/12861072\">12861072</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+National+Academy+of+Sciences&amp;rft.atitle=Foreign-language+experience+in+infancy%3A+Effects+of+short-term+exposure+and+social+interaction+on+phonetic+learning&amp;rft.volume=100&amp;rft.issue=15&amp;rft.pages=9096-9101&amp;rft.date=2003-07&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC166444&amp;rft_id=info%3Apmid%2F12861072&amp;rft_id=info%3Adoi%2F10.1073%2Fpnas.1532872100&amp;rft_id=info%3Abibcode%2F2003PNAS..100.9096K&amp;rft.aulast=Kuhl&amp;rft.aufirst=Patricia+K.&amp;rft.au=Feng-Ming+Tsao&amp;rft.au=Huei-Mei+Liu&amp;rft_id=%2F%2Fwww.ncbi.nlm.nih.gov%2Fpmc%2Farticles%2FPMC166444&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-32\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-32\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Iverson, P., Kuhl, P.K., Akahane-Yamada, R., Diesh, E., Thokura, Y., Kettermann, A., Siebert, C., (2003). \"A perceptual interference account of acquisition difficulties for non-native phonemes\". <i>Cognition</i>. <b>89</b> (1): B47\u2013B57. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/S0010-0277%2802%2900198-1\">10.1016/S0010-0277(02)00198-1</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognition&amp;rft.atitle=A+perceptual+interference+account+of+acquisition+difficulties+for+non-native+phonemes&amp;rft.volume=89&amp;rft.issue=1&amp;rft.pages=B47-B57&amp;rft.date=2003&amp;rft_id=info%3Adoi%2F10.1016%2FS0010-0277%2802%2900198-1&amp;rft.au=Iverson%2C+P.%2C+Kuhl%2C+P.K.%2C+Akahane-Yamada%2C+R.%2C+Diesh%2C+E.%2C+Thokura%2C+Y.%2C+Kettermann%2C+A.%2C+Siebert%2C+C.%2C&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-33\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-33\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation encyclopaedia\">Best, C. T. (1995). \"A direct realist view of cross-language speech perception: New Directions in Research and Theory\".  In Winifred Strange. <i>Speech perception and linguistic experience: Theoretical and methodological issues</i>. Baltimore: York Press. pp.&#160;171\u2013204.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=A+direct+realist+view+of+cross-language+speech+perception%3A+New+Directions+in+Research+and+Theory&amp;rft.btitle=Speech+perception+and+linguistic+experience%3A+Theoretical+and+methodological+issues&amp;rft.place=Baltimore&amp;rft.pages=171-204&amp;rft.pub=York+Press&amp;rft.date=1995&amp;rft.au=Best%2C+C.+T.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-34\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-34\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation encyclopaedia\">Flege, J., (1995). \"Second language speech learning: Theory, findings and problems\".  In Winifred Strange. <i>Speech perception and linguistic experience: Theoretical and methodological issues</i>. Baltimore: York Press. pp.&#160;233\u2013277.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Second+language+speech+learning%3A+Theory%2C+findings+and+problems&amp;rft.btitle=Speech+perception+and+linguistic+experience%3A+Theoretical+and+methodological+issues&amp;rft.place=Baltimore&amp;rft.pages=233-277&amp;rft.pub=York+Press&amp;rft.date=1995&amp;rft.au=Flege%2C+J.%2C&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-35\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-35\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Uhler; Yoshinaga-Itano; Gabbard; Rothpletz; Jenkins (March 2011). \"infant speech perception in young cochlear implant users\". <i>Journal of the American Academy of Audiology</i>. <b>22</b> (3): 129\u2013142. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.3766/jaaa.22.3.2\">10.3766/jaaa.22.3.2</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+American+Academy+of+Audiology&amp;rft.atitle=infant+speech+perception+in+young+cochlear+implant+users&amp;rft.volume=22&amp;rft.issue=3&amp;rft.pages=129-142&amp;rft.date=2011-03&amp;rft_id=info%3Adoi%2F10.3766%2Fjaaa.22.3.2&amp;rft.au=Uhler&amp;rft.au=Yoshinaga-Itano&amp;rft.au=Gabbard&amp;rft.au=Rothpletz&amp;rft.au=Jenkins&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-cse2001-36\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-cse2001_36-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-cse2001_36-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Cs\u00e9pe, V.; Osman-Sagi, J.; Molnar, M.; Gosy, M. (2001). \"Impaired speech perception in aphasic patients: event-related potential and neuropsychological assessment\". <i>Neuropsychologia</i>. <b>39</b> (11): 1194\u20131208. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/S0028-3932%2801%2900052-5\">10.1016/S0028-3932(01)00052-5</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/11527557\">11527557</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Neuropsychologia&amp;rft.atitle=Impaired+speech+perception+in+aphasic+patients%3A+event-related+potential+and+neuropsychological+assessment&amp;rft.volume=39&amp;rft.issue=11&amp;rft.pages=1194-1208&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1016%2FS0028-3932%2801%2900052-5&amp;rft_id=info%3Apmid%2F11527557&amp;rft.au=Cs%C3%A9pe%2C+V.&amp;rft.au=Osman-Sagi%2C+J.&amp;rft.au=Molnar%2C+M.&amp;rft.au=Gosy%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-loi1998-37\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-loi1998_37-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-loi1998_37-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Loizou, P. (1998). \"Introduction to cochlear implants\". <i>IEEE Signal Processing Magazine</i>. <b>39</b> (11): 101\u2013130.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Signal+Processing+Magazine&amp;rft.atitle=Introduction+to+cochlear+implants&amp;rft.volume=39&amp;rft.issue=11&amp;rft.pages=101-130&amp;rft.date=1998&amp;rft.au=Loizou%2C+P.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Deutsch,_Diana-38\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Deutsch,_Diana_38-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Deutsch,_Diana_38-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Deutsch,_Diana_38-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Deutsch,_Diana_38-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Deutsch, Diana; Henthorn, Trevor; Dolson, Mark (Spring 2004). <a rel=\"nofollow\" class=\"external text\" href=\"http://deutsch.ucsd.edu/pdf/MP-2004-21_357-372.pdf\">\"Speech patterns heard early in life influence later perception of the tritone paradox\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Music Perception</i>. <b>21</b> (3): 357\u201372. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1525/mp.2004.21.3.357\">10.1525/mp.2004.21.3.357</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">29 April</span> 2014</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Music+Perception&amp;rft.atitle=Speech+patterns+heard+early+in+life+influence+later+perception+of+the+tritone+paradox&amp;rft.ssn=spring&amp;rft.volume=21&amp;rft.issue=3&amp;rft.pages=357-72&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1525%2Fmp.2004.21.3.357&amp;rft.aulast=Deutsch&amp;rft.aufirst=Diana&amp;rft.au=Henthorn%2C+Trevor&amp;rft.au=Dolson%2C+Mark&amp;rft_id=http%3A%2F%2Fdeutsch.ucsd.edu%2Fpdf%2FMP-2004-21_357-372.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Portuguese-39\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Portuguese_39-0\">^</a></b></span> <span class=\"reference-text\">Marques, C et al. (2007). Musicians detect pitch violation in foreign language better than nonmusicians: Behavioral and electrophysiological evidence. \"Journal of Cognitive Neuroscience, 19\", 1453-1463.</span>\n</li>\n<li id=\"cite_note-:0-40\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-:0_40-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-:0_40-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-:0_40-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">O'Callaghan, Casey (2010). <a rel=\"nofollow\" class=\"external text\" href=\"http://onlinelibrary.wiley.com/doi/10.1111/phis.2010.20.issue-1/issuetoc\">\"Experiencing Speech\"</a>. <i>Philosophical Issue, Philosophy of Mind</i>. <b>20</b>: 305\u2013327. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1111/j.1533-6077.2010.00186.x\">10.1111/j.1533-6077.2010.00186.x</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Philosophical+Issue%2C+Philosophy+of+Mind&amp;rft.atitle=Experiencing+Speech&amp;rft.volume=20&amp;rft.pages=305-327&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1533-6077.2010.00186.x&amp;rft.aulast=O%27Callaghan&amp;rft.aufirst=Casey&amp;rft_id=http%3A%2F%2Fonlinelibrary.wiley.com%2Fdoi%2F10.1111%2Fphis.2010.20.issue-1%2Fissuetoc&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-mcc1986-41\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-mcc1986_41-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">McClelland, J.L. &amp; Elman, J.L. (1986). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20070421153000/http://www.cnbc.cmu.edu/~jlm/papers/McClellandElman86.pdf\">\"The TRACE model of speech perception\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Cognitive Psychology</i>. <b>18</b> (1): 1\u201386. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/0010-0285%2886%2990015-0\">10.1016/0010-0285(86)90015-0</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/3753912\">3753912</a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cnbc.cmu.edu/~jlm/papers/McClellandElman86.pdf\">the original</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span> on 2007-04-21<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-19</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Psychology&amp;rft.atitle=The+TRACE+model+of+speech+perception&amp;rft.volume=18&amp;rft.issue=1&amp;rft.pages=1-86&amp;rft.date=1986&amp;rft_id=info%3Adoi%2F10.1016%2F0010-0285%2886%2990015-0&amp;rft_id=info%3Apmid%2F3753912&amp;rft.au=McClelland%2C+J.L.&amp;rft.au=Elman%2C+J.L.&amp;rft_id=http%3A%2F%2Fwww.cnbc.cmu.edu%2F~jlm%2Fpapers%2FMcClellandElman86.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-kaz-42\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-kaz_42-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation conference\">Kazanina, N., Phillips, C., Idsardi, W. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://aix1.uottawa.ca/~nkazanin/Papers/kazanina-phillips-idsardi_PNAS_2006_reprint.pdf\">\"The influence of meaning on the perception of speech sounds\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>PNAS</i>. <b>30</b>. pp.&#160;11381\u201311386<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-19</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=The+influence+of+meaning+on+the+perception+of+speech+sounds&amp;rft.btitle=PNAS&amp;rft.pages=11381-11386&amp;rft.date=2006&amp;rft.au=Kazanina%2C+N.%2C+Phillips%2C+C.%2C+Idsardi%2C+W.&amp;rft_id=http%3A%2F%2Faix1.uottawa.ca%2F~nkazanin%2FPapers%2Fkazanina-phillips-idsardi_PNAS_2006_reprint.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span><sup class=\"noprint Inline-Template\"><span style=\"white-space: nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Link_rot\" title=\"Wikipedia:Link rot\"><span title=\"&#160;Dead link since September 2018\">permanent dead link</span></a></i>&#93;</span></sup></span>\n</li>\n<li id=\"cite_note-43\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-43\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Gocken, J.M. &amp; Fox R.A. (2001). \"Neurological Evidence in Support of a Specialized Phonetic Processing Module\". <i>Brain and Language</i>. <b>78</b> (2): 241\u2013253. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1006/brln.2001.2467\">10.1006/brln.2001.2467</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/11500073\">11500073</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Brain+and+Language&amp;rft.atitle=Neurological+Evidence+in+Support+of+a+Specialized+Phonetic+Processing+Module&amp;rft.volume=78&amp;rft.issue=2&amp;rft.pages=241-253&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1006%2Fbrln.2001.2467&amp;rft_id=info%3Apmid%2F11500073&amp;rft.au=Gocken%2C+J.M.&amp;rft.au=Fox+R.A.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-44\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-44\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Dehaene-Lambertz, G., Pallier, C., Serniclaes, W., Sprenger-Charolles, L., Jobert, A., &amp; Dehaene, S. (2005). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.pallier.org/papers/Dehaene-LambertzPallier_Sinewaves_Neuroimgage_2004.pdf\">\"Neural correlates of switching from auditory to speech perception\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>NeuroImage</i>. <b>24</b> (1): 21\u201333. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/j.neuroimage.2004.09.039\">10.1016/j.neuroimage.2004.09.039</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/15588593\">15588593</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-07-04</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=NeuroImage&amp;rft.atitle=Neural+correlates+of+switching+from+auditory+to+speech+perception&amp;rft.volume=24&amp;rft.issue=1&amp;rft.pages=21-33&amp;rft.date=2005&amp;rft_id=info%3Adoi%2F10.1016%2Fj.neuroimage.2004.09.039&amp;rft_id=info%3Apmid%2F15588593&amp;rft.au=Dehaene-Lambertz%2C+G.%2C+Pallier%2C+C.%2C+Serniclaes%2C+W.%2C+Sprenger-Charolles%2C+L.%2C+Jobert%2C+A.%2C+%26+Dehaene%2C+S.&amp;rft_id=http%3A%2F%2Fwww.pallier.org%2Fpapers%2FDehaene-LambertzPallier_Sinewaves_Neuroimgage_2004.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-45\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-45\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">N\u00e4\u00e4t\u00e4nen, R. (2001). \"The perception of speech sounds by the human brain as reflected by the mismatch negativity (MMN) and its magnetic equivalent (MMNm)\". <i>Psychophysiology</i>. <b>38</b> (1): 1\u201321. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1111/1469-8986.3810001\">10.1111/1469-8986.3810001</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/11321610\">11321610</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychophysiology&amp;rft.atitle=The+perception+of+speech+sounds+by+the+human+brain+as+reflected+by+the+mismatch+negativity+%28MMN%29+and+its+magnetic+equivalent+%28MMNm%29&amp;rft.volume=38&amp;rft.issue=1&amp;rft.pages=1-21&amp;rft.date=2001&amp;rft_id=info%3Adoi%2F10.1111%2F1469-8986.3810001&amp;rft_id=info%3Apmid%2F11321610&amp;rft.au=N%C3%A4%C3%A4t%C3%A4nen%2C+R.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Ingram-46\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Ingram_46-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Ingram_46-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-Ingram_46-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-Ingram_46-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Ingram, John. C.L. (2007). <i>Neurolinguistics: An Introduction to Spoken Language Processing and its Disorders</i>. Cambridge: Cambridge University Press. pp.&#160;113\u2013127.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Neurolinguistics%3A+An+Introduction+to+Spoken+Language+Processing+and+its+Disorders&amp;rft.place=Cambridge&amp;rft.pages=113-127&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2007&amp;rft.aulast=Ingram&amp;rft.aufirst=John.+C.L.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Parker-47\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Parker_47-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Parker_47-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Parker, Ellen M.; R.L. Diehl; K.R. Kluender (1986). \"Trading Relations in Speech and Non-speech\". <i>Attention, Perception, &amp; Psychophysics</i>. <b>39</b> (2): 129\u2013142. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.3758/bf03211495\">10.3758/bf03211495</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Attention%2C+Perception%2C+%26+Psychophysics&amp;rft.atitle=Trading+Relations+in+Speech+and+Non-speech&amp;rft.volume=39&amp;rft.issue=2&amp;rft.pages=129-142&amp;rft.date=1986&amp;rft_id=info%3Adoi%2F10.3758%2Fbf03211495&amp;rft.aulast=Parker&amp;rft.aufirst=Ellen+M.&amp;rft.au=R.L.+Diehl&amp;rft.au=K.R.+Kluender&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-lib57-48\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-lib57_48-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Liberman, A.M., Harris, K.S., Hoffman, H.S., Griffith, B.C. (1957). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.haskins.yale.edu/Reprints/HL0022.pdf\">\"The discrimination of speech sounds within and across phoneme boundaries\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>Journal of Experimental Psychology</i>. <b>54</b> (5): 358\u2013368. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1037/h0044417\">10.1037/h0044417</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/13481283\">13481283</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-18</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Experimental+Psychology&amp;rft.atitle=The+discrimination+of+speech+sounds+within+and+across+phoneme+boundaries&amp;rft.volume=54&amp;rft.issue=5&amp;rft.pages=358-368&amp;rft.date=1957&amp;rft_id=info%3Adoi%2F10.1037%2Fh0044417&amp;rft_id=info%3Apmid%2F13481283&amp;rft.au=Liberman%2C+A.M.%2C+Harris%2C+K.S.%2C+Hoffman%2C+H.S.%2C+Griffith%2C+B.C.&amp;rft_id=http%3A%2F%2Fwww.haskins.yale.edu%2FReprints%2FHL0022.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-lib67-49\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-lib67_49-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Liberman, A.M., Cooper, F.S., Shankweiler, D.P., &amp; Studdert-Kennedy, M. (1967). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.haskins.yale.edu/Reprints/HL0069.pdf\">\"Perception of the speech code\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>Psychological Review</i>. <b>74</b> (6): 431\u2013461. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1037/h0020279\">10.1037/h0020279</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/4170865\">4170865</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-19</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychological+Review&amp;rft.atitle=Perception+of+the+speech+code&amp;rft.volume=74&amp;rft.issue=6&amp;rft.pages=431-461&amp;rft.date=1967&amp;rft_id=info%3Adoi%2F10.1037%2Fh0020279&amp;rft_id=info%3Apmid%2F4170865&amp;rft.au=Liberman%2C+A.M.%2C+Cooper%2C+F.S.%2C+Shankweiler%2C+D.P.%2C+%26+Studdert-Kennedy%2C+M.&amp;rft_id=http%3A%2F%2Fwww.haskins.yale.edu%2FReprints%2FHL0069.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-lib70-50\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-lib70_50-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Liberman, A.M. (1970). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.haskins.yale.edu/Reprints/HL0099.pdf\">\"The grammars of speech and language\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>Cognitive Psychology</i>. <b>1</b> (4): 301\u2013323. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/0010-0285%2870%2990018-6\">10.1016/0010-0285(70)90018-6</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-07-19</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Psychology&amp;rft.atitle=The+grammars+of+speech+and+language&amp;rft.volume=1&amp;rft.issue=4&amp;rft.pages=301-323&amp;rft.date=1970&amp;rft_id=info%3Adoi%2F10.1016%2F0010-0285%2870%2990018-6&amp;rft.au=Liberman%2C+A.M.&amp;rft_id=http%3A%2F%2Fwww.haskins.yale.edu%2FReprints%2FHL0099.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-lib85-51\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-lib85_51-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-lib85_51-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Liberman, A.M. &amp; Mattingly, I.G. (1985). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.haskins.yale.edu/Reprints/HL0519.pdf\">\"The motor theory of speech perception revised\"</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span>. <i>Cognition</i>. <b>21</b> (1): 1\u201336. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/0010-0277%2885%2990021-6\">10.1016/0010-0277(85)90021-6</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/4075760\">4075760</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-07-19</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognition&amp;rft.atitle=The+motor+theory+of+speech+perception+revised&amp;rft.volume=21&amp;rft.issue=1&amp;rft.pages=1-36&amp;rft.date=1985&amp;rft_id=info%3Adoi%2F10.1016%2F0010-0277%2885%2990021-6&amp;rft_id=info%3Apmid%2F4075760&amp;rft.au=Liberman%2C+A.M.&amp;rft.au=Mattingly%2C+I.G.&amp;rft_id=http%3A%2F%2Fwww.haskins.yale.edu%2FReprints%2FHL0519.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-hay-52\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-hay_52-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-hay_52-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-hay_52-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">Hayward, Katrina (2000). <i>Experimental Phonetics: An Introduction</i>. Harlow: Longman.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Experimental+Phonetics%3A+An+Introduction&amp;rft.place=Harlow&amp;rft.pub=Longman&amp;rft.date=2000&amp;rft.aulast=Hayward&amp;rft.aufirst=Katrina&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-53\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-53\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Randy L. Diehl; Andrew J. Lotto; Lori L. Holt (2004). \"Speech perception\". <i>Annual Review of Psychology</i>. <b>55</b> (1): 149\u2013179. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1146/annurev.psych.55.090902.142028\">10.1146/annurev.psych.55.090902.142028</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/14744213\">14744213</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Annual+Review+of+Psychology&amp;rft.atitle=Speech+perception&amp;rft.volume=55&amp;rft.issue=1&amp;rft.pages=149-179&amp;rft.date=2004&amp;rft_id=info%3Adoi%2F10.1146%2Fannurev.psych.55.090902.142028&amp;rft_id=info%3Apmid%2F14744213&amp;rft.au=Randy+L.+Diehl&amp;rft.au=Andrew+J.+Lotto&amp;rft.au=Lori+L.+Holt&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-54\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-54\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Massaro, D.W. (1989). \"Testing between the TRACE Model and the Fuzzy Logical Model of Speech perception\". <i>Cognitive Psychology</i>. <b>21</b> (3): 398\u2013421. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/0010-0285%2889%2990014-5\">10.1016/0010-0285(89)90014-5</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/2758786\">2758786</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Cognitive+Psychology&amp;rft.atitle=Testing+between+the+TRACE+Model+and+the+Fuzzy+Logical+Model+of+Speech+perception&amp;rft.volume=21&amp;rft.issue=3&amp;rft.pages=398-421&amp;rft.date=1989&amp;rft_id=info%3Adoi%2F10.1016%2F0010-0285%2889%2990014-5&amp;rft_id=info%3Apmid%2F2758786&amp;rft.au=Massaro%2C+D.W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-oden1978-55\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-oden1978_55-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Oden, G.C., Massaro, D.W. (1978). \"Integration of featural information in speech perception\". <i>Psychological Review</i>. <b>85</b> (3): 172\u2013191. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1037/0033-295X.85.3.172\">10.1037/0033-295X.85.3.172</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/663005\">663005</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Psychological+Review&amp;rft.atitle=Integration+of+featural+information+in+speech+perception&amp;rft.volume=85&amp;rft.issue=3&amp;rft.pages=172-191&amp;rft.date=1978&amp;rft_id=info%3Adoi%2F10.1037%2F0033-295X.85.3.172&amp;rft_id=info%3Apmid%2F663005&amp;rft.au=Oden%2C+G.C.%2C+Massaro%2C+D.W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><span class=\"citation-comment\" style=\"display:none; color:#33aa33; margin-left:0.3em\">CS1 maint: Multiple names: authors list (<a href=\"/wiki/Category:CS1_maint:_Multiple_names:_authors_list\" title=\"Category:CS1 maint: Multiple names: authors list\">link</a>) </span></span>\n</li>\n<li id=\"cite_note-56\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-56\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Stevens, K.N. (2002). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20070609151908/http://linguistics.berkeley.edu/~kjohnson/ling210/stevens2002.pdf\">\"Toward a model of lexical access based on acoustic landmarks and distinctive features\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Journal of the Acoustical Society of America</i>. <b>111</b> (4): 1872\u20131891. <a href=\"/wiki/Bibcode\" title=\"Bibcode\">Bibcode</a>:<a rel=\"nofollow\" class=\"external text\" href=\"http://adsabs.harvard.edu/abs/2002ASAJ..111.1872S\">2002ASAJ..111.1872S</a>. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1121/1.1458026\">10.1121/1.1458026</a>. <a href=\"/wiki/PubMed_Identifier\" class=\"mw-redirect\" title=\"PubMed Identifier\">PMID</a>&#160;<a rel=\"nofollow\" class=\"external text\" href=\"//www.ncbi.nlm.nih.gov/pubmed/12002871\">12002871</a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://linguistics.berkeley.edu/~kjohnson/ling210/stevens2002.pdf\">the original</a> <span style=\"font-size:85%;\">(<a href=\"/wiki/PDF\" title=\"PDF\">PDF</a>)</span> on 2007-06-09<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2007-05-17</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+the+Acoustical+Society+of+America&amp;rft.atitle=Toward+a+model+of+lexical+access+based+on+acoustic+landmarks+and+distinctive+features&amp;rft.volume=111&amp;rft.issue=4&amp;rft.pages=1872-1891&amp;rft.date=2002&amp;rft_id=info%3Apmid%2F12002871&amp;rft_id=info%3Adoi%2F10.1121%2F1.1458026&amp;rft_id=info%3Abibcode%2F2002ASAJ..111.1872S&amp;rft.au=Stevens%2C+K.N.&amp;rft_id=http%3A%2F%2Flinguistics.berkeley.edu%2F~kjohnson%2Fling210%2Fstevens2002.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3ASpeech+perception\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n</ol></div>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Speech_perception&amp;action=edit&amp;section=46\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"http://publishing.royalsociety.org/perception-speech\">Dedicated issue of <i>Philosophical Transactions B</i> on the Perception of Speech. Some articles are freely available.</a><sup class=\"noprint Inline-Template\"><span style=\"white-space: nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Link_rot\" title=\"Wikipedia:Link rot\"><span title=\"&#160;Dead link since May 2018\">permanent dead link</span></a></i>&#93;</span></sup></li></ul>\n<div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Mental_processes\" style=\"padding:3px\"><table class=\"nowraplinks collapsible autocollapse navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\" style=\"background:#efefef;\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Mental_processes\" title=\"Template:Mental processes\"><abbr title=\"View this template\" style=\"background:#efefef;;;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Mental_processes\" title=\"Template talk:Mental processes\"><abbr title=\"Discuss this template\" style=\"background:#efefef;;;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Template:Mental_processes&amp;action=edit\"><abbr title=\"Edit this template\" style=\"background:#efefef;;;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">e</abbr></a></li></ul></div><div id=\"Mental_processes\" style=\"font-size:114%;margin:0 4em\"><a href=\"/wiki/Mental_process\" title=\"Mental process\">Mental processes</a></div></th></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"background:#efefef;;width:1%\"><a href=\"/wiki/Cognition\" title=\"Cognition\">Cognition</a></th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Awareness\" title=\"Awareness\">Awareness</a></li>\n<li><a href=\"/wiki/Cognitive_dissonance\" title=\"Cognitive dissonance\">Cognitive dissonance</a></li>\n<li><a href=\"/wiki/Understanding\" title=\"Understanding\">Comprehension</a></li>\n<li><a href=\"/wiki/Consciousness\" title=\"Consciousness\">Consciousness</a></li>\n<li><a href=\"/wiki/Imagination\" title=\"Imagination\">Imagination</a></li>\n<li><a href=\"/wiki/Intuition\" title=\"Intuition\">Intuition</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"background:#efefef;;width:1%\"><a href=\"/wiki/Perception\" title=\"Perception\">Perception</a></th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Amodal_perception\" title=\"Amodal perception\">Amodal</a></li>\n<li><a href=\"/wiki/Haptic_perception\" title=\"Haptic perception\">Haptic <span style=\"font-size:85%;\">(touch)</span></a></li>\n<li><a href=\"/wiki/Psychoacoustics\" title=\"Psychoacoustics\">Sound</a>\n<ul><li><a href=\"/wiki/Pitch_(music)\" title=\"Pitch (music)\">pitch</a></li>\n<li><a href=\"/wiki/Harmonic\" title=\"Harmonic\">harmonics</a></li>\n<li><a class=\"mw-selflink selflink\">speech</a></li></ul></li>\n<li><a href=\"/wiki/Social_perception\" title=\"Social perception\">Social</a></li>\n<li><a href=\"/wiki/Aesthetic_interpretation\" title=\"Aesthetic interpretation\">Perception as interpretation</a></li>\n<li><a href=\"/wiki/Visual_perception\" title=\"Visual perception\">Visual</a></li>\n<li><a href=\"/wiki/Color_vision\" title=\"Color vision\">Color</a>\n<ul><li><a href=\"/wiki/RGB_color_model\" title=\"RGB color model\">RGB model</a></li></ul></li>\n<li><a href=\"/wiki/Peripheral_vision\" title=\"Peripheral vision\">Peripheral</a></li>\n<li><a href=\"/wiki/Depth_perception\" title=\"Depth perception\">Depth</a></li>\n<li><a href=\"/wiki/Form_perception\" title=\"Form perception\">Form</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"background:#efefef;;width:1%\"><a href=\"/wiki/Memory\" title=\"Memory\">Memory</a></th><td class=\"navbox-list navbox-odd hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Encoding_(memory)\" title=\"Encoding (memory)\">Encoding</a></li>\n<li><a href=\"/wiki/Storage_(memory)\" title=\"Storage (memory)\">Storage</a></li>\n<li><a href=\"/wiki/Recall_(memory)\" title=\"Recall (memory)\">Recall</a></li>\n<li><a href=\"/wiki/Memory_consolidation\" title=\"Memory consolidation\">Consolidation</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"background:#efefef;;width:1%\">Other</th><td class=\"navbox-list navbox-even hlist\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Attention\" title=\"Attention\">Attention</a></li>\n<li><a href=\"/wiki/Institute_of_Higher_Nervous_Activity\" title=\"Institute of Higher Nervous Activity\">Higher nervous activity</a></li>\n<li><a href=\"/wiki/Intention\" title=\"Intention\">Intention</a></li>\n<li><a href=\"/wiki/Learning\" title=\"Learning\">Learning</a></li>\n<li><a href=\"/wiki/Fatigue_(medical)#Mental_fatigue\" class=\"mw-redirect\" title=\"Fatigue (medical)\">Mental fatigue</a></li>\n<li><a href=\"/wiki/Set_(psychology)\" title=\"Set (psychology)\">Mental set</a></li>\n<li><a href=\"/wiki/Thought\" title=\"Thought\">Thinking</a></li>\n<li><a href=\"/wiki/Volition_(psychology)\" title=\"Volition (psychology)\">Volition</a></li></ul>\n</div></td></tr></tbody></table></div>\n<div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Authority_control_frameless_&amp;#124;text-top_&amp;#124;10px_&amp;#124;alt=Edit_this_at_Wikidata_&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q643696&amp;#124;Edit_this_at_Wikidata\" style=\"padding:3px\"><table class=\"nowraplinks hlist navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th id=\"Authority_control_frameless_&amp;#124;text-top_&amp;#124;10px_&amp;#124;alt=Edit_this_at_Wikidata_&amp;#124;link=https&amp;#58;//www.wikidata.org/wiki/Q643696&amp;#124;Edit_this_at_Wikidata\" scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Help:Authority_control\" title=\"Help:Authority control\">Authority control</a> <a href=\"https://www.wikidata.org/wiki/Q643696\" title=\"Edit this at Wikidata\"><img alt=\"Edit this at Wikidata\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/73/Blue_pencil.svg/10px-Blue_pencil.svg.png\" width=\"10\" height=\"10\" style=\"vertical-align: text-top\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/7/73/Blue_pencil.svg/15px-Blue_pencil.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/7/73/Blue_pencil.svg/20px-Blue_pencil.svg.png 2x\" data-file-width=\"600\" data-file-height=\"600\" /></a></th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><span class=\"nowrap\"><a href=\"/wiki/Integrated_Authority_File\" title=\"Integrated Authority File\">GND</a>: <span class=\"uid\"><a rel=\"nofollow\" class=\"external text\" href=\"https://d-nb.info/gnd/4077744-3\">4077744-3</a></span></span></li></ul>\n</div></td></tr></tbody></table></div>\n\n<!-- \nNewPP limit report\nParsed by mw2283\nCached time: 20180915173430\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.664 seconds\nReal time usage: 0.750 seconds\nPreprocessor visited node count: 3451/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 136573/2097152 bytes\nTemplate argument size: 1789/2097152 bytes\nHighest expansion depth: 12/40\nExpensive parser function count: 4/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 81487/5000000 bytes\nNumber of Wikibase entities loaded: 1/400\nLua time usage: 0.322/10.000 seconds\nLua memory usage: 4.78 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  583.666      1 -total\n 63.67%  371.615      1 Template:Reflist\n 32.76%  191.230     40 Template:Cite_journal\n  9.06%   52.879      6 Template:Cite_encyclopedia\n  8.15%   47.570      1 Template:Authority_control\n  7.94%   46.314      3 Template:Fix\n  7.61%   44.393      1 Template:Citation_needed\n  5.17%   30.183      6 Template:Category_handler\n  4.84%   28.235      1 Template:Anchor\n  2.95%   17.225      4 Template:Cite_book\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:5366050-0!canonical and timestamp 20180915173430 and revision id 859692764\n -->\n</div>"},"langlinks":[{"lang":"de","url":"https://de.wikipedia.org/wiki/Sprachverst%C3%A4ndnis","langname":"German","autonym":"Deutsch","*":"Sprachverst\u00e4ndnis"},{"lang":"fr","url":"https://fr.wikipedia.org/wiki/Perception_de_la_parole","langname":"French","autonym":"fran\u00e7ais","*":"Perception de la parole"},{"lang":"hu","url":"https://hu.wikipedia.org/wiki/Besz%C3%A9d%C3%A9rz%C3%A9kel%C3%A9s","langname":"Hungarian","autonym":"magyar","*":"Besz\u00e9d\u00e9rz\u00e9kel\u00e9s"}],"categories":[{"sortkey":"","hidden":"","*":"CS1_maint:_Multiple_names:_authors_list"},{"sortkey":"","hidden":"","*":"All_articles_with_dead_external_links"},{"sortkey":"","hidden":"","*":"Articles_with_dead_external_links_from_September_2018"},{"sortkey":"","hidden":"","*":"Articles_with_permanently_dead_external_links"},{"sortkey":"","hidden":"","*":"All_articles_with_unsourced_statements"},{"sortkey":"","hidden":"","*":"Articles_with_unsourced_statements_from_July_2010"},{"sortkey":"","hidden":"","*":"Articles_with_dead_external_links_from_May_2018"},{"sortkey":"","hidden":"","*":"Wikipedia_articles_with_GND_identifiers"},{"sortkey":"","*":"Developmental_psychology"},{"sortkey":"","*":"Phonetics"},{"sortkey":"","*":"Cognition"},{"sortkey":"","*":"Hearing"},{"sortkey":"","*":"Auditory_perception"},{"sortkey":"","*":"Speech"}],"links":[{"ns":14,"exists":"","*":"Category:Articles with unsourced statements from July 2010"},{"ns":14,"exists":"","*":"Category:Articles with dead external links from September 2018"},{"ns":14,"exists":"","*":"Category:Articles with dead external links from May 2018"},{"ns":14,"exists":"","*":"Category:Wikipedia articles with GND identifiers"},{"ns":14,"exists":"","*":"Category:CS1 maint: Multiple names: authors list"},{"ns":10,"exists":"","*":"Template:Mental processes"},{"ns":0,"exists":"","*":"Acoustic landmarks and distinctive features"},{"ns":0,"exists":"","*":"Aesthetic interpretation"},{"ns":0,"exists":"","*":"Agnosia"},{"ns":0,"exists":"","*":"Alvin Liberman"},{"ns":0,"exists":"","*":"Amodal perception"},{"ns":0,"exists":"","*":"Aphasia"},{"ns":0,"exists":"","*":"Arthur S. Abramson"},{"ns":0,"exists":"","*":"Aspiration (phonetics)"},{"ns":0,"exists":"","*":"Attention"},{"ns":0,"exists":"","*":"Auditory processing disorder"},{"ns":0,"exists":"","*":"Awareness"},{"ns":0,"exists":"","*":"Bark scale"},{"ns":0,"exists":"","*":"Bibcode"},{"ns":0,"exists":"","*":"Bilabial plosive"},{"ns":0,"exists":"","*":"Carol Fowler"},{"ns":0,"exists":"","*":"Categorical perception"},{"ns":0,"exists":"","*":"Coarticulation"},{"ns":0,"exists":"","*":"Cochlear implant"},{"ns":0,"exists":"","*":"Cognition"},{"ns":0,"exists":"","*":"Cognitive dissonance"},{"ns":0,"exists":"","*":"Cognitive neuroscience of music"},{"ns":0,"exists":"","*":"Cognitive psychology"},{"ns":0,"exists":"","*":"Color vision"},{"ns":0,"exists":"","*":"Consciousness"},{"ns":0,"exists":"","*":"Coronal consonant"},{"ns":0,"exists":"","*":"Critical period hypothesis"},{"ns":0,"exists":"","*":"Deep brain stimulation"},{"ns":0,"exists":"","*":"Depth perception"},{"ns":0,"exists":"","*":"Dichotic listening"},{"ns":0,"exists":"","*":"Digital object identifier"},{"ns":0,"exists":"","*":"Direct realism"},{"ns":0,"exists":"","*":"Discrimination test"},{"ns":0,"exists":"","*":"Distal stimulus"},{"ns":0,"exists":"","*":"Dominic W. Massaro"},{"ns":0,"exists":"","*":"Duplex perception"},{"ns":0,"exists":"","*":"Encoding (memory)"},{"ns":0,"exists":"","*":"Event-related potential"},{"ns":0,"exists":"","*":"Expressive aphasia"},{"ns":0,"exists":"","*":"Fatigue (medical)"},{"ns":0,"exists":"","*":"Foreign language"},{"ns":0,"exists":"","*":"Form perception"},{"ns":0,"exists":"","*":"Formant"},{"ns":0,"exists":"","*":"Formants"},{"ns":0,"exists":"","*":"Fuzzy logic"},{"ns":0,"exists":"","*":"Genie (feral child)"},{"ns":0,"exists":"","*":"Habituate"},{"ns":0,"exists":"","*":"Haptic perception"},{"ns":0,"exists":"","*":"Harmonic"},{"ns":0,"exists":"","*":"Haskins Laboratories"},{"ns":0,"exists":"","*":"Hearing (sense)"},{"ns":0,"exists":"","*":"Imagination"},{"ns":0,"exists":"","*":"Institute of Higher Nervous Activity"},{"ns":0,"exists":"","*":"Integrated Authority File"},{"ns":0,"exists":"","*":"Intention"},{"ns":0,"exists":"","*":"International Standard Book Number"},{"ns":0,"exists":"","*":"International Standard Serial Number"},{"ns":0,"exists":"","*":"Intuition"},{"ns":0,"exists":"","*":"Kenneth N. Stevens"},{"ns":0,"exists":"","*":"Language"},{"ns":0,"exists":"","*":"Language acquisition"},{"ns":0,"exists":"","*":"Learning"},{"ns":0,"exists":"","*":"Leigh Lisker"},{"ns":0,"exists":"","*":"Linguistics"},{"ns":0,"exists":"","*":"Liquid consonant"},{"ns":0,"exists":"","*":"Magnetoencephalography"},{"ns":0,"exists":"","*":"Manner of articulation"},{"ns":0,"exists":"","*":"McGurk effect"},{"ns":0,"exists":"","*":"Memory"},{"ns":0,"exists":"","*":"Memory consolidation"},{"ns":0,"exists":"","*":"Mental process"},{"ns":0,"exists":"","*":"Mismatch negativity"},{"ns":0,"exists":"","*":"Modularity of mind"},{"ns":0,"exists":"","*":"Morphology (linguistics)"},{"ns":0,"exists":"","*":"Motor theory of speech perception"},{"ns":0,"exists":"","*":"Multisensory integration"},{"ns":0,"exists":"","*":"Music psychology"},{"ns":0,"exists":"","*":"Near-infrared spectroscopy"},{"ns":0,"exists":"","*":"Near infrared spectroscopy"},{"ns":0,"exists":"","*":"Neurocomputational speech processing"},{"ns":0,"exists":"","*":"Nikolai Trubetzkoy"},{"ns":0,"exists":"","*":"Origin of speech"},{"ns":0,"exists":"","*":"PDF"},{"ns":0,"exists":"","*":"Perception"},{"ns":0,"exists":"","*":"Perception of English /r/ and /l/ by Japanese speakers"},{"ns":0,"exists":"","*":"Perceptual constancy"},{"ns":0,"exists":"","*":"Peripheral vision"},{"ns":0,"exists":"","*":"Phonagnosia"},{"ns":0,"exists":"","*":"Phonemes"},{"ns":0,"exists":"","*":"Phonemic restoration effect"},{"ns":0,"exists":"","*":"Phonetic"},{"ns":0,"exists":"","*":"Phonetics"},{"ns":0,"exists":"","*":"Phonology"},{"ns":0,"exists":"","*":"Pitch (music)"},{"ns":0,"exists":"","*":"Place of articulation"},{"ns":0,"exists":"","*":"Plosives"},{"ns":0,"exists":"","*":"Pre-voicing (phonetics)"},{"ns":0,"exists":"","*":"Progressive nonfluent aphasia"},{"ns":0,"exists":"","*":"Psychoacoustics"},{"ns":0,"exists":"","*":"Psychology"},{"ns":0,"exists":"","*":"PubMed Central"},{"ns":0,"exists":"","*":"PubMed Identifier"},{"ns":0,"exists":"","*":"RGB color model"},{"ns":0,"exists":"","*":"Recall (memory)"},{"ns":0,"exists":"","*":"Receptive aphasia"},{"ns":0,"exists":"","*":"Second language"},{"ns":0,"exists":"","*":"Second language acquisition"},{"ns":0,"exists":"","*":"Semantics"},{"ns":0,"exists":"","*":"Set (psychology)"},{"ns":0,"exists":"","*":"Social perception"},{"ns":0,"exists":"","*":"Speech"},{"ns":0,"exists":"","*":"Speech-Language Pathology"},{"ns":0,"exists":"","*":"Speech recognition"},{"ns":0,"exists":"","*":"Speech segmentation"},{"ns":0,"exists":"","*":"Statistical learning in language acquisition"},{"ns":0,"exists":"","*":"Storage (memory)"},{"ns":0,"exists":"","*":"Syllables"},{"ns":0,"exists":"","*":"Syntax"},{"ns":0,"exists":"","*":"Thai language"},{"ns":0,"exists":"","*":"Thought"},{"ns":0,"exists":"","*":"Tritone paradox"},{"ns":0,"exists":"","*":"Understanding"},{"ns":0,"exists":"","*":"Visual perception"},{"ns":0,"exists":"","*":"Voice onset time"},{"ns":0,"exists":"","*":"Volition (psychology)"},{"ns":0,"exists":"","*":"Words"},{"ns":0,"*":"Formant transition"},{"ns":0,"*":"Acoustic cues"},{"ns":0,"*":"Formant transitions"},{"ns":0,"*":"Speech agnosia"},{"ns":4,"exists":"","*":"Wikipedia:Citation needed"},{"ns":4,"exists":"","*":"Wikipedia:Link rot"},{"ns":11,"exists":"","*":"Template talk:Mental processes"},{"ns":12,"exists":"","*":"Help:Authority control"}],"templates":[{"ns":10,"exists":"","*":"Template:IPA"},{"ns":10,"exists":"","*":"Template:Anchor"},{"ns":10,"exists":"","*":"Template:Main"},{"ns":10,"exists":"","*":"Template:Citation needed"},{"ns":10,"exists":"","*":"Template:Fix"},{"ns":10,"exists":"","*":"Template:Category handler"},{"ns":10,"exists":"","*":"Template:Fix/category"},{"ns":10,"exists":"","*":"Template:Delink"},{"ns":10,"exists":"","*":"Template:See also"},{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Column-width"},{"ns":10,"exists":"","*":"Template:Cite encyclopedia"},{"ns":10,"exists":"","*":"Template:Cite journal"},{"ns":10,"exists":"","*":"Template:Cite book"},{"ns":10,"exists":"","*":"Template:Cite conference"},{"ns":10,"exists":"","*":"Template:Cite web"},{"ns":10,"exists":"","*":"Template:Dead link"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":10,"exists":"","*":"Template:Mental processes"},{"ns":10,"exists":"","*":"Template:Navbox"},{"ns":10,"exists":"","*":"Template:Small"},{"ns":10,"exists":"","*":"Template:Authority control"},{"ns":10,"exists":"","*":"Template:EditAtWikidata"},{"ns":828,"exists":"","*":"Module:Anchor"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:TableTools"},{"ns":828,"exists":"","*":"Module:Main"},{"ns":828,"exists":"","*":"Module:Hatnote"},{"ns":828,"exists":"","*":"Module:Hatnote list"},{"ns":828,"exists":"","*":"Module:Unsubst"},{"ns":828,"exists":"","*":"Module:Category handler"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Category handler/data"},{"ns":828,"exists":"","*":"Module:Category handler/config"},{"ns":828,"exists":"","*":"Module:Category handler/shared"},{"ns":828,"exists":"","*":"Module:Category handler/blacklist"},{"ns":828,"exists":"","*":"Module:Namespace detect/data"},{"ns":828,"exists":"","*":"Module:Namespace detect/config"},{"ns":828,"exists":"","*":"Module:Delink"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:Labelled list hatnote"},{"ns":828,"exists":"","*":"Module:Citation/CS1"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Configuration"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Whitelist"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Utilities"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Date validation"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Identifiers"},{"ns":828,"exists":"","*":"Module:Citation/CS1/COinS"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"},{"ns":828,"exists":"","*":"Module:Navbox"},{"ns":828,"exists":"","*":"Module:Navbar"},{"ns":828,"exists":"","*":"Module:Authority control"},{"ns":828,"exists":"","*":"Module:ResolveEntityId"},{"ns":828,"exists":"","*":"Module:EditAtWikidata"}],"images":["Lock-green.svg","Spectrograms_of_syllables_dee_dah_doo.png","Spectrogram_of_I_owe_you.png","Standard_and_normalized_vowel_space2.png","Categorization-and-discrimination-curves.png","Blue_pencil.svg"],"externallinks":["http://adsabs.harvard.edu/abs/1976ASAJ...59.1208K","//doi.org/10.1121/1.380986","//www.ncbi.nlm.nih.gov/pubmed/956516","http://www.haskins.yale.edu/Reprints/HL0016.pdf","http://adsabs.harvard.edu/abs/1957ASAJ...29..117L","//doi.org/10.1121/1.1908635","http://adsabs.harvard.edu/abs/2001ASAJ..109..748H","//doi.org/10.1121/1.1337959","//www.ncbi.nlm.nih.gov/pubmed/11248979","http://www.haskins.yale.edu/Reprints/HL0067.pdf","//www.ncbi.nlm.nih.gov/pubmed/6044530","http://adsabs.harvard.edu/abs/1995ASAJ...97.3099H","//doi.org/10.1121/1.411872","//www.ncbi.nlm.nih.gov/pubmed/7759650","https://web.archive.org/web/20140430034452/http://babytalk.iupui.edu/pdfs/HoustonJusczyk_2000.pdf","//doi.org/10.1037/0096-1523.26.5.1570","http://babytalk.iupui.edu/pdfs/HoustonJusczyk_2000.pdf","//doi.org/10.1515/LING.2010.027","http://adsabs.harvard.edu/abs/1986ASAJ...79.1086S","//doi.org/10.1121/1.393381","//www.ncbi.nlm.nih.gov/pubmed/3700864","http://corpus.linguistics.berkeley.edu/~kjohnson/papers/revised_chapter.pdf","http://adsabs.harvard.edu/abs/1995ASAJ...97..553I","//doi.org/10.1121/1.412280","//www.ncbi.nlm.nih.gov/pubmed/7860832","http://www.haskins.yale.edu/Reprints/HL0087.pdf","http://adsabs.harvard.edu/abs/1970Sci...167..392W","//doi.org/10.1126/science.167.3917.392","//www.ncbi.nlm.nih.gov/pubmed/5409744","https://web.archive.org/web/20130614064103/http://www.sfu.ca/~yuew/documents/jongman%20wang%20kim%20JSLHR%20proof.pdf","//doi.org/10.1044/1092-4388(2003/106)","//www.ncbi.nlm.nih.gov/pubmed/14700361","https://www.sfu.ca/~yuew/documents/jongman%20wang%20kim%20JSLHR%20proof.pdf","//doi.org/10.1111/j.1467-8721.2008.00553.x","//www.worldcat.org/issn/0963-7214","//doi.org/10.1038/nrn2113","//www.ncbi.nlm.nih.gov/pubmed/17431404","//doi.org/10.3109/02699206.2010.507297","//doi.org/10.3109/17549507.2010.497560","https://www.merriam-webster.com/dictionary/agnosia","http://www.tulane.edu/~h0Ward/BrLg/index.html","https://neuro.psychiatryonline.org/doi/10.1176/appi.neuropsych.14040073","//doi.org/10.1176/appi.neuropsych.14040073","//www.worldcat.org/issn/0895-0172","http://www.sciencedirect.com/science/article/pii/S0010945289800073","//doi.org/10.1093/neucas/5.5.394","//doi.org/10.1523/JNEUROSCI.1984-06.2007","//www.ncbi.nlm.nih.gov/pubmed/17215392","//www.ncbi.nlm.nih.gov/pmc/articles/PMC166444","http://adsabs.harvard.edu/abs/2003PNAS..100.9096K","//doi.org/10.1073/pnas.1532872100","//www.ncbi.nlm.nih.gov/pubmed/12861072","//doi.org/10.1016/S0010-0277(02)00198-1","//doi.org/10.3766/jaaa.22.3.2","//doi.org/10.1016/S0028-3932(01)00052-5","//www.ncbi.nlm.nih.gov/pubmed/11527557","http://deutsch.ucsd.edu/pdf/MP-2004-21_357-372.pdf","//doi.org/10.1525/mp.2004.21.3.357","http://onlinelibrary.wiley.com/doi/10.1111/phis.2010.20.issue-1/issuetoc","//doi.org/10.1111/j.1533-6077.2010.00186.x","https://web.archive.org/web/20070421153000/http://www.cnbc.cmu.edu/~jlm/papers/McClellandElman86.pdf","//doi.org/10.1016/0010-0285(86)90015-0","//www.ncbi.nlm.nih.gov/pubmed/3753912","http://www.cnbc.cmu.edu/~jlm/papers/McClellandElman86.pdf","http://aix1.uottawa.ca/~nkazanin/Papers/kazanina-phillips-idsardi_PNAS_2006_reprint.pdf","//doi.org/10.1006/brln.2001.2467","//www.ncbi.nlm.nih.gov/pubmed/11500073","http://www.pallier.org/papers/Dehaene-LambertzPallier_Sinewaves_Neuroimgage_2004.pdf","//doi.org/10.1016/j.neuroimage.2004.09.039","//www.ncbi.nlm.nih.gov/pubmed/15588593","//doi.org/10.1111/1469-8986.3810001","//www.ncbi.nlm.nih.gov/pubmed/11321610","//doi.org/10.3758/bf03211495","http://www.haskins.yale.edu/Reprints/HL0022.pdf","//doi.org/10.1037/h0044417","//www.ncbi.nlm.nih.gov/pubmed/13481283","http://www.haskins.yale.edu/Reprints/HL0069.pdf","//doi.org/10.1037/h0020279","//www.ncbi.nlm.nih.gov/pubmed/4170865","http://www.haskins.yale.edu/Reprints/HL0099.pdf","//doi.org/10.1016/0010-0285(70)90018-6","http://www.haskins.yale.edu/Reprints/HL0519.pdf","//doi.org/10.1016/0010-0277(85)90021-6","//www.ncbi.nlm.nih.gov/pubmed/4075760","//doi.org/10.1146/annurev.psych.55.090902.142028","//www.ncbi.nlm.nih.gov/pubmed/14744213","//doi.org/10.1016/0010-0285(89)90014-5","//www.ncbi.nlm.nih.gov/pubmed/2758786","//doi.org/10.1037/0033-295X.85.3.172","//www.ncbi.nlm.nih.gov/pubmed/663005","https://web.archive.org/web/20070609151908/http://linguistics.berkeley.edu/~kjohnson/ling210/stevens2002.pdf","http://adsabs.harvard.edu/abs/2002ASAJ..111.1872S","//doi.org/10.1121/1.1458026","//www.ncbi.nlm.nih.gov/pubmed/12002871","http://linguistics.berkeley.edu/~kjohnson/ling210/stevens2002.pdf","https://www.wikidata.org/wiki/Q643696","http://www.haskins.yale.edu/featured/demo-liskabram/index.html","http://publishing.royalsociety.org/perception-speech","https://d-nb.info/gnd/4077744-3"],"sections":[{"toclevel":1,"level":"2","line":"Basics","number":"1","index":"1","fromtitle":"Speech_perception","byteoffset":688,"anchor":"Basics"},{"toclevel":2,"level":"3","line":"Acoustic cues","number":"1.1","index":"2","fromtitle":"Speech_perception","byteoffset":1128,"anchor":"Acoustic_cues"},{"toclevel":2,"level":"3","line":"Linearity and the segmentation problem","number":"1.2","index":"3","fromtitle":"Speech_perception","byteoffset":5148,"anchor":"Linearity_and_the_segmentation_problem"},{"toclevel":2,"level":"3","line":"Lack of invariance","number":"1.3","index":"4","fromtitle":"Speech_perception","byteoffset":6554,"anchor":"Lack_of_invariance"},{"toclevel":3,"level":"4","line":"Context-induced variation","number":"1.3.1","index":"5","fromtitle":"Speech_perception","byteoffset":6885,"anchor":"Context-induced_variation"},{"toclevel":3,"level":"4","line":"Variation due to differing speech conditions","number":"1.3.2","index":"6","fromtitle":"Speech_perception","byteoffset":7927,"anchor":"Variation_due_to_differing_speech_conditions"},{"toclevel":3,"level":"4","line":"Variation due to different speaker identity","number":"1.3.3","index":"7","fromtitle":"Speech_perception","byteoffset":8536,"anchor":"Variation_due_to_different_speaker_identity"},{"toclevel":2,"level":"3","line":"Perceptual constancy and normalization","number":"1.4","index":"8","fromtitle":"Speech_perception","byteoffset":10485,"anchor":"Perceptual_constancy_and_normalization"},{"toclevel":2,"level":"3","line":"Categorical perception","number":"1.5","index":"9","fromtitle":"Speech_perception","byteoffset":13268,"anchor":"Categorical_perception"},{"toclevel":2,"level":"3","line":"Top-down influences","number":"1.6","index":"10","fromtitle":"Speech_perception","byteoffset":16690,"anchor":"Top-down_influences"},{"toclevel":2,"level":"3","line":"Acquired brain disabilities","number":"1.7","index":"11","fromtitle":"Speech_perception","byteoffset":19510,"anchor":"Acquired_brain_disabilities"},{"toclevel":3,"level":"4","line":"Aphasia","number":"1.7.1","index":"12","fromtitle":"Speech_perception","byteoffset":21945,"anchor":"Aphasia"},{"toclevel":3,"level":"4","line":"Parkinson's disease","number":"1.7.2","index":"13","fromtitle":"Speech_perception","byteoffset":23849,"anchor":"Parkinson's_disease"},{"toclevel":3,"level":"4","line":"Agnosia","number":"1.7.3","index":"14","fromtitle":"Speech_perception","byteoffset":24853,"anchor":"Agnosia"},{"toclevel":4,"level":"5","line":"Speech agnosia","number":"1.7.3.1","index":"15","fromtitle":"Speech_perception","byteoffset":25346,"anchor":"Speech_agnosia"},{"toclevel":4,"level":"5","line":"Phonagnosia","number":"1.7.3.2","index":"16","fromtitle":"Speech_perception","byteoffset":26213,"anchor":"Phonagnosia"},{"toclevel":2,"level":"3","line":"Treatments","number":"1.8","index":"17","fromtitle":"Speech_perception","byteoffset":27069,"anchor":"Treatments"},{"toclevel":3,"level":"4","line":"Aphasia","number":"1.8.1","index":"18","fromtitle":"Speech_perception","byteoffset":27087,"anchor":"Aphasia_2"},{"toclevel":3,"level":"4","line":"Parkinson's disease","number":"1.8.2","index":"19","fromtitle":"Speech_perception","byteoffset":27878,"anchor":"Parkinson's_disease_2"},{"toclevel":3,"level":"4","line":"Speech agnosia","number":"1.8.3","index":"20","fromtitle":"Speech_perception","byteoffset":28690,"anchor":"Speech_agnosia_2"},{"toclevel":3,"level":"4","line":"Phonagnosia","number":"1.8.4","index":"21","fromtitle":"Speech_perception","byteoffset":29214,"anchor":"Phonagnosia_2"},{"toclevel":1,"level":"2","line":"Research topics","number":"2","index":"22","fromtitle":"Speech_perception","byteoffset":30165,"anchor":"Research_topics"},{"toclevel":2,"level":"3","line":"Infant speech perception","number":"2.1","index":"23","fromtitle":"Speech_perception","byteoffset":30186,"anchor":"Infant_speech_perception"},{"toclevel":2,"level":"3","line":"Cross-language and second-language","number":"2.2","index":"24","fromtitle":"Speech_perception","byteoffset":34918,"anchor":"Cross-language_and_second-language"},{"toclevel":2,"level":"3","line":"In language or hearing impairment","number":"2.3","index":"25","fromtitle":"Speech_perception","byteoffset":37365,"anchor":"In_language_or_hearing_impairment"},{"toclevel":3,"level":"4","line":"Listeners with aphasia","number":"2.3.1","index":"26","fromtitle":"Speech_perception","byteoffset":37973,"anchor":"Listeners_with_aphasia"},{"toclevel":3,"level":"4","line":"Listeners with cochlear implants","number":"2.3.2","index":"27","fromtitle":"Speech_perception","byteoffset":39085,"anchor":"Listeners_with_cochlear_implants"},{"toclevel":2,"level":"3","line":"Noise","number":"2.4","index":"28","fromtitle":"Speech_perception","byteoffset":40672,"anchor":"Noise"},{"toclevel":2,"level":"3","line":"Music-language connection","number":"2.5","index":"29","fromtitle":"Speech_perception","byteoffset":41206,"anchor":"Music-language_connection"},{"toclevel":2,"level":"3","line":"Speech phenomenology","number":"2.6","index":"30","fromtitle":"Speech_perception","byteoffset":44054,"anchor":"Speech_phenomenology"},{"toclevel":3,"level":"4","line":"The experience of speech","number":"2.6.1","index":"31","fromtitle":"Speech_perception","byteoffset":44084,"anchor":"The_experience_of_speech"},{"toclevel":1,"level":"2","line":"Research methods","number":"3","index":"32","fromtitle":"Speech_perception","byteoffset":45554,"anchor":"Research_methods"},{"toclevel":2,"level":"3","line":"Behavioral methods","number":"3.1","index":"33","fromtitle":"Speech_perception","byteoffset":45740,"anchor":"Behavioral_methods"},{"toclevel":3,"level":"4","line":"Sinewave Speech","number":"3.1.1","index":"34","fromtitle":"Speech_perception","byteoffset":46148,"anchor":"Sinewave_Speech"},{"toclevel":2,"level":"3","line":"Computational methods","number":"3.2","index":"35","fromtitle":"Speech_perception","byteoffset":46704,"anchor":"Computational_methods"},{"toclevel":2,"level":"3","line":"Neurophysiological methods","number":"3.3","index":"36","fromtitle":"Speech_perception","byteoffset":47671,"anchor":"Neurophysiological_methods"},{"toclevel":1,"level":"2","line":"Theories","number":"4","index":"37","fromtitle":"Speech_perception","byteoffset":50977,"anchor":"Theories"},{"toclevel":2,"level":"3","line":"Speech mode hypothesis","number":"4.1","index":"38","fromtitle":"Speech_perception","byteoffset":51387,"anchor":"Speech_mode_hypothesis"},{"toclevel":2,"level":"3","line":"Motor theory","number":"4.2","index":"39","fromtitle":"Speech_perception","byteoffset":53161,"anchor":"Motor_theory"},{"toclevel":2,"level":"3","line":"Direct realist theory","number":"4.3","index":"40","fromtitle":"Speech_perception","byteoffset":57260,"anchor":"Direct_realist_theory"},{"toclevel":2,"level":"3","line":"Fuzzy-logical model","number":"4.4","index":"41","fromtitle":"Speech_perception","byteoffset":58555,"anchor":"Fuzzy-logical_model"},{"toclevel":2,"level":"3","line":"Acoustic landmarks and distinctive features","number":"4.5","index":"42","fromtitle":"Speech_perception","byteoffset":60299,"anchor":"Acoustic_landmarks_and_distinctive_features"},{"toclevel":2,"level":"3","line":"Exemplar theory","number":"4.6","index":"43","fromtitle":"Speech_perception","byteoffset":61855,"anchor":"Exemplar_theory"},{"toclevel":1,"level":"2","line":"See also","number":"5","index":"44","fromtitle":"Speech_perception","byteoffset":63394,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"References","number":"6","index":"45","fromtitle":"Speech_perception","byteoffset":63588,"anchor":"References"},{"toclevel":1,"level":"2","line":"External links","number":"7","index":"46","fromtitle":"Speech_perception","byteoffset":63621,"anchor":"External_links"}],"parsewarnings":[],"displaytitle":"Speech perception","iwlinks":[],"properties":[{"name":"wikibase_item","*":"Q643696"}]}}