{"parse":{"title":"N-gram","pageid":986182,"revid":835900923,"text":{"*":"<div class=\"mw-parser-output\"><div role=\"note\" class=\"hatnote navigation-not-searchable\">For applications in computational genomics, see <a href=\"/wiki/K-mer\" title=\"K-mer\">k-mer</a>. For Google phrase-usage graphs, see <a href=\"/wiki/Google_Ngram_Viewer\" title=\"Google Ngram Viewer\">Google Ngram Viewer</a>.</div>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Not to be confused with <a href=\"/wiki/Engram_(disambiguation)\" class=\"mw-redirect mw-disambig\" title=\"Engram (disambiguation)\">Engram (disambiguation)</a>.</div>\n<table class=\"plainlinks metadata ambox ambox-style ambox-More_footnotes\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/40px-Text_document_with_red_question_mark.svg.png\" width=\"40\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/60px-Text_document_with_red_question_mark.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/80px-Text_document_with_red_question_mark.svg.png 2x\" data-file-width=\"48\" data-file-height=\"48\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article includes a <a href=\"/wiki/Wikipedia:Citing_sources\" title=\"Wikipedia:Citing sources\">list of references</a>, but <b>its sources remain unclear</b> because it has <b>insufficient <a href=\"/wiki/Wikipedia:Citing_sources#Inline_citations\" title=\"Wikipedia:Citing sources\">inline citations</a></b>.<span class=\"hide-when-compact\"> Please help to <a href=\"/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check\" title=\"Wikipedia:WikiProject Fact and Reference Check\">improve</a> this article by <a href=\"/wiki/Wikipedia:When_to_cite\" title=\"Wikipedia:When to cite\">introducing</a> more precise citations.</span>  <small><i>(February 2011)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<p>In the fields of <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a> and <a href=\"/wiki/Probability\" title=\"Probability\">probability</a>, an <b><i>n</i>-gram</b> is a contiguous sequence of <i>n</i> items from a given <a href=\"/wiki/Sample_(statistics)\" title=\"Sample (statistics)\">sample</a> of text or speech. The items can be <a href=\"/wiki/Phoneme\" title=\"Phoneme\">phonemes</a>, <a href=\"/wiki/Syllable\" title=\"Syllable\">syllables</a>, <a href=\"/wiki/Letter_(alphabet)\" title=\"Letter (alphabet)\">letters</a>, <a href=\"/wiki/Word\" title=\"Word\">words</a> or <a href=\"/wiki/Base_pairs\" class=\"mw-redirect\" title=\"Base pairs\">base pairs</a> according to the application.  The <i>n</i>-grams typically are collected from a <a href=\"/wiki/Text_corpus\" title=\"Text corpus\">text</a> or <a href=\"/wiki/Speech_corpus\" title=\"Speech corpus\">speech corpus</a>. When the items are words, <span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span>-grams may also be called <i>shingles</i><sup class=\"noprint Inline-Template\" style=\"margin-left:0.1em; white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Please_clarify\" title=\"Wikipedia:Please clarify\"><span title=\"The text near this tag may need clarification or removal of jargon. (December 2017)\">clarification needed</span></a></i>&#93;</sup>.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup>\n</p><p>Using <a href=\"/wiki/Latin_numerical_prefixes\" class=\"mw-redirect\" title=\"Latin numerical prefixes\">Latin numerical prefixes</a>, an <i>n</i>-gram of size 1 is referred to as a \"unigram\"; size 2 is a \"<a href=\"/wiki/Bigram\" title=\"Bigram\">bigram</a>\" (or, less commonly, a \"digram\"); size 3 is a \"<a href=\"/wiki/Trigram\" title=\"Trigram\">trigram</a>\". <a href=\"/wiki/Cardinal_number_(linguistics)\" title=\"Cardinal number (linguistics)\">English cardinal numbers</a> are sometimes used, e.g., \"four-gram\", \"five-gram\", and so on.    In computational biology, a <a href=\"/wiki/Polymer\" title=\"Polymer\">polymer</a> or <a href=\"/wiki/Oligomer\" title=\"Oligomer\">oligomer</a> of a known size is called a <a href=\"/wiki/K-mer\" title=\"K-mer\"><i>k</i>-mer</a> instead of an <i>n</i>-gram, with specific names using <a href=\"/wiki/Greek_numerical_prefixes\" class=\"mw-redirect\" title=\"Greek numerical prefixes\">Greek numerical prefixes</a> such as \"monomer\", \"dimer\", \"trimer\", \"tetramer\", \"pentamer\", etc., or English cardinal numbers, \"one-mer\", \"two-mer\", \"three-mer\", etc.\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Applications\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Applications</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Examples\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Examples</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#n-gram_models\"><span class=\"tocnumber\">3</span> <span class=\"toctext\"><i>n</i>-gram models</span></a></li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Applications_and_considerations\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Applications and considerations</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"#Out-of-vocabulary_words\"><span class=\"tocnumber\">4.1</span> <span class=\"toctext\">Out-of-vocabulary words</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#n-grams_for_approximate_matching\"><span class=\"tocnumber\">5</span> <span class=\"toctext\"><i>n</i>-grams for approximate matching</span></a></li>\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#Other_applications\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Other applications</span></a></li>\n<li class=\"toclevel-1 tocsection-8\"><a href=\"#Bias-versus-variance_trade-off\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Bias-versus-variance trade-off</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-9\"><a href=\"#Smoothing_techniques\"><span class=\"tocnumber\">7.1</span> <span class=\"toctext\">Smoothing techniques</span></a></li>\n<li class=\"toclevel-2 tocsection-10\"><a href=\"#Skip-gram\"><span class=\"tocnumber\">7.2</span> <span class=\"toctext\">Skip-gram</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-11\"><a href=\"#Syntactic_n-grams\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">Syntactic <i>n</i>-grams</span></a></li>\n<li class=\"toclevel-1 tocsection-12\"><a href=\"#See_also\"><span class=\"tocnumber\">9</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-13\"><a href=\"#References\"><span class=\"tocnumber\">10</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-14\"><a href=\"#External_links\"><span class=\"tocnumber\">11</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Applications\">Applications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=1\" title=\"Edit section: Applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>An <b><i>n</i>-gram model</b> is a type of probabilistic <a href=\"/wiki/Language_model\" title=\"Language model\">language model</a> for predicting the next item in such a sequence in the form of a (<i>n</i>&#160;\u2212&#160;1)\u2013order <a href=\"/wiki/Markov_chain\" title=\"Markov chain\">Markov model</a>.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup> <i>n</i>-gram models are now widely used in <a href=\"/wiki/Probability\" title=\"Probability\">probability</a>, <a href=\"/wiki/Communication_theory\" title=\"Communication theory\">communication theory</a>, <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a> (for instance, statistical <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a>), <a href=\"/wiki/Computational_biology\" title=\"Computational biology\">computational biology</a> (for instance, biological <a href=\"/wiki/Sequence_analysis\" title=\"Sequence analysis\">sequence analysis</a>), and <a href=\"/wiki/Data_compression\" title=\"Data compression\">data compression</a>. Two benefits of <i>n</i>-gram models (and algorithms that use them) are simplicity and scalability \u2013 with larger <i>n</i>, a model can store more context with a well-understood <a href=\"/wiki/Space%E2%80%93time_tradeoff\" title=\"Space\u2013time tradeoff\">space\u2013time tradeoff</a>, enabling small experiments to scale up efficiently.\n</p>\n<h2><span class=\"mw-headline\" id=\"Examples\">Examples</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=2\" title=\"Edit section: Examples\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<table class=\"wikitable\" style=\"font-size:85%;\">\n<caption>Figure 1 <i>n</i>-gram examples from various disciplines\n</caption>\n<tbody><tr>\n<th>Field</th>\n<th>Unit</th>\n<th>Sample sequence</th>\n<th>1-gram sequence</th>\n<th>2-gram sequence</th>\n<th>3-gram sequence\n</th></tr>\n<tr>\n<th>Vernacular name</th>\n<th></th>\n<th></th>\n<th>unigram</th>\n<th>bigram</th>\n<th>trigram\n</th></tr>\n<tr>\n<th>Order of resulting <a href=\"/wiki/Markov_model\" title=\"Markov model\">Markov model</a></th>\n<th></th>\n<th></th>\n<th>0</th>\n<th>1</th>\n<th>2\n</th></tr>\n<tr>\n<td><a href=\"/wiki/Protein_sequencing\" title=\"Protein sequencing\">Protein sequencing</a></td>\n<td><a href=\"/wiki/Amino_acid\" title=\"Amino acid\">amino acid</a></td>\n<td>\u2026 Cys-Gly-Leu-Ser-Trp \u2026</td>\n<td>\u2026, Cys, Gly, Leu, Ser, Trp, \u2026</td>\n<td>\u2026, Cys-Gly, Gly-Leu, Leu-Ser, Ser-Trp, \u2026</td>\n<td>\u2026, Cys-Gly-Leu, Gly-Leu-Ser, Leu-Ser-Trp, \u2026\n</td></tr>\n<tr>\n<td><a href=\"/wiki/DNA_sequencing\" title=\"DNA sequencing\">DNA sequencing</a></td>\n<td><a href=\"/wiki/Base_pair\" title=\"Base pair\">base pair</a></td>\n<td>\u2026AGCTTCGA\u2026</td>\n<td>\u2026, A, G, C, T, T, C, G, A, \u2026</td>\n<td>\u2026, AG, GC, CT, TT, TC, CG, GA, \u2026</td>\n<td>\u2026, AGC, GCT, CTT, TTC, TCG, CGA, \u2026\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">Computational linguistics</a></td>\n<td><a href=\"/wiki/Character_(computing)\" title=\"Character (computing)\">character</a></td>\n<td>\u2026to_be_or_not_to_be\u2026</td>\n<td>\u2026, t, o, _, b, e, _, o, r, _, n, o, t, _, t, o, _, b, e, \u2026</td>\n<td>\u2026, to, o_, _b, be, e_, _o, or, r_, _n, no, ot, t_, _t, to, o_, _b, be, \u2026</td>\n<td>\u2026, to_, o_b, _be, be_, e_o, _or, or_, r_n, _no, not, ot_, t_t, _to, to_, o_b, _be, \u2026\n</td></tr>\n<tr>\n<td><a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">Computational linguistics</a></td>\n<td><a href=\"/wiki/Word\" title=\"Word\">word</a></td>\n<td>\u2026 to be or not to be \u2026</td>\n<td>\u2026, to, be, or, not, to, be, \u2026</td>\n<td>\u2026, to be, be or, or not, not to, to be, \u2026</td>\n<td>\u2026, to be or, be or not, or not to, not to be, \u2026\n</td></tr></tbody></table>\n<p>Figure 1 shows several example sequences and the corresponding 1-gram, 2-gram and 3-gram sequences.\n</p><p>Here are further examples; these are word-level 3-grams and 4-grams (and counts of the number of times they appeared) from the Google <i>n</i>-gram corpus.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup>\n</p><p>3-grams\n</p>\n<ul><li>ceramics collectables collectibles (55)</li>\n<li>ceramics collectables fine (130)</li>\n<li>ceramics collected by (52)</li>\n<li>ceramics collectible pottery (50)</li>\n<li>ceramics collectibles cooking (45)</li></ul>\n<p>4-grams\n</p>\n<ul><li>serve as the incoming (92)</li>\n<li>serve as the incubator (99)</li>\n<li>serve as the independent (794)</li>\n<li>serve as the index (223)</li>\n<li>serve as the indication (72)</li>\n<li>serve as the indicator (120)</li></ul>\n<h2><span class=\"mw-headline\" id=\"n-gram_models\"><i>n</i>-gram models</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=3\" title=\"Edit section: n-gram models\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>An <b><i>n</i>-gram model</b> models sequences, notably natural languages, using the statistical properties of <i>n</i>-grams.\n</p><p>This idea can be traced to an experiment by <a href=\"/wiki/Claude_Shannon\" title=\"Claude Shannon\">Claude Shannon</a>'s work in <a href=\"/wiki/Information_theory\" title=\"Information theory\">information theory</a>.  Shannon posed the question: given a sequence of letters (for example, the sequence \"for ex\"), what is the <a href=\"/wiki/Likelihood\" class=\"mw-redirect\" title=\"Likelihood\">likelihood</a> of the next letter?  From training data, one can derive a <a href=\"/wiki/Probability_distribution\" title=\"Probability distribution\">probability distribution</a> for the next letter given a history of size <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle n}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>n</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle n}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/a601995d55609f2d9f5e233e36fbe9ea26011b3b\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.395ex; height:1.676ex;\" alt=\"n\"/></span>:  <i>a</i> = 0.4, <i>b</i> = 0.00001, <i>c</i> = 0, ....; where the probabilities of all possible \"next-letters\" sum to 1.0.\n</p><p>More concisely, an <i>n</i>-gram model predicts <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle x_{i}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <msub>\n          <mi>x</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle x_{i}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e87000dd6142b81d041896a30fe58f0c3acb2158\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.671ex; width:2.129ex; height:2.009ex;\" alt=\"x_{i}\"/></span> based on <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle x_{i-(n-1)},\\dots ,x_{i-1}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <msub>\n          <mi>x</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n            <mo>&#x2212;<!-- \u2212 --></mo>\n            <mo stretchy=\"false\">(</mo>\n            <mi>n</mi>\n            <mo>&#x2212;<!-- \u2212 --></mo>\n            <mn>1</mn>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n        </msub>\n        <mo>,</mo>\n        <mo>&#x2026;<!-- \u2026 --></mo>\n        <mo>,</mo>\n        <msub>\n          <mi>x</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n            <mo>&#x2212;<!-- \u2212 --></mo>\n            <mn>1</mn>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle x_{i-(n-1)},\\dots ,x_{i-1}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/9a91f8159d5cc978c4c82b72490de99717d3acfc\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -1.171ex; width:17.182ex; height:2.509ex;\" alt=\"x_{i-(n-1)}, \\dots, x_{i-1}\"/></span>. In probability terms, this is <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle P(x_{i}\\mid x_{i-(n-1)},\\dots ,x_{i-1})}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <msub>\n          <mi>x</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mo>&#x2223;<!-- \u2223 --></mo>\n        <msub>\n          <mi>x</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n            <mo>&#x2212;<!-- \u2212 --></mo>\n            <mo stretchy=\"false\">(</mo>\n            <mi>n</mi>\n            <mo>&#x2212;<!-- \u2212 --></mo>\n            <mn>1</mn>\n            <mo stretchy=\"false\">)</mo>\n          </mrow>\n        </msub>\n        <mo>,</mo>\n        <mo>&#x2026;<!-- \u2026 --></mo>\n        <mo>,</mo>\n        <msub>\n          <mi>x</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n            <mo>&#x2212;<!-- \u2212 --></mo>\n            <mn>1</mn>\n          </mrow>\n        </msub>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle P(x_{i}\\mid x_{i-(n-1)},\\dots ,x_{i-1})}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/d6b32cd7c23550a9250578aeb47e0e73bb5d3036\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -1.171ex; width:24.803ex; height:3.176ex;\" alt=\"P(x_{i}\\mid x_{{i-(n-1)}},\\dots ,x_{{i-1}})\"/></span>. When used for <a href=\"/wiki/Language_model\" title=\"Language model\">language modeling</a>, independence assumptions are made so that each word depends only on the last <i>n</i>&#160;\u2212&#160;1 words.  This <a href=\"/wiki/Markov_model\" title=\"Markov model\">Markov model</a> is used as an approximation of the true underlying language.  This assumption is important because it massively simplifies the problem of estimating the language model from data.  In addition, because of the open nature of language, it is common to group words unknown to the language model together.\n</p><p>Note that in a simple <i>n</i>-gram language model, the probability of a word, conditioned on some number of previous words (one word in a bigram model, two words in a trigram model, etc.) can be described as following a <a href=\"/wiki/Categorical_distribution\" title=\"Categorical distribution\">categorical distribution</a> (often imprecisely called a \"<a href=\"/wiki/Multinomial_distribution\" title=\"Multinomial distribution\">multinomial distribution</a>\").\n</p><p>In practice, the probability distributions are smoothed by assigning non-zero probabilities to unseen words or <i>n</i>-grams; see <a href=\"/wiki/N-gram#Smoothing_techniques\" title=\"N-gram\">smoothing techniques</a>.\n</p>\n<h2><span class=\"mw-headline\" id=\"Applications_and_considerations\">Applications and considerations</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=4\" title=\"Edit section: Applications and considerations\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p><i>n</i>-gram models are widely used in statistical <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a>.  In <a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">speech recognition</a>, <a href=\"/wiki/Phonemes\" class=\"mw-redirect\" title=\"Phonemes\">phonemes</a> and sequences of phonemes are modeled using a <i>n</i>-gram distribution.  For parsing, words are modeled such that each <i>n</i>-gram is composed of <i>n</i> words.  For <a href=\"/wiki/Language_identification\" title=\"Language identification\">language identification</a>, sequences of <a href=\"/wiki/Character_(symbol)\" title=\"Character (symbol)\">characters</a>/<a href=\"/wiki/Grapheme\" title=\"Grapheme\">graphemes</a> (<i>e.g.</i>, <a href=\"/wiki/Letter_(alphabet)\" title=\"Letter (alphabet)\">letters of the alphabet</a>) are modeled for different languages.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\">&#91;4&#93;</a></sup>  For sequences of characters, the 3-grams (sometimes referred to as \"trigrams\") that can be generated from \"good morning\" are \"goo\", \"ood\", \"od \", \"d m\", \" mo\", \"mor\" and so forth, counting the space character as a gram (sometimes the beginning and end of a text are modeled explicitly, adding \"__g\", \"_go\", \"ng_\", and \"g__\").  For sequences of words,  the trigrams (shingles) that can be generated from \"the dog smelled like a skunk\" are \"# the dog\", \"the dog smelled\", \"dog smelled like\", \"smelled like a\", \"like a skunk\" and \"a skunk #\".\n</p><p>Practitioners<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions\" title=\"Wikipedia:Manual of Style/Words to watch\"><span title=\"The material near this tag possibly uses too-vague attribution or weasel words. (June 2014)\">who?</span></a></i>&#93;</sup> more interested in multiple word terms might preprocess strings to remove spaces.<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Manual_of_Style/Words_to_watch#Unsupported_attributions\" title=\"Wikipedia:Manual of Style/Words to watch\"><span title=\"The material near this tag possibly uses too-vague attribution or weasel words. (June 2014)\">who?</span></a></i>&#93;</sup> Many simply collapse <a href=\"/wiki/Whitespace_character\" title=\"Whitespace character\">whitespace</a> to a single space while preserving paragraph marks, because the whitespace is frequently either an element of writing style or introduces layout or presentation not required by the prediction and deduction methodology. Punctuation is also commonly reduced or removed by preprocessing and is frequently used to trigger functionality.\n</p><p><i>n</i>-grams can also be used for sequences of words or almost any type of data. For example, they have been used for extracting features for clustering large sets of satellite earth images and for determining what part of the Earth a particular image came from.<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\">&#91;5&#93;</a></sup>  They have also been very successful as the first pass in genetic sequence search and in the identification of the species from which short sequences of DNA originated.<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup>\n</p><p><i>n</i>-gram models are often criticized because they lack any explicit representation of long range dependency. This is because the only explicit dependency range is (<i>n</i>&#160;\u2212&#160;1) tokens for an <i>n</i>-gram model, and since natural languages incorporate many cases of unbounded dependencies (such as <a href=\"/wiki/Wh-movement\" title=\"Wh-movement\">wh-movement</a>), this means that an <i>n</i>-gram model cannot in principle distinguish unbounded dependencies from noise (since long range correlations drop exponentially with distance for any Markov model). For this reason, <i>n</i>-gram models have not made much impact on linguistic theory, where part of the explicit goal is to model such dependencies.\n</p><p>Another criticism that has been made is that Markov models of language, including <i>n</i>-gram models, do not explicitly capture the performance/competence distinction.  This is because <i>n</i>-gram models are not designed to model linguistic knowledge as such, and make no claims to being (even potentially) complete models of linguistic knowledge; instead, they are used in practical applications.\n</p><p>In practice, <i>n</i>-gram models have been shown  to be extremely effective in modeling language data, which is a core component in modern statistical <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">language</a> applications.\n</p><p>Most modern applications that rely on <i>n</i>-gram based models, such as <a href=\"/wiki/Machine_translation\" title=\"Machine translation\">machine translation</a> applications, do not rely exclusively on such models; instead, they typically also incorporate <a href=\"/wiki/Bayesian_inference\" title=\"Bayesian inference\">Bayesian inference</a>. Modern statistical models are typically made up of two parts, a <a href=\"/wiki/Prior_distribution\" class=\"mw-redirect\" title=\"Prior distribution\">prior distribution</a> describing the inherent likelihood of a possible result and a <a href=\"/wiki/Likelihood_function\" title=\"Likelihood function\">likelihood function</a> used to assess the compatibility of a possible result with observed data. When a language model is used, it is used as part of the prior distribution (e.g. to gauge the inherent \"goodness\" of a possible translation), and even then it is often not the only component in this distribution.\n</p><p><a href=\"/wiki/Feature_engineering\" title=\"Feature engineering\">Handcrafted features</a> of various sorts are also used, for example variables that represent the position of a word in a sentence or the general topic of discourse. In addition, features based on the structure of the potential result, such as syntactic considerations, are often used. Such features are also used as part of the likelihood function, which makes use of the observed data. Conventional linguistic theory can be incorporated in these features (although in practice, it is rare that features specific to generative or other particular theories of grammar are incorporated, as <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguists</a> tend to be \"agnostic\" towards individual theories of grammar<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (November 2011)\">citation needed</span></a></i>&#93;</sup>).\n</p>\n<h3><span class=\"mw-headline\" id=\"Out-of-vocabulary_words\">Out-of-vocabulary words</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=5\" title=\"Edit section: Out-of-vocabulary words\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Statistical_machine_translation\" title=\"Statistical machine translation\">Statistical machine translation</a></div>\n<p>An issue when using n-gram language models are out-of-vocabulary (OOV) words. They are encountered in <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a> and <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a> when the input includes words which were not present in a system's dictionary or database during its preparation. By default, when a language model is estimated, the entire observed vocabulary is used. In some cases, it may be necessary to estimate the language model with a specific fixed vocabulary. In such a scenario, the n-grams in the <a href=\"/wiki/Text_corpus\" title=\"Text corpus\">corpus</a> that contain an out-of-vocabulary word are ignored. The n-gram probabilities are smoothed over all the words in the vocabulary even if they were not observed.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;7&#93;</a></sup>\n</p><p>Nonetheless, it is essential in some cases to explicitly model the probability of out-of-vocabulary words by introducing a special token (e.g. <i>&lt;unk&gt;</i>) into the vocabulary. Out-of-vocabulary words in the corpus are effectively replaced with this special &lt;unk&gt; token before n-grams counts are cumulated. With this option, it is possible to estimate the transition probabilities of n-grams involving out-of-vocabulary words.<sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;8&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"n-grams_for_approximate_matching\"><i>n</i>-grams for approximate matching</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=6\" title=\"Edit section: n-grams for approximate matching\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Approximate_string_matching\" title=\"Approximate string matching\">Approximate string matching</a></div>\n<p><i>n</i>-grams can also be used for efficient approximate matching.  By converting a sequence of items to a set of <i>n</i>-grams, it can be embedded in a <a href=\"/wiki/Vector_space\" title=\"Vector space\">vector space</a>, thus allowing the sequence to be compared to other sequences in an efficient manner. For example, if we convert strings with only letters in the English alphabet into single character 3-grams, we get a <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle 26^{3}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <msup>\n          <mn>26</mn>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mn>3</mn>\n          </mrow>\n        </msup>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle 26^{3}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/1cdb2393bc4ee35b201fc726c29becad25523122\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:3.379ex; height:2.676ex;\" alt=\"26^3\"/></span>-dimensional space (the first dimension measures the number of occurrences of \"aaa\", the second \"aab\", and so forth for all possible combinations of three letters). Using this representation, we lose information about the string. For example, both the strings \"abc\" and \"bca\" give rise to exactly the same 2-gram \"bc\" (although {\"ab\", \"bc\"} is clearly not the same as {\"bc\", \"ca\"}). However, we know empirically that if two strings of real text have a similar vector representation (as measured by <a href=\"/wiki/Cosine_similarity\" title=\"Cosine similarity\">cosine distance</a>) then they are likely to be similar. Other metrics have also been applied to vectors of <i>n</i>-grams with varying, sometimes better, results. For example, <a href=\"/wiki/Z-score\" class=\"mw-redirect\" title=\"Z-score\">z-scores</a> have been used to compare documents by examining how many standard deviations each <i>n</i>-gram differs from its mean occurrence in a large collection, or <a href=\"/wiki/Text_corpus\" title=\"Text corpus\">text corpus</a>, of documents (which form the \"background\" vector).  In the event of small counts, the <a href=\"/w/index.php?title=G-score&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"G-score (page does not exist)\">g-score</a> (also known as <a href=\"/wiki/G-test\" title=\"G-test\">g-test</a>) may give better results for comparing alternative models.\n</p><p>It is also possible to take a more principled approach to the statistics of <i>n</i>-grams, modeling similarity as the likelihood that two strings came from the same source directly in terms of a problem in <a href=\"/wiki/Bayesian_inference\" title=\"Bayesian inference\">Bayesian inference</a>.\n</p><p><i>n</i>-gram-based searching can also be used for <a href=\"/wiki/Plagiarism_detection\" title=\"Plagiarism detection\">plagiarism detection</a>.\n</p>\n<h2><span class=\"mw-headline\" id=\"Other_applications\">Other applications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=7\" title=\"Edit section: Other applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p><i>n</i>-grams find use in several areas of computer science, <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a>, and applied mathematics.\n</p><p>They have been used to:\n</p>\n<ul><li>design <a href=\"/wiki/Kernel_trick\" class=\"mw-redirect\" title=\"Kernel trick\">kernels</a> that allow <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a> algorithms such as <a href=\"/wiki/Support_vector_machine\" title=\"Support vector machine\">support vector machines</a> to learn from string data</li>\n<li>find likely candidates for the correct spelling of a misspelled word</li>\n<li>improve compression in <a href=\"/wiki/Data_compression\" title=\"Data compression\">compression algorithms</a> where a small area of data requires <i>n</i>-grams of greater length</li>\n<li>assess the probability of a given word sequence appearing in text of a language of interest in pattern recognition systems, <a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">speech recognition</a>, OCR (<a href=\"/wiki/Optical_character_recognition\" title=\"Optical character recognition\">optical character recognition</a>), <a href=\"/wiki/Intelligent_Character_Recognition\" class=\"mw-redirect\" title=\"Intelligent Character Recognition\">Intelligent Character Recognition</a> (<a href=\"/wiki/Intelligent_character_recognition\" title=\"Intelligent character recognition\">ICR</a>), <a href=\"/wiki/Machine_translation\" title=\"Machine translation\">machine translation</a> and similar applications</li>\n<li>improve retrieval in <a href=\"/wiki/Information_retrieval\" title=\"Information retrieval\">information retrieval</a> systems when it is hoped to find similar \"documents\" (a term for which the conventional meaning is sometimes stretched, depending on the data set) given a single query document and a database of reference documents</li>\n<li>improve retrieval performance in genetic sequence analysis as in the <a href=\"/wiki/BLAST\" title=\"BLAST\">BLAST</a> family of programs</li>\n<li>identify the language a text is in or the species a small sequence of DNA was taken from</li>\n<li>predict letters or words at random in order to create text, as in the <a href=\"/wiki/Dissociated_press\" title=\"Dissociated press\">dissociated press</a> algorithm.</li>\n<li><a href=\"/wiki/Cryptanalysis\" title=\"Cryptanalysis\">cryptanalysis</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"Bias-versus-variance_trade-off\">Bias-versus-variance trade-off</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=8\" title=\"Edit section: Bias-versus-variance trade-off\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>To choose a value for <i>n</i> in an <i>n</i>-gram model, it is necessary to find the right trade off between the stability of the estimate against its appropriateness. This means that trigram (i.e. triplets of words) is a common choice with large training corpora (millions of words), whereas a bigram is often used with smaller ones.\n</p>\n<h3><span class=\"mw-headline\" id=\"Smoothing_techniques\">Smoothing techniques</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=9\" title=\"Edit section: Smoothing techniques\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>There are problems of balance weight between <i>infrequent grams</i> (for example, if a proper name appeared in the training data) and <i>frequent grams</i>.   Also, items not seen in the training data will be given a <a href=\"/wiki/Probability\" title=\"Probability\">probability</a> of 0.0 without <a href=\"/wiki/Smoothing\" title=\"Smoothing\">smoothing</a>. For unseen but plausible data from a sample, one can introduce <a href=\"/wiki/Pseudocount\" class=\"mw-redirect\" title=\"Pseudocount\">pseudocounts</a>.  Pseudocounts are generally motivated on Bayesian grounds.\n</p><p>In practice it is necessary to <i>smooth</i> the probability distributions by also assigning non-zero probabilities to unseen words or <i>n</i>-grams.  The reason is that models derived directly from the <i>n</i>-gram frequency counts have severe problems when confronted with any <i>n</i>-grams that have not explicitly been seen before \u2013 <a href=\"/wiki/PPM_compression_algorithm\" class=\"mw-redirect\" title=\"PPM compression algorithm\">the zero-frequency problem</a>.  Various smoothing methods are used, from simple \"add-one\" (Laplace) smoothing (assign a count of 1 to unseen <i>n</i>-grams; see <a href=\"/wiki/Rule_of_succession\" title=\"Rule of succession\">Rule of succession</a>) to more sophisticated models, such as <a href=\"/wiki/Good%E2%80%93Turing_discounting\" class=\"mw-redirect\" title=\"Good\u2013Turing discounting\">Good\u2013Turing discounting</a> or <a href=\"/wiki/Katz%27s_back-off_model\" title=\"Katz&#39;s back-off model\">back-off models</a>.  Some of these methods are equivalent to assigning a <a href=\"/wiki/Prior_distribution\" class=\"mw-redirect\" title=\"Prior distribution\">prior distribution</a> to the probabilities of the <i>n</i>-grams and using <a href=\"/wiki/Bayesian_inference\" title=\"Bayesian inference\">Bayesian inference</a> to compute the resulting <a href=\"/wiki/Posterior_distribution\" class=\"mw-redirect\" title=\"Posterior distribution\">posterior</a> <i>n</i>-gram probabilities.  However, the more sophisticated smoothing models were typically not derived in this fashion, but instead through independent considerations.\n</p>\n<ul><li><a href=\"/wiki/Linear_interpolation\" title=\"Linear interpolation\">Linear interpolation</a> (e.g., taking the <a href=\"/wiki/Weighted_mean\" class=\"mw-redirect\" title=\"Weighted mean\">weighted mean</a> of the unigram, bigram, and trigram)</li>\n<li><a href=\"/wiki/Good%E2%80%93Turing_frequency_estimation\" title=\"Good\u2013Turing frequency estimation\">Good\u2013Turing</a> discounting</li>\n<li><a href=\"/w/index.php?title=Witten%E2%80%93Bell_discounting&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Witten\u2013Bell discounting (page does not exist)\">Witten\u2013Bell discounting</a></li>\n<li><a href=\"/wiki/Additive_smoothing\" title=\"Additive smoothing\">Lidstone's smoothing</a></li>\n<li><a href=\"/wiki/Katz%27s_back-off_model\" title=\"Katz&#39;s back-off model\">Katz's back-off model</a> (trigram)</li>\n<li><a href=\"/wiki/Kneser%E2%80%93Ney_smoothing\" title=\"Kneser\u2013Ney smoothing\">Kneser\u2013Ney smoothing</a></li></ul>\n<h3><span class=\"mw-headline\" id=\"Skip-gram\">Skip-gram</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=10\" title=\"Edit section: Skip-gram\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>In the field of <a href=\"/wiki/Computational_linguistics\" title=\"Computational linguistics\">computational linguistics</a>, in particular <a href=\"/wiki/Language_model\" title=\"Language model\">language modeling</a>, <b>skip-grams</b><sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup> are a generalization of <i>n</i>-grams in which the components (typically words) need not be consecutive in the text under consideration, but may leave gaps that are <i>skipped</i> over.<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\">&#91;10&#93;</a></sup> They provide one way of overcoming the <a href=\"/w/index.php?title=Data_sparsity_problem&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Data sparsity problem (page does not exist)\">data sparsity problem</a> found with conventional <i>n</i>-gram analysis.\n</p><p>Formally, an <span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span>-gram is a consecutive subsequence of length <span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span> of some sequence of tokens <span class=\"texhtml\"><i>w</i><sub>1</sub> \u2026 <i>w</i><sub><i>n</i></sub></span>. A <span class=\"texhtml mvar\" style=\"font-style:italic;\">k</span>-skip-<span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span>-gram is a length-<span class=\"texhtml mvar\" style=\"font-style:italic;\">n</span> subsequence where the components occur at distance at most <span class=\"texhtml mvar\" style=\"font-style:italic;\">k</span> from each other.\n</p><p>For example, in the input text:\n</p>\n<dl><dd><i>the rain in Spain falls mainly on the plain</i></dd></dl>\n<p>the set of 1-skip-2-grams includes all the bigrams (2-grams), and in addition the subsequences\n</p>\n<dl><dd><i>the in</i>, <i>rain Spain</i>, <i>in falls</i>, <i>Spain mainly</i>, <i>falls on</i>, <i>mainly the</i>, and <i>on plain</i>.</dd></dl>\n<h2><span class=\"mw-headline\" id=\"Syntactic_n-grams\">Syntactic <i>n</i>-grams</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=11\" title=\"Edit section: Syntactic n-grams\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Syntactic <i>n</i>-grams are <i>n</i>-grams defined by paths in syntactic dependency or constituent trees rather than the linear structure of the text.<sup id=\"cite_ref-sng_11-0\" class=\"reference\"><a href=\"#cite_note-sng-11\">&#91;11&#93;</a></sup><sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup><sup id=\"cite_ref-13\" class=\"reference\"><a href=\"#cite_note-13\">&#91;13&#93;</a></sup> For example, the sentence \"economic news has little effect on financial markets\" can be transformed to syntactic <i>n</i>-grams following the tree structure of its <a href=\"/wiki/Dependency_grammar\" title=\"Dependency grammar\">dependency relations</a>: news-economic, effect-little, effect-on-markets-financial.<sup id=\"cite_ref-sng_11-1\" class=\"reference\"><a href=\"#cite_note-sng-11\">&#91;11&#93;</a></sup>\n</p><p>Syntactic <i>n</i>-grams are intended to reflect syntactic structure more faithfully than linear <i>n</i>-grams, and have many of the same applications, especially as features in a Vector Space Model. Syntactic <i>n</i>-grams for certain tasks gives better results than the use of standard <i>n</i>-grams, for example, for authorship attribution.<sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\">&#91;14&#93;</a></sup>\n</p><p>Another type of syntactic <i>n</i>-grams are part-of-speech <i>n</i>-grams, defined as fixed-length contiguous overlapping subsequences that are extracted from part-of-speech sequences of text. Part-of-speech <i>n</i>-grams have several applications, most commonly in information retrieval<sup id=\"cite_ref-15\" class=\"reference\"><a href=\"#cite_note-15\">&#91;15&#93;</a></sup>.\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=12\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Collocation\" title=\"Collocation\">Collocation</a></li>\n<li><a href=\"/wiki/Hidden_Markov_model\" title=\"Hidden Markov model\">Hidden Markov model</a></li>\n<li><a href=\"/wiki/N-tuple\" class=\"mw-redirect\" title=\"N-tuple\">n-tuple</a></li>\n<li><a href=\"/wiki/String_kernel\" title=\"String kernel\">String kernel</a></li>\n<li><a href=\"/wiki/MinHash\" title=\"MinHash\">MinHash</a></li>\n<li><a href=\"/wiki/Feature_extraction\" title=\"Feature extraction\">Feature extraction</a></li>\n<li><a href=\"/wiki/Longest_common_substring_problem\" title=\"Longest common substring problem\">Longest common substring problem</a></li></ul>\n<div role=\"navigation\" class=\"navbox\" aria-labelledby=\"Natural_language_processing\" style=\"padding:3px\"><table class=\"nowraplinks hlist collapsible collapsed navbox-inner\" style=\"border-spacing:0;background:transparent;color:inherit\"><tbody><tr><th scope=\"col\" class=\"navbox-title\" colspan=\"2\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Natural_Language_Processing\" title=\"Template:Natural Language Processing\"><abbr title=\"View this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Natural_Language_Processing\" title=\"Template talk:Natural Language Processing\"><abbr title=\"Discuss this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Template:Natural_Language_Processing&amp;action=edit\"><abbr title=\"Edit this template\" style=\";;background:none transparent;border:none;-moz-box-shadow:none;-webkit-box-shadow:none;box-shadow:none; padding:0;\">e</abbr></a></li></ul></div><div id=\"Natural_language_processing\" style=\"font-size:114%;margin:0 4em\"><a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">Natural language processing</a></div></th></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\">General terms</th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Natural_language_understanding\" title=\"Natural language understanding\">Natural language understanding</a></li>\n<li><a href=\"/wiki/Text_corpus\" title=\"Text corpus\">Text corpus</a></li>\n<li><a href=\"/wiki/Speech_corpus\" title=\"Speech corpus\">Speech corpus</a></li>\n<li><a href=\"/wiki/Stop_words\" title=\"Stop words\">Stopwords</a></li>\n<li><a href=\"/wiki/Bag-of-words_model\" title=\"Bag-of-words model\">Bag-of-words</a></li>\n<li><a href=\"/wiki/AI-complete\" title=\"AI-complete\">AI-complete</a></li>\n<li><a class=\"mw-selflink selflink\">n-gram</a> (<a href=\"/wiki/Bigram\" title=\"Bigram\">Bigram</a>, <a href=\"/wiki/Trigram\" title=\"Trigram\">Trigram</a>)</li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Text_mining\" title=\"Text mining\">Text analysis</a></th><td class=\"navbox-list navbox-even\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Text_segmentation\" title=\"Text segmentation\">Text segmentation</a></li>\n<li><a href=\"/wiki/Part-of-speech_tagging\" title=\"Part-of-speech tagging\">Part-of-speech tagging</a></li>\n<li><a href=\"/wiki/Shallow_parsing\" title=\"Shallow parsing\">Text chunking</a></li>\n<li><a href=\"/wiki/Compound_term_processing\" title=\"Compound term processing\">Compound term processing</a></li>\n<li><a href=\"/wiki/Collocation_extraction\" title=\"Collocation extraction\">Collocation extraction</a></li>\n<li><a href=\"/wiki/Stemming\" title=\"Stemming\">Stemming</a></li>\n<li><a href=\"/wiki/Lemmatisation\" title=\"Lemmatisation\">Lemmatisation</a></li>\n<li><a href=\"/wiki/Named-entity_recognition\" title=\"Named-entity recognition\">Named-entity recognition</a></li>\n<li><a href=\"/wiki/Coreference#Coreference_resolution\" title=\"Coreference\">Coreference resolution</a></li>\n<li><a href=\"/wiki/Sentiment_analysis\" title=\"Sentiment analysis\">Sentiment analysis</a></li>\n<li><a href=\"/wiki/Concept_mining\" title=\"Concept mining\">Concept mining</a></li>\n<li><a href=\"/wiki/Parsing\" title=\"Parsing\">Parsing</a></li>\n<li><a href=\"/wiki/Word-sense_disambiguation\" title=\"Word-sense disambiguation\">Word-sense disambiguation</a></li>\n<li><a href=\"/wiki/Ontology_learning\" title=\"Ontology learning\">Ontology learning</a></li>\n<li><a href=\"/wiki/Terminology_extraction\" title=\"Terminology extraction\">Terminology extraction</a></li>\n<li><a href=\"/wiki/Truecasing\" title=\"Truecasing\">Truecasing</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Automatic_summarization\" title=\"Automatic summarization\">Automatic summarization</a></th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Multi-document_summarization\" title=\"Multi-document summarization\">Multi-document summarization</a></li>\n<li><a href=\"/wiki/Sentence_extraction\" title=\"Sentence extraction\">Sentence extraction</a></li>\n<li><a href=\"/wiki/Text_simplification\" title=\"Text simplification\">Text simplification</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Machine_translation\" title=\"Machine translation\">Machine translation</a></th><td class=\"navbox-list navbox-even\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Computer-assisted_translation\" title=\"Computer-assisted translation\">Computer-assisted</a></li>\n<li><a href=\"/wiki/Example-based_machine_translation\" title=\"Example-based machine translation\">Example-based</a></li>\n<li><a href=\"/wiki/Rule-based_machine_translation\" title=\"Rule-based machine translation\">Rule-based</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Automatic_identification_and_data_capture\" title=\"Automatic identification and data capture\">Automatic identification<br />and data capture</a></th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Speech_recognition\" title=\"Speech recognition\">Speech recognition</a></li>\n<li><a href=\"/wiki/Speech_synthesis\" title=\"Speech synthesis\">Speech synthesis</a></li>\n<li><a href=\"/wiki/Optical_character_recognition\" title=\"Optical character recognition\">Optical character recognition</a></li>\n<li><a href=\"/wiki/Natural_language_generation\" title=\"Natural language generation\">Natural language generation</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Topic_model\" title=\"Topic model\">Topic model</a></th><td class=\"navbox-list navbox-even\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Pachinko_allocation\" title=\"Pachinko allocation\">Pachinko allocation</a></li>\n<li><a href=\"/wiki/Latent_Dirichlet_allocation\" title=\"Latent Dirichlet allocation\">Latent Dirichlet allocation</a></li>\n<li><a href=\"/wiki/Latent_semantic_analysis\" title=\"Latent semantic analysis\">Latent semantic analysis</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Computer-assisted_reviewing\" title=\"Computer-assisted reviewing\">Computer-assisted<br />reviewing</a></th><td class=\"navbox-list navbox-odd\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Automated_essay_scoring\" title=\"Automated essay scoring\">Automated essay scoring</a></li>\n<li><a href=\"/wiki/Concordancer\" title=\"Concordancer\">Concordancer</a></li>\n<li><a href=\"/wiki/Grammar_checker\" title=\"Grammar checker\">Grammar checker</a></li>\n<li><a href=\"/wiki/Predictive_text\" title=\"Predictive text\">Predictive text</a></li>\n<li><a href=\"/wiki/Spell_checker\" title=\"Spell checker\">Spell checker</a></li>\n<li><a href=\"/wiki/Syntax_guessing\" title=\"Syntax guessing\">Syntax guessing</a></li></ul>\n</div></td></tr><tr><th scope=\"row\" class=\"navbox-group\" style=\"width:1%\"><a href=\"/wiki/Natural_language_user_interface\" class=\"mw-redirect\" title=\"Natural language user interface\">Natural language<br />user interface</a></th><td class=\"navbox-list navbox-even\" style=\"text-align:left;border-left-width:2px;border-left-style:solid;width:100%;padding:0px\"><div style=\"padding:0em 0.25em\">\n<ul><li><a href=\"/wiki/Automated_online_assistant\" class=\"mw-redirect\" title=\"Automated online assistant\">Automated online assistant</a></li>\n<li><a href=\"/wiki/Chatbot\" title=\"Chatbot\">Chatbot</a></li>\n<li><a href=\"/wiki/Interactive_fiction\" title=\"Interactive fiction\">Interactive fiction</a></li>\n<li><a href=\"/wiki/Question_answering\" title=\"Question answering\">Question answering</a></li>\n<li><a href=\"/wiki/Voice_user_interface\" title=\"Voice user interface\">Voice user interface</a></li></ul>\n</div></td></tr></tbody></table></div>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=13\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist\" style=\"list-style-type: decimal;\">\n<div class=\"mw-references-wrap mw-references-columns\"><ol class=\"references\">\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Broder, Andrei Z.; Glassman, Steven C.; Manasse, Mark S.; Zweig, Geoffrey (1997). \"Syntactic clustering of the web\". <i>Computer Networks and ISDN Systems</i>. <b>29</b> (8): 1157\u20131166. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/s0169-7552%2897%2900031-7\">10.1016/s0169-7552(97)00031-7</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computer+Networks+and+ISDN+Systems&amp;rft.atitle=Syntactic+clustering+of+the+web&amp;rft.volume=29&amp;rft.issue=8&amp;rft.pages=1157-1166&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1016%2Fs0169-7552%2897%2900031-7&amp;rft.aulast=Broder&amp;rft.aufirst=Andrei+Z.&amp;rft.au=Glassman%2C+Steven+C.&amp;rft.au=Manasse%2C+Mark+S.&amp;rft.au=Zweig%2C+Geoffrey&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external free\" href=\"https://www.coursera.org/learn/natural-language-processing/lecture/UnEHs/07-01-noisy-channel-model-8-33\">https://www.coursera.org/learn/natural-language-processing/lecture/UnEHs/07-01-noisy-channel-model-8-33</a></span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">Alex Franz and Thorsten Brants (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html\">\"All Our <i>N</i>-gram are Belong to You\"</a>. <i>Google Research Blog</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2011-12-16</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=unknown&amp;rft.jtitle=Google+Research+Blog&amp;rft.atitle=All+Our+N-gram+are+Belong+to+You&amp;rft.date=2006&amp;rft.au=Alex+Franz+and+Thorsten+Brants&amp;rft_id=http%3A%2F%2Fgoogleresearch.blogspot.com%2F2006%2F08%2Fall-our-n-gram-are-belong-to-you.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Ted Dunning (1994). <a rel=\"nofollow\" class=\"external text\" href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.1958\">\"Statistical Identification of Language\"</a>. New Mexico State University.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Statistical+Identification+of+Language&amp;rft.date=1994&amp;rft.au=Ted+Dunning&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.48.1958&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> Technical Report MCCS 94\u2013273</span>\n</li>\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Soffer, A (1997). \"Image categorization using texture features\". <i>Proceedings of the Fourth International Conference on</i>. <b>1</b> (233): 237. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1109/ICDAR.1997.619847\">10.1109/ICDAR.1997.619847</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proceedings+of+the+Fourth+International+Conference+on&amp;rft.atitle=Image+categorization+using+texture+features&amp;rft.volume=1&amp;rft.issue=233&amp;rft.pages=237&amp;rft.date=1997&amp;rft_id=info%3Adoi%2F10.1109%2FICDAR.1997.619847&amp;rft.aulast=Soffer&amp;rft.aufirst=A&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Tomovi\u0107, Andrija; Jani\u010di\u0107, Predrag; Ke\u0161elj<i>, Vlado (2006). \"</i>n<i>-Gram-based classification and unsupervised hierarchical clustering of genome sequences\". </i>Computer Methods and Programs in Biomedicine<i>. <b>81</b> (2): 137\u2013153. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/j.cmpb.2005.11.007\">10.1016/j.cmpb.2005.11.007</a>.</i></cite><i><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computer+Methods+and+Programs+in+Biomedicine&amp;rft.atitle=n-Gram-based+classification+and+unsupervised+hierarchical+clustering+of+genome+sequences&amp;rft.volume=81&amp;rft.issue=2&amp;rft.pages=137-153&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1016%2Fj.cmpb.2005.11.007&amp;rft.aulast=Tomovi%C4%87&amp;rft.aufirst=Andrija&amp;rft.au=Jani%C4%8Di%C4%87%2C+Predrag&amp;rft.au=Ke%C5%A1elj%27%27%2C+Vlado&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></i></span>\n</li>\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Wo\u0142k, K.; Marasek, K.; Glinkowski, W. (2015). \"Telemedicine as a special case of Machine Translation\". <i>Computerized Medical Imaging and Graphics</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computerized+Medical+Imaging+and+Graphics&amp;rft.atitle=Telemedicine+as+a+special+case+of+Machine+Translation&amp;rft.date=2015&amp;rft.aulast=Wo%C5%82k&amp;rft.aufirst=K.&amp;rft.au=Marasek%2C+K.&amp;rft.au=Glinkowski%2C+W.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation conference\">Wo\u0142k K., Marasek K. (2014). <i>Polish-English Speech Statistical Machine Translation Systems for the IWSLT 2014</i>. Proceedings of the 11th International Workshop on Spoken Language Translation. Tahoe Lake, USA.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Polish-English+Speech+Statistical+Machine+Translation+Systems+for+the+IWSLT+2014&amp;rft.date=2014&amp;rft.au=Wo%C5%82k+K.%2C+Marasek+K.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Huang, Xuedong; Alleva, Fileno; Hon, Hsiao-wuen; Hwang, Mei-yuh; Rosenfeld, Ronald (1 January 1992). <a rel=\"nofollow\" class=\"external text\" href=\"http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.1629\">\"The SPHINX-II Speech Recognition System: An Overview\"</a>. <b>7</b>: 137\u2013148  &#8211; via CiteSeer.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=The+SPHINX-II+Speech+Recognition+System%3A+An+Overview&amp;rft.volume=7&amp;rft.pages=137-148&amp;rft.date=1992-01-01&amp;rft.aulast=Huang&amp;rft.aufirst=Xuedong&amp;rft.au=Alleva%2C+Fileno&amp;rft.au=Hon%2C+Hsiao-wuen&amp;rft.au=Hwang%2C+Mei-yuh&amp;rft.au=Rosenfeld%2C+Ronald&amp;rft_id=http%3A%2F%2Fciteseerx.ist.psu.edu%2Fviewdoc%2Fsummary%3Fdoi%3D10.1.1.45.1629&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation web\">David Guthrie;  et al. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf\">\"A Closer Look at Skip-gram Modelling\"</a> <span style=\"font-size:85%;\">(PDF)</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=unknown&amp;rft.btitle=A+Closer+Look+at+Skip-gram+Modelling&amp;rft.date=2006&amp;rft.au=David+Guthrie&amp;rft_id=http%3A%2F%2Fhomepages.inf.ed.ac.uk%2Fballison%2Fpdf%2Flrec_skipgrams.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-sng-11\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-sng_11-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-sng_11-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Sidorov, Grigori; Velazquez, Francisco; Stamatatos, Efstathios; Gelbukh, Alexander; Chanona-Hern\u00e1ndez, Liliana (2012). \"Syntactic Dependency-based <i>n</i>-grams as Classification Features\". <i>LNAI 7630</i>: 1\u201311.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=LNAI+7630&amp;rft.atitle=Syntactic+Dependency-based+n-grams+as+Classification+Features&amp;rft.pages=1-11&amp;rft.date=2012&amp;rft.aulast=Sidorov&amp;rft.aufirst=Grigori&amp;rft.au=Velazquez%2C+Francisco&amp;rft.au=Stamatatos%2C+Efstathios&amp;rft.au=Gelbukh%2C+Alexander&amp;rft.au=Chanona-Hern%C3%A1ndez%2C+Liliana&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Sidorov, Grigori (2013). \"Syntactic Dependency-Based <i>n</i>-grams in Rule Based Automatic English as Second Language Grammar Correction\". <i>International Journal of Computational Linguistics and Applications</i>. <b>4</b> (2): 169\u2013188.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Computational+Linguistics+and+Applications&amp;rft.atitle=Syntactic+Dependency-Based+n-grams+in+Rule+Based+Automatic+English+as+Second+Language+Grammar+Correction&amp;rft.volume=4&amp;rft.issue=2&amp;rft.pages=169-188&amp;rft.date=2013&amp;rft.aulast=Sidorov&amp;rft.aufirst=Grigori&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-13\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Figueroa, Alejandro; Atkinson, John (2012). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/publication/262176888_Contextual_Language_Models_For_Ranking_Answers_To_Natural_Language_Definition_Questions\">\"Contextual Language Models For Ranking Answers To Natural Language Definition Questions\"</a>. <i>Computational Intelligence</i>. <b>28</b> (4): 528\u2013548. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1111/j.1467-8640.2012.00426.x\">10.1111/j.1467-8640.2012.00426.x</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computational+Intelligence&amp;rft.atitle=Contextual+Language+Models+For+Ranking+Answers+To+Natural+Language+Definition+Questions&amp;rft.volume=28&amp;rft.issue=4&amp;rft.pages=528-548&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1467-8640.2012.00426.x&amp;rft.aulast=Figueroa&amp;rft.aufirst=Alejandro&amp;rft.au=Atkinson%2C+John&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F262176888_Contextual_Language_Models_For_Ranking_Answers_To_Natural_Language_Definition_Questions&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Sidorov, Grigori; Velasquez, Francisco; Stamatatos, Efstathios; Gelbukh, Alexander; Chanona-Hern\u00e1ndez, Liliana. \"Syntactic <i>n</i>-Grams as Machine Learning Features for Natural Language Processing\". <i>Expert Systems with Applications</i>. <b>41</b> (3): 853\u2013860. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/j.eswa.2013.08.015\">10.1016/j.eswa.2013.08.015</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Expert+Systems+with+Applications&amp;rft.atitle=Syntactic+n-Grams+as+Machine+Learning+Features+for+Natural+Language+Processing&amp;rft.volume=41&amp;rft.issue=3&amp;rft.pages=853-860&amp;rft_id=info%3Adoi%2F10.1016%2Fj.eswa.2013.08.015&amp;rft.aulast=Sidorov&amp;rft.aufirst=Grigori&amp;rft.au=Velasquez%2C+Francisco&amp;rft.au=Stamatatos%2C+Efstathios&amp;rft.au=Gelbukh%2C+Alexander&amp;rft.au=Chanona-Hern%C3%A1ndez%2C+Liliana&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-15\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Lioma, C.; van Rijsbergen, C. J. K. (2008). <a rel=\"nofollow\" class=\"external text\" href=\"http://main.cl-lab.dk/www/publications/2008/pdf/part-of-speech-n-grams-and-information-retrieval.pdf\">\"Part of Speech n-Grams and Information Retrieval\"</a> <span style=\"font-size:85%;\">(PDF)</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=Part+of+Speech+n-Grams+and+Information+Retrieval&amp;rft.date=2008&amp;rft.aulast=Lioma&amp;rft.aufirst=C.&amp;rft.au=van+Rijsbergen%2C+C.+J.+K.&amp;rft_id=http%3A%2F%2Fmain.cl-lab.dk%2Fwww%2Fpublications%2F2008%2Fpdf%2Fpart-of-speech-n-grams-and-information-retrieval.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> <i>French Review of Applied Linguistics, Special issue on Information Extraction and Linguistics, vol.XIII, no. 2008/1, Cairn</i> 9 - 22.</span>\n</li>\n</ol></div></div>\n<ul><li>Christopher D. Manning, Hinrich Sch\u00fctze, <i>Foundations of Statistical Natural Language Processing</i>, MIT Press: 1999.  <a href=\"/wiki/International_Standard_Book_Number\" title=\"International Standard Book Number\">ISBN</a>&#160;<a href=\"/wiki/Special:BookSources/0-262-13360-1\" title=\"Special:BookSources/0-262-13360-1\">0-262-13360-1</a>.</li>\n<li><cite class=\"citation journal\">White, Owen; Dunning, Ted; Sutton, Granger; Adams, Mark; Venter, J.Craig; Fields, Chris (1993). \"A quality control algorithm for dna sequencing projects\". <i>Nucleic Acids Research</i>. <b>21</b> (16): 3829\u20133838. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1093/nar/21.16.3829\">10.1093/nar/21.16.3829</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Nucleic+Acids+Research&amp;rft.atitle=A+quality+control+algorithm+for+dna+sequencing+projects&amp;rft.volume=21&amp;rft.issue=16&amp;rft.pages=3829-3838&amp;rft.date=1993&amp;rft_id=info%3Adoi%2F10.1093%2Fnar%2F21.16.3829&amp;rft.aulast=White&amp;rft.aufirst=Owen&amp;rft.au=Dunning%2C+Ted&amp;rft.au=Sutton%2C+Granger&amp;rft.au=Adams%2C+Mark&amp;rft.au=Venter%2C+J.Craig&amp;rft.au=Fields%2C+Chris&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li>Frederick J. Damerau, <i>Markov Models and Linguistic Theory</i>.  Mouton.  The Hague, 1971.</li>\n<li><cite class=\"citation journal\">Figueroa, Alejandro; Atkinson, John (2012). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/publication/262176888_Contextual_Language_Models_For_Ranking_Answers_To_Natural_Language_Definition_Questions\">\"Contextual Language Models For Ranking Answers To Natural Language Definition Questions\"</a>. <i>Computational Intelligence</i>. <b>28</b> (4): 528\u2013548. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1111/j.1467-8640.2012.00426.x\">10.1111/j.1467-8640.2012.00426.x</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Computational+Intelligence&amp;rft.atitle=Contextual+Language+Models+For+Ranking+Answers+To+Natural+Language+Definition+Questions&amp;rft.volume=28&amp;rft.issue=4&amp;rft.pages=528-548&amp;rft.date=2012&amp;rft_id=info%3Adoi%2F10.1111%2Fj.1467-8640.2012.00426.x&amp;rft.aulast=Figueroa&amp;rft.aufirst=Alejandro&amp;rft.au=Atkinson%2C+John&amp;rft_id=https%3A%2F%2Fwww.researchgate.net%2Fpublication%2F262176888_Contextual_Language_Models_For_Ranking_Answers_To_Natural_Language_Definition_Questions&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite class=\"citation conference\">Brocardo, Marcelo Luiz; Issa Traore; Sherif Saad; Isaac Woungang (2013). <a rel=\"nofollow\" class=\"external text\" href=\"https://www.uvic.ca/engineering/ece/isot/assets/docs/Authorship_Verification_for_Short_Messages_using_Stylometry.pdf\"><i>Authorship Verification for Short Messages Using Stylometry</i></a> <span style=\"font-size:85%;\">(PDF)</span>. IEEE Intl. Conference on Computer, Information and Telecommunication Systems (CITS).</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Authorship+Verification+for+Short+Messages+Using+Stylometry&amp;rft.date=2013&amp;rft.aulast=Brocardo&amp;rft.aufirst=Marcelo+Luiz&amp;rft.au=Issa+Traore&amp;rft.au=Sherif+Saad&amp;rft.au=Isaac+Woungang&amp;rft_id=https%3A%2F%2Fwww.uvic.ca%2Fengineering%2Fece%2Fisot%2Fassets%2Fdocs%2FAuthorship_Verification_for_Short_Messages_using_Stylometry.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AN-gram\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li></ul>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=N-gram&amp;action=edit&amp;section=14\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/ngrams\">Google's Google Book <i>n</i>-gram viewer</a> and <a rel=\"nofollow\" class=\"external text\" href=\"http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html\">Web <i>n</i>-grams database</a> (September 2006)</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://research.microsoft.com/web-ngram\">Microsoft's web <i>n</i>-grams service</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://data.statoperator.com/\">STATOPERATOR N-grams Project Weighted <i>n</i>-gram viewer for every domain in Alexa Top 1M</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.ngrams.info/\">1,000,000 most frequent 2,3,4,5-grams</a> from the 425 million word <a href=\"/wiki/Corpus_of_Contemporary_American_English\" title=\"Corpus of Contemporary American English\">Corpus of Contemporary American English</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.peachnote.com/\">Peachnote's music ngram viewer</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.w3.org/TR/ngram-spec/\">Stochastic Language Models (<i>n</i>-Gram) Specification</a> (W3C)</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/lm.pdf\">Michael Collin's notes on <i>n</i>-Gram Language Models</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth/\">OpenRefine: Clustering In Depth</a></li></ul>\n<p class=\"mw-empty-elt\">\n</p>\n<!-- \nNewPP limit report\nParsed by mw2225\nCached time: 20180915050039\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.392 seconds\nReal time usage: 0.500 seconds\nPreprocessor visited node count: 2073/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 58604/2097152 bytes\nTemplate argument size: 2987/2097152 bytes\nHighest expansion depth: 15/40\nExpensive parser function count: 5/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 16359/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.177/10.000 seconds\nLua memory usage: 5.99 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  365.851      1 -total\n 33.06%  120.962      1 Template:Reflist\n 26.17%   95.740     13 Template:Cite_journal\n 12.11%   44.317      1 Template:About\n 11.49%   42.020      1 Template:More_footnotes\n  8.47%   31.004      3 Template:Fix\n  8.24%   30.129      1 Template:Ambox\n  7.62%   27.880      1 Template:Clarify\n  7.54%   27.584      4 Template:Delink\n  6.86%   25.102      1 Template:Fix-span\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:986182-0!canonical!math=5 and timestamp 20180915050039 and revision id 835900923\n -->\n</div>"},"langlinks":[{"lang":"ca","url":"https://ca.wikipedia.org/wiki/N-grama","langname":"Catalan","autonym":"catal\u00e0","*":"N-grama"},{"lang":"cs","url":"https://cs.wikipedia.org/wiki/N-gram","langname":"Czech","autonym":"\u010de\u0161tina","*":"N-gram"},{"lang":"de","url":"https://de.wikipedia.org/wiki/N-Gramm","langname":"German","autonym":"Deutsch","*":"N-Gramm"},{"lang":"es","url":"https://es.wikipedia.org/wiki/N-grama","langname":"Spanish","autonym":"espa\u00f1ol","*":"N-grama"},{"lang":"eu","url":"https://eu.wikipedia.org/wiki/N-grama","langname":"Basque","autonym":"euskara","*":"N-grama"},{"lang":"fa","url":"https://fa.wikipedia.org/wiki/%D8%A7%D9%86-%DA%AF%D8%B1%D9%85","langname":"Persian","autonym":"\u0641\u0627\u0631\u0633\u06cc","*":"\u0627\u0646-\u06af\u0631\u0645"},{"lang":"fr","url":"https://fr.wikipedia.org/wiki/N-gramme","langname":"French","autonym":"fran\u00e7ais","*":"N-gramme"},{"lang":"it","url":"https://it.wikipedia.org/wiki/N-gramma","langname":"Italian","autonym":"italiano","*":"N-gramma"},{"lang":"no","url":"https://no.wikipedia.org/wiki/N-gram","langname":"Norwegian","autonym":"norsk","*":"N-gram"},{"lang":"mhr","url":"https://mhr.wikipedia.org/wiki/N-%D0%B3%D1%80%D0%B0%D0%BC","langname":"Eastern Mari","autonym":"\u043e\u043b\u044b\u043a \u043c\u0430\u0440\u0438\u0439","*":"N-\u0433\u0440\u0430\u043c"},{"lang":"pl","url":"https://pl.wikipedia.org/wiki/N-gram","langname":"Polish","autonym":"polski","*":"N-gram"},{"lang":"ru","url":"https://ru.wikipedia.org/wiki/N-%D0%B3%D1%80%D0%B0%D0%BC%D0%BC%D0%B0","langname":"Russian","autonym":"\u0440\u0443\u0441\u0441\u043a\u0438\u0439","*":"N-\u0433\u0440\u0430\u043c\u043c\u0430"},{"lang":"sk","url":"https://sk.wikipedia.org/wiki/N-gram","langname":"Slovak","autonym":"sloven\u010dina","*":"N-gram"},{"lang":"fi","url":"https://fi.wikipedia.org/wiki/N-grammi","langname":"Finnish","autonym":"suomi","*":"N-grammi"},{"lang":"uk","url":"https://uk.wikipedia.org/wiki/N-%D0%B3%D1%80%D0%B0%D0%BC","langname":"Ukrainian","autonym":"\u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430","*":"N-\u0433\u0440\u0430\u043c"},{"lang":"zh","url":"https://zh.wikipedia.org/wiki/N%E5%85%83%E8%AF%AD%E6%B3%95","langname":"Chinese","autonym":"\u4e2d\u6587","*":"N\u5143\u8bed\u6cd5"}],"categories":[{"sortkey":"","hidden":"","*":"Articles_lacking_in-text_citations_from_February_2011"},{"sortkey":"","hidden":"","*":"All_articles_lacking_in-text_citations"},{"sortkey":"","hidden":"","*":"Wikipedia_articles_needing_clarification_from_December_2017"},{"sortkey":"","hidden":"","*":"All_articles_with_specifically_marked_weasel-worded_phrases"},{"sortkey":"","hidden":"","*":"Articles_with_specifically_marked_weasel-worded_phrases_from_June_2014"},{"sortkey":"","hidden":"","*":"All_articles_with_unsourced_statements"},{"sortkey":"","hidden":"","*":"Articles_with_unsourced_statements_from_November_2011"},{"sortkey":"","hidden":"","*":"Use_dmy_dates_from_April_2017"},{"sortkey":"","*":"Natural_language_processing"},{"sortkey":"","*":"Computational_linguistics"},{"sortkey":"","*":"Language_modeling"},{"sortkey":"","*":"Speech_recognition"},{"sortkey":"","*":"Corpus_linguistics"},{"sortkey":"","*":"Probabilistic_models"}],"links":[{"ns":14,"exists":"","*":"Category:Articles lacking in-text citations from February 2011"},{"ns":14,"exists":"","*":"Category:Wikipedia articles needing clarification from December 2017"},{"ns":14,"exists":"","*":"Category:Articles with specifically marked weasel-worded phrases from June 2014"},{"ns":14,"exists":"","*":"Category:Articles with unsourced statements from November 2011"},{"ns":14,"exists":"","*":"Category:Use dmy dates from April 2017"},{"ns":0,"exists":"","*":"N-gram"},{"ns":0,"exists":"","*":"AI-complete"},{"ns":0,"exists":"","*":"Additive smoothing"},{"ns":0,"exists":"","*":"Amino acid"},{"ns":0,"exists":"","*":"Approximate string matching"},{"ns":0,"exists":"","*":"Automated essay scoring"},{"ns":0,"exists":"","*":"Automated online assistant"},{"ns":0,"exists":"","*":"Automatic identification and data capture"},{"ns":0,"exists":"","*":"Automatic summarization"},{"ns":0,"exists":"","*":"BLAST"},{"ns":0,"exists":"","*":"Bag-of-words model"},{"ns":0,"exists":"","*":"Base pair"},{"ns":0,"exists":"","*":"Base pairs"},{"ns":0,"exists":"","*":"Bayesian inference"},{"ns":0,"exists":"","*":"Bigram"},{"ns":0,"exists":"","*":"Cardinal number (linguistics)"},{"ns":0,"exists":"","*":"Categorical distribution"},{"ns":0,"exists":"","*":"Character (computing)"},{"ns":0,"exists":"","*":"Character (symbol)"},{"ns":0,"exists":"","*":"Chatbot"},{"ns":0,"exists":"","*":"Claude Shannon"},{"ns":0,"exists":"","*":"Collocation"},{"ns":0,"exists":"","*":"Collocation extraction"},{"ns":0,"exists":"","*":"Communication theory"},{"ns":0,"exists":"","*":"Compound term processing"},{"ns":0,"exists":"","*":"Computational biology"},{"ns":0,"exists":"","*":"Computational linguistics"},{"ns":0,"exists":"","*":"Computer-assisted reviewing"},{"ns":0,"exists":"","*":"Computer-assisted translation"},{"ns":0,"exists":"","*":"Concept mining"},{"ns":0,"exists":"","*":"Concordancer"},{"ns":0,"exists":"","*":"Coreference"},{"ns":0,"exists":"","*":"Corpus of Contemporary American English"},{"ns":0,"exists":"","*":"Cosine similarity"},{"ns":0,"exists":"","*":"Cryptanalysis"},{"ns":0,"exists":"","*":"DNA sequencing"},{"ns":0,"exists":"","*":"Data compression"},{"ns":0,"exists":"","*":"Dependency grammar"},{"ns":0,"exists":"","*":"Digital object identifier"},{"ns":0,"exists":"","*":"Dissociated press"},{"ns":0,"exists":"","*":"Engram (disambiguation)"},{"ns":0,"exists":"","*":"Example-based machine translation"},{"ns":0,"exists":"","*":"Feature engineering"},{"ns":0,"exists":"","*":"Feature extraction"},{"ns":0,"exists":"","*":"G-test"},{"ns":0,"exists":"","*":"Good\u2013Turing discounting"},{"ns":0,"exists":"","*":"Good\u2013Turing frequency estimation"},{"ns":0,"exists":"","*":"Google Ngram Viewer"},{"ns":0,"exists":"","*":"Grammar checker"},{"ns":0,"exists":"","*":"Grapheme"},{"ns":0,"exists":"","*":"Greek numerical prefixes"},{"ns":0,"exists":"","*":"Hidden Markov model"},{"ns":0,"exists":"","*":"Information retrieval"},{"ns":0,"exists":"","*":"Information theory"},{"ns":0,"exists":"","*":"Intelligent Character Recognition"},{"ns":0,"exists":"","*":"Intelligent character recognition"},{"ns":0,"exists":"","*":"Interactive fiction"},{"ns":0,"exists":"","*":"International Standard Book Number"},{"ns":0,"exists":"","*":"K-mer"},{"ns":0,"exists":"","*":"Katz's back-off model"},{"ns":0,"exists":"","*":"Kernel trick"},{"ns":0,"exists":"","*":"Kneser\u2013Ney smoothing"},{"ns":0,"exists":"","*":"Language identification"},{"ns":0,"exists":"","*":"Language model"},{"ns":0,"exists":"","*":"Latent Dirichlet allocation"},{"ns":0,"exists":"","*":"Latent semantic analysis"},{"ns":0,"exists":"","*":"Latin numerical prefixes"},{"ns":0,"exists":"","*":"Lemmatisation"},{"ns":0,"exists":"","*":"Letter (alphabet)"},{"ns":0,"exists":"","*":"Likelihood"},{"ns":0,"exists":"","*":"Likelihood function"},{"ns":0,"exists":"","*":"Linear interpolation"},{"ns":0,"exists":"","*":"Longest common substring problem"},{"ns":0,"exists":"","*":"Machine learning"},{"ns":0,"exists":"","*":"Machine translation"},{"ns":0,"exists":"","*":"Markov chain"},{"ns":0,"exists":"","*":"Markov model"},{"ns":0,"exists":"","*":"MinHash"},{"ns":0,"exists":"","*":"Multi-document summarization"},{"ns":0,"exists":"","*":"Multinomial distribution"},{"ns":0,"exists":"","*":"N-tuple"},{"ns":0,"exists":"","*":"Named-entity recognition"},{"ns":0,"exists":"","*":"Natural language generation"},{"ns":0,"exists":"","*":"Natural language processing"},{"ns":0,"exists":"","*":"Natural language understanding"},{"ns":0,"exists":"","*":"Natural language user interface"},{"ns":0,"exists":"","*":"Oligomer"},{"ns":0,"exists":"","*":"Ontology learning"},{"ns":0,"exists":"","*":"Optical character recognition"},{"ns":0,"exists":"","*":"PPM compression algorithm"},{"ns":0,"exists":"","*":"Pachinko allocation"},{"ns":0,"exists":"","*":"Parsing"},{"ns":0,"exists":"","*":"Part-of-speech tagging"},{"ns":0,"exists":"","*":"Phoneme"},{"ns":0,"exists":"","*":"Phonemes"},{"ns":0,"exists":"","*":"Plagiarism detection"},{"ns":0,"exists":"","*":"Polymer"},{"ns":0,"exists":"","*":"Posterior distribution"},{"ns":0,"exists":"","*":"Predictive text"},{"ns":0,"exists":"","*":"Prior distribution"},{"ns":0,"exists":"","*":"Probability"},{"ns":0,"exists":"","*":"Probability distribution"},{"ns":0,"exists":"","*":"Protein sequencing"},{"ns":0,"exists":"","*":"Pseudocount"},{"ns":0,"exists":"","*":"Question answering"},{"ns":0,"exists":"","*":"Rule-based machine translation"},{"ns":0,"exists":"","*":"Rule of succession"},{"ns":0,"exists":"","*":"Sample (statistics)"},{"ns":0,"exists":"","*":"Sentence extraction"},{"ns":0,"exists":"","*":"Sentiment analysis"},{"ns":0,"exists":"","*":"Sequence analysis"},{"ns":0,"exists":"","*":"Shallow parsing"},{"ns":0,"exists":"","*":"Smoothing"},{"ns":0,"exists":"","*":"Space\u2013time tradeoff"},{"ns":0,"exists":"","*":"Speech corpus"},{"ns":0,"exists":"","*":"Speech recognition"},{"ns":0,"exists":"","*":"Speech synthesis"},{"ns":0,"exists":"","*":"Spell checker"},{"ns":0,"exists":"","*":"Statistical machine translation"},{"ns":0,"exists":"","*":"Stemming"},{"ns":0,"exists":"","*":"Stop words"},{"ns":0,"exists":"","*":"String kernel"},{"ns":0,"exists":"","*":"Support vector machine"},{"ns":0,"exists":"","*":"Syllable"},{"ns":0,"exists":"","*":"Syntax guessing"},{"ns":0,"exists":"","*":"Terminology extraction"},{"ns":0,"exists":"","*":"Text corpus"},{"ns":0,"exists":"","*":"Text mining"},{"ns":0,"exists":"","*":"Text segmentation"},{"ns":0,"exists":"","*":"Text simplification"},{"ns":0,"exists":"","*":"Topic model"},{"ns":0,"exists":"","*":"Trigram"},{"ns":0,"exists":"","*":"Truecasing"},{"ns":0,"exists":"","*":"Vector space"},{"ns":0,"exists":"","*":"Voice user interface"},{"ns":0,"exists":"","*":"Weighted mean"},{"ns":0,"exists":"","*":"Wh-movement"},{"ns":0,"exists":"","*":"Whitespace character"},{"ns":0,"exists":"","*":"Word"},{"ns":0,"exists":"","*":"Word-sense disambiguation"},{"ns":0,"exists":"","*":"Z-score"},{"ns":0,"*":"G-score"},{"ns":0,"*":"Witten\u2013Bell discounting"},{"ns":0,"*":"Data sparsity problem"},{"ns":10,"exists":"","*":"Template:Natural Language Processing"},{"ns":4,"exists":"","*":"Wikipedia:Citation needed"},{"ns":4,"exists":"","*":"Wikipedia:Citing sources"},{"ns":4,"exists":"","*":"Wikipedia:Manual of Style/Words to watch"},{"ns":4,"exists":"","*":"Wikipedia:Please clarify"},{"ns":4,"exists":"","*":"Wikipedia:When to cite"},{"ns":4,"exists":"","*":"Wikipedia:WikiProject Fact and Reference Check"},{"ns":11,"exists":"","*":"Template talk:Natural Language Processing"},{"ns":12,"exists":"","*":"Help:Maintenance template removal"}],"templates":[{"ns":10,"exists":"","*":"Template:About"},{"ns":10,"exists":"","*":"Template:Distinguish"},{"ns":10,"exists":"","*":"Template:More footnotes"},{"ns":10,"exists":"","*":"Template:Ambox"},{"ns":10,"exists":"","*":"Template:Yesno-no"},{"ns":10,"exists":"","*":"Template:Yesno"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":10,"exists":"","*":"Template:Mvar"},{"ns":10,"exists":"","*":"Template:Clarify"},{"ns":10,"exists":"","*":"Template:Fix-span"},{"ns":10,"exists":"","*":"Template:Category handler"},{"ns":10,"exists":"","*":"Template:Fix/category"},{"ns":10,"exists":"","*":"Template:Replace"},{"ns":10,"exists":"","*":"Template:Delink"},{"ns":10,"exists":"","*":"Template:Who"},{"ns":10,"exists":"","*":"Template:Fix"},{"ns":10,"exists":"","*":"Template:Citation needed"},{"ns":10,"exists":"","*":"Template:Main"},{"ns":10,"exists":"","*":"Template:Math"},{"ns":10,"exists":"","*":"Template:Natural Language Processing"},{"ns":10,"exists":"","*":"Template:Navbox"},{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Cite journal"},{"ns":10,"exists":"","*":"Template:Cite web"},{"ns":10,"exists":"","*":"Template:Cite conference"},{"ns":10,"exists":"","*":"Template:ISBN"},{"ns":10,"exists":"","*":"Template:Catalog lookup link"},{"ns":10,"exists":"","*":"Template:Trim"},{"ns":10,"exists":"","*":"Template:Error-small"},{"ns":10,"exists":"","*":"Template:Tl"},{"ns":10,"exists":"","*":"Template:Use dmy dates"},{"ns":10,"exists":"","*":"Template:DMCA"},{"ns":10,"exists":"","*":"Template:Dated maintenance category"},{"ns":10,"exists":"","*":"Template:FULLROOTPAGENAME"},{"ns":10,"exists":"","*":"Template:Ns has subpages"},{"ns":828,"exists":"","*":"Module:About"},{"ns":828,"exists":"","*":"Module:Hatnote"},{"ns":828,"exists":"","*":"Module:Hatnote list"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Pagetype"},{"ns":828,"exists":"","*":"Module:Pagetype/config"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Namespace detect"},{"ns":828,"exists":"","*":"Module:Namespace detect/data"},{"ns":828,"exists":"","*":"Module:Namespace detect/config"},{"ns":828,"exists":"","*":"Module:Distinguish"},{"ns":828,"exists":"","*":"Module:TableTools"},{"ns":828,"exists":"","*":"Module:Unsubst"},{"ns":828,"exists":"","*":"Module:Message box"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:Message box/configuration"},{"ns":828,"exists":"","*":"Module:Category handler"},{"ns":828,"exists":"","*":"Module:Category handler/data"},{"ns":828,"exists":"","*":"Module:Category handler/config"},{"ns":828,"exists":"","*":"Module:Category handler/shared"},{"ns":828,"exists":"","*":"Module:Category handler/blacklist"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"},{"ns":828,"exists":"","*":"Module:String"},{"ns":828,"exists":"","*":"Module:Delink"},{"ns":828,"exists":"","*":"Module:Main"},{"ns":828,"exists":"","*":"Module:Navbox"},{"ns":828,"exists":"","*":"Module:Navbar"},{"ns":828,"exists":"","*":"Module:Citation/CS1"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Configuration"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Whitelist"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Utilities"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Date validation"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Identifiers"},{"ns":828,"exists":"","*":"Module:Citation/CS1/COinS"},{"ns":828,"exists":"","*":"Module:Check isxn"},{"ns":828,"exists":"","*":"Module:Error"},{"ns":828,"exists":"","*":"Module:Ns has subpages"}],"images":["Text_document_with_red_question_mark.svg"],"externallinks":["//doi.org/10.1016/s0169-7552(97)00031-7","http://googleresearch.blogspot.com/2006/08/all-our-n-gram-are-belong-to-you.html","http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.48.1958","//doi.org/10.1109/ICDAR.1997.619847","//doi.org/10.1016/j.cmpb.2005.11.007","http://citeseerx.ist.psu.edu/viewdoc/summary?doi=10.1.1.45.1629","http://homepages.inf.ed.ac.uk/ballison/pdf/lrec_skipgrams.pdf","https://www.researchgate.net/publication/262176888_Contextual_Language_Models_For_Ranking_Answers_To_Natural_Language_Definition_Questions","//doi.org/10.1111/j.1467-8640.2012.00426.x","//doi.org/10.1016/j.eswa.2013.08.015","http://main.cl-lab.dk/www/publications/2008/pdf/part-of-speech-n-grams-and-information-retrieval.pdf","https://www.coursera.org/learn/natural-language-processing/lecture/UnEHs/07-01-noisy-channel-model-8-33","//doi.org/10.1093/nar/21.16.3829","https://www.uvic.ca/engineering/ece/isot/assets/docs/Authorship_Verification_for_Short_Messages_using_Stylometry.pdf","https://books.google.com/ngrams","http://research.microsoft.com/web-ngram","http://data.statoperator.com/","http://www.ngrams.info/","http://www.peachnote.com/","http://www.w3.org/TR/ngram-spec/","http://www.cs.columbia.edu/~mcollins/courses/nlp2011/notes/lm.pdf","https://github.com/OpenRefine/OpenRefine/wiki/Clustering-In-Depth/"],"sections":[{"toclevel":1,"level":"2","line":"Applications","number":"1","index":"1","fromtitle":"N-gram","byteoffset":1678,"anchor":"Applications"},{"toclevel":1,"level":"2","line":"Examples","number":"2","index":"2","fromtitle":"N-gram","byteoffset":2533,"anchor":"Examples"},{"toclevel":1,"level":"2","line":"<i>n</i>-gram models","number":"3","index":"3","fromtitle":"N-gram","byteoffset":4698,"anchor":"n-gram_models"},{"toclevel":1,"level":"2","line":"Applications and considerations","number":"4","index":"4","fromtitle":"N-gram","byteoffset":6480,"anchor":"Applications_and_considerations"},{"toclevel":2,"level":"3","line":"Out-of-vocabulary words","number":"4.1","index":"5","fromtitle":"N-gram","byteoffset":12111,"anchor":"Out-of-vocabulary_words"},{"toclevel":1,"level":"2","line":"<i>n</i>-grams for approximate matching","number":"5","index":"6","fromtitle":"N-gram","byteoffset":13844,"anchor":"n-grams_for_approximate_matching"},{"toclevel":1,"level":"2","line":"Other applications","number":"6","index":"7","fromtitle":"N-gram","byteoffset":15671,"anchor":"Other_applications"},{"toclevel":1,"level":"2","line":"Bias-versus-variance trade-off","number":"7","index":"8","fromtitle":"N-gram","byteoffset":17085,"anchor":"Bias-versus-variance_trade-off"},{"toclevel":2,"level":"3","line":"Smoothing techniques","number":"7.1","index":"9","fromtitle":"N-gram","byteoffset":17453,"anchor":"Smoothing_techniques"},{"toclevel":2,"level":"3","line":"Skip-gram","number":"7.2","index":"10","fromtitle":"N-gram","byteoffset":19214,"anchor":"Skip-gram"},{"toclevel":1,"level":"2","line":"Syntactic <i>n</i>-grams","number":"8","index":"11","fromtitle":"N-gram","byteoffset":20790,"anchor":"Syntactic_n-grams"},{"toclevel":1,"level":"2","line":"See also","number":"9","index":"12","fromtitle":"N-gram","byteoffset":23759,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"References","number":"10","index":"13","fromtitle":"N-gram","byteoffset":23964,"anchor":"References"},{"toclevel":1,"level":"2","line":"External links","number":"11","index":"14","fromtitle":"N-gram","byteoffset":25545,"anchor":"External_links"}],"parsewarnings":[],"displaytitle":"<i>n</i>-gram","iwlinks":[],"properties":[{"name":"displaytitle","*":"<i>n</i>-gram"},{"name":"wikibase_item","*":"Q94489"}]}}