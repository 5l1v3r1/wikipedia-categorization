{"parse":{"title":"Grammar induction","pageid":4375576,"revid":859446382,"text":{"*":"<div class=\"mw-parser-output\"><table class=\"vertical-navbox nowraplinks\" style=\"float:right;clear:right;width:22.0em;margin:0 0 1.0em 1.0em;background:#f9f9f9;border:1px solid #aaa;padding:0.2em;border-spacing:0.4em 0;text-align:center;line-height:1.4em;font-size:88%\"><tbody><tr><th style=\"padding:0.2em 0.4em 0.2em;font-size:145%;line-height:1.2em\"><a href=\"/wiki/Machine_learning\" title=\"Machine learning\">Machine learning</a> and<br /><a href=\"/wiki/Data_mining\" title=\"Data mining\">data mining</a></th></tr><tr><td style=\"padding:0.2em 0 0.4em;padding:0.25em 0.25em 0.75em;\"><a href=\"/wiki/File:Kernel_Machine.svg\" class=\"image\"><img alt=\"Kernel Machine.svg\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/220px-Kernel_Machine.svg.png\" width=\"220\" height=\"100\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/330px-Kernel_Machine.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Kernel_Machine.svg/440px-Kernel_Machine.svg.png 2x\" data-file-width=\"512\" data-file-height=\"233\" /></a></td></tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\">Problems</div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Statistical_classification\" title=\"Statistical classification\">Classification</a></li>\n<li><a href=\"/wiki/Cluster_analysis\" title=\"Cluster analysis\">Clustering</a></li>\n<li><a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">Regression</a></li>\n<li><a href=\"/wiki/Anomaly_detection\" title=\"Anomaly detection\">Anomaly detection</a></li>\n<li><a href=\"/wiki/Automated_machine_learning\" title=\"Automated machine learning\">AutoML</a></li>\n<li><a href=\"/wiki/Association_rule_learning\" title=\"Association rule learning\">Association rules</a></li>\n<li><a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">Reinforcement learning</a></li>\n<li><a href=\"/wiki/Structured_prediction\" title=\"Structured prediction\">Structured prediction</a></li>\n<li><a href=\"/wiki/Feature_engineering\" title=\"Feature engineering\">Feature engineering</a></li>\n<li><a href=\"/wiki/Feature_learning\" title=\"Feature learning\">Feature learning</a></li>\n<li><a href=\"/wiki/Online_machine_learning\" title=\"Online machine learning\">Online learning</a></li>\n<li><a href=\"/wiki/Semi-supervised_learning\" title=\"Semi-supervised learning\">Semi-supervised learning</a></li>\n<li><a href=\"/wiki/Unsupervised_learning\" title=\"Unsupervised learning\">Unsupervised learning</a></li>\n<li><a href=\"/wiki/Learning_to_rank\" title=\"Learning to rank\">Learning to rank</a></li>\n<li><a class=\"mw-selflink selflink\">Grammar induction</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\"><div style=\"padding:0.1em 0;line-height:1.2em;\"><a href=\"/wiki/Supervised_learning\" title=\"Supervised learning\">Supervised learning</a><br /><span style=\"font-weight:normal;\"><span style=\"font-size:85%;\">(<b><a href=\"/wiki/Statistical_classification\" title=\"Statistical classification\">classification</a></b>&#160;&#8226;&#32;<b><a href=\"/wiki/Regression_analysis\" title=\"Regression analysis\">regression</a></b>)</span></span> </div></div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Decision_tree_learning\" title=\"Decision tree learning\">Decision trees</a></li>\n<li><a href=\"/wiki/Ensemble_learning\" title=\"Ensemble learning\">Ensembles</a> (<a href=\"/wiki/Bootstrap_aggregating\" title=\"Bootstrap aggregating\">Bagging</a>, <a href=\"/wiki/Boosting_(machine_learning)\" title=\"Boosting (machine learning)\">Boosting</a>, <a href=\"/wiki/Random_forest\" title=\"Random forest\">Random forest</a>)</li>\n<li><a href=\"/wiki/K-nearest_neighbors_algorithm\" title=\"K-nearest neighbors algorithm\"><i>k</i>-NN</a></li>\n<li><a href=\"/wiki/Linear_regression\" title=\"Linear regression\">Linear regression</a></li>\n<li><a href=\"/wiki/Naive_Bayes_classifier\" title=\"Naive Bayes classifier\">Naive Bayes</a></li>\n<li><a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">Neural networks</a></li>\n<li><a href=\"/wiki/Logistic_regression\" title=\"Logistic regression\">Logistic regression</a></li>\n<li><a href=\"/wiki/Perceptron\" title=\"Perceptron\">Perceptron</a></li>\n<li><a href=\"/wiki/Relevance_vector_machine\" title=\"Relevance vector machine\">Relevance vector machine (RVM)</a></li>\n<li><a href=\"/wiki/Support_vector_machine\" title=\"Support vector machine\">Support vector machine (SVM)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\"><a href=\"/wiki/Cluster_analysis\" title=\"Cluster analysis\">Clustering</a></div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/BIRCH\" title=\"BIRCH\">BIRCH</a></li>\n<li><a href=\"/wiki/CURE_data_clustering_algorithm\" class=\"mw-redirect\" title=\"CURE data clustering algorithm\">CURE</a></li>\n<li><a href=\"/wiki/Hierarchical_clustering\" title=\"Hierarchical clustering\">Hierarchical</a></li>\n<li><a href=\"/wiki/K-means_clustering\" title=\"K-means clustering\"><i>k</i>-means</a></li>\n<li><a href=\"/wiki/Expectation%E2%80%93maximization_algorithm\" title=\"Expectation\u2013maximization algorithm\">Expectation\u2013maximization (EM)</a></li>\n<li><br /><a href=\"/wiki/DBSCAN\" title=\"DBSCAN\">DBSCAN</a></li>\n<li><a href=\"/wiki/OPTICS_algorithm\" title=\"OPTICS algorithm\">OPTICS</a></li>\n<li><a href=\"/wiki/Mean-shift\" class=\"mw-redirect\" title=\"Mean-shift\">Mean-shift</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\"><a href=\"/wiki/Dimensionality_reduction\" title=\"Dimensionality reduction\">Dimensionality reduction</a></div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Factor_analysis\" title=\"Factor analysis\">Factor analysis</a></li>\n<li><a href=\"/wiki/Canonical_correlation_analysis\" class=\"mw-redirect\" title=\"Canonical correlation analysis\">CCA</a></li>\n<li><a href=\"/wiki/Independent_component_analysis\" title=\"Independent component analysis\">ICA</a></li>\n<li><a href=\"/wiki/Linear_discriminant_analysis\" title=\"Linear discriminant analysis\">LDA</a></li>\n<li><a href=\"/wiki/Non-negative_matrix_factorization\" title=\"Non-negative matrix factorization\">NMF</a></li>\n<li><a href=\"/wiki/Principal_component_analysis\" title=\"Principal component analysis\">PCA</a></li>\n<li><a href=\"/wiki/T-distributed_stochastic_neighbor_embedding\" title=\"T-distributed stochastic neighbor embedding\">t-SNE</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\"><a href=\"/wiki/Structured_prediction\" title=\"Structured prediction\">Structured prediction</a></div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Graphical_model\" title=\"Graphical model\">Graphical models</a> (<a href=\"/wiki/Bayesian_network\" title=\"Bayesian network\">Bayes net</a>, <a href=\"/wiki/Conditional_random_field\" title=\"Conditional random field\">CRF</a>, <a href=\"/wiki/Hidden_Markov_model\" title=\"Hidden Markov model\">HMM</a>)</li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\"><a href=\"/wiki/Anomaly_detection\" title=\"Anomaly detection\">Anomaly detection</a></div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/K-nearest_neighbors_classification\" class=\"mw-redirect\" title=\"K-nearest neighbors classification\"><i>k</i>-NN</a></li>\n<li><a href=\"/wiki/Local_outlier_factor\" title=\"Local outlier factor\">Local outlier factor</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\"><a href=\"/wiki/Artificial_neural_network\" title=\"Artificial neural network\">Neural nets</a></div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Autoencoder\" title=\"Autoencoder\">Autoencoder</a></li>\n<li><a href=\"/wiki/Deep_learning\" title=\"Deep learning\">Deep learning</a></li>\n<li><a href=\"/wiki/Multilayer_perceptron\" title=\"Multilayer perceptron\">Multilayer perceptron</a></li>\n<li><a href=\"/wiki/Recurrent_neural_network\" title=\"Recurrent neural network\">RNN</a> (<a href=\"/wiki/Long_short-term_memory\" title=\"Long short-term memory\">LSTM</a>, <a href=\"/wiki/Gated_recurrent_unit\" title=\"Gated recurrent unit\">GRU</a>)</li>\n<li><a href=\"/wiki/Restricted_Boltzmann_machine\" title=\"Restricted Boltzmann machine\">Restricted Boltzmann machine</a></li>\n<li><a href=\"/wiki/Self-organizing_map\" title=\"Self-organizing map\">SOM</a></li>\n<li><a href=\"/wiki/Convolutional_neural_network\" title=\"Convolutional neural network\">Convolutional neural network</a> (<a href=\"/wiki/U-Net\" title=\"U-Net\">U-Net</a>)</li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\"><a href=\"/wiki/Reinforcement_learning\" title=\"Reinforcement learning\">Reinforcement learning</a></div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Q-learning\" title=\"Q-learning\">Q-learning</a></li>\n<li><a href=\"/wiki/State%E2%80%93action%E2%80%93reward%E2%80%93state%E2%80%93action\" title=\"State\u2013action\u2013reward\u2013state\u2013action\">SARSA</a></li>\n<li><a href=\"/wiki/Temporal_difference_learning\" title=\"Temporal difference learning\">Temporal difference (TD)</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\">Theory</div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Bias-variance_dilemma\" class=\"mw-redirect\" title=\"Bias-variance dilemma\">Bias-variance dilemma</a></li>\n<li><a href=\"/wiki/Computational_learning_theory\" title=\"Computational learning theory\">Computational learning theory</a></li>\n<li><a href=\"/wiki/Empirical_risk_minimization\" title=\"Empirical risk minimization\">Empirical risk minimization</a></li>\n<li><a href=\"/wiki/Occam_learning\" title=\"Occam learning\">Occam learning</a></li>\n<li><a href=\"/wiki/Probably_approximately_correct_learning\" title=\"Probably approximately correct learning\">PAC learning</a></li>\n<li><a href=\"/wiki/Statistical_learning_theory\" title=\"Statistical learning theory\">Statistical learning</a></li>\n<li><a href=\"/wiki/Vapnik%E2%80%93Chervonenkis_theory\" title=\"Vapnik\u2013Chervonenkis theory\">VC theory</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\">Machine-learning venues</div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Conference_on_Neural_Information_Processing_Systems\" title=\"Conference on Neural Information Processing Systems\">NIPS</a></li>\n<li><a href=\"/wiki/International_Conference_on_Machine_Learning\" title=\"International Conference on Machine Learning\">ICML</a></li>\n<li><a href=\"/wiki/Machine_Learning_(journal)\" title=\"Machine Learning (journal)\">ML</a></li>\n<li><a href=\"/wiki/Journal_of_Machine_Learning_Research\" title=\"Journal of Machine Learning Research\">JMLR</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/list/cs.LG/recent\">ArXiv:cs.LG</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\"><a href=\"/wiki/Glossary_of_artificial_intelligence\" title=\"Glossary of artificial intelligence\">Glossary of artificial intelligence</a></div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/Glossary_of_artificial_intelligence\" title=\"Glossary of artificial intelligence\">Glossary of artificial intelligence</a></li></ul>\n</div></div></div></td>\n</tr><tr><td style=\"padding:0 0.1em 0.4em\">\n<div class=\"NavFrame collapsed\" style=\"border:none;padding:0\"><div class=\"NavHead\" style=\"font-size:105%;background:transparent;text-align:left\">Related articles</div><div class=\"NavContent\" style=\"font-size:105%;padding:0.2em 0 0.4em;text-align:center\"><div class=\"hlist\">\n<ul><li><a href=\"/wiki/List_of_datasets_for_machine-learning_research\" class=\"mw-redirect\" title=\"List of datasets for machine-learning research\">List of datasets for machine-learning research</a></li>\n<li><a href=\"/wiki/Outline_of_machine_learning\" title=\"Outline of machine learning\">Outline of machine learning</a></li></ul>\n</div></div></div></td>\n</tr><tr><td class=\"plainlist\" style=\"padding:0.3em 0.4em 0.3em;font-weight:bold;border-top: 1px solid #aaa; border-bottom: 1px solid #aaa;border-top:1px solid #aaa;border-bottom:1px solid #aaa;\">\n<ul><li><a href=\"/wiki/File:Portal-puzzle.svg\" class=\"image\"><img alt=\"Portal-puzzle.svg\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/16px-Portal-puzzle.svg.png\" width=\"16\" height=\"14\" class=\"noviewer\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/24px-Portal-puzzle.svg.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/f/fd/Portal-puzzle.svg/32px-Portal-puzzle.svg.png 2x\" data-file-width=\"32\" data-file-height=\"28\" /></a> <a href=\"/wiki/Portal:Machine_learning\" title=\"Portal:Machine learning\">Machine learning&#32;portal</a></li></ul></td></tr><tr><td style=\"text-align:right;font-size:115%;padding-top: 0.6em;\"><div class=\"plainlinks hlist navbar mini\"><ul><li class=\"nv-view\"><a href=\"/wiki/Template:Machine_learning_bar\" title=\"Template:Machine learning bar\"><abbr title=\"View this template\">v</abbr></a></li><li class=\"nv-talk\"><a href=\"/wiki/Template_talk:Machine_learning_bar\" title=\"Template talk:Machine learning bar\"><abbr title=\"Discuss this template\">t</abbr></a></li><li class=\"nv-edit\"><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Template:Machine_learning_bar&amp;action=edit\"><abbr title=\"Edit this template\">e</abbr></a></li></ul></div></td></tr></tbody></table>\n<p><b>Grammar induction</b> (or <b>grammatical inference</b><sup id=\"cite_ref-Grammatical_Inference_1-0\" class=\"reference\"><a href=\"#cite_note-Grammatical_Inference-1\">&#91;1&#93;</a></sup>) is the process in <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a> of learning a <a href=\"/wiki/Formal_grammar\" title=\"Formal grammar\">formal grammar</a> (usually as a collection of <i>re-write rules</i> or <i><a href=\"/wiki/Productions_(computer_science)\" class=\"mw-redirect\" title=\"Productions (computer science)\">productions</a></i> or alternatively as a <a href=\"/wiki/Finite_state_machine\" class=\"mw-redirect\" title=\"Finite state machine\">finite state machine</a> or automaton of some kind) from a set of observations, thus constructing a model which accounts for the characteristics of the observed objects. More generally, grammatical inference is that branch of machine learning where the instance space consists of discrete combinatorial objects such as strings, trees and graphs.\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Grammar_classes\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Grammar classes</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Learning_models\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Learning models</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Methodologies\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Methodologies</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Grammatical_inference_by_trial-and-error\"><span class=\"tocnumber\">3.1</span> <span class=\"toctext\">Grammatical inference by trial-and-error</span></a></li>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"#Grammatical_inference_by_genetic_algorithms\"><span class=\"tocnumber\">3.2</span> <span class=\"toctext\">Grammatical inference by genetic algorithms</span></a></li>\n<li class=\"toclevel-2 tocsection-6\"><a href=\"#Grammatical_inference_by_greedy_algorithms\"><span class=\"tocnumber\">3.3</span> <span class=\"toctext\">Grammatical inference by greedy algorithms</span></a></li>\n<li class=\"toclevel-2 tocsection-7\"><a href=\"#Distributional_learning\"><span class=\"tocnumber\">3.4</span> <span class=\"toctext\">Distributional learning</span></a></li>\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Learning_of_pattern_languages\"><span class=\"tocnumber\">3.5</span> <span class=\"toctext\">Learning of pattern languages</span></a></li>\n<li class=\"toclevel-2 tocsection-9\"><a href=\"#Pattern_theory\"><span class=\"tocnumber\">3.6</span> <span class=\"toctext\">Pattern theory</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-10\"><a href=\"#Applications\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Applications</span></a></li>\n<li class=\"toclevel-1 tocsection-11\"><a href=\"#See_also\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-12\"><a href=\"#Notes\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">Notes</span></a></li>\n<li class=\"toclevel-1 tocsection-13\"><a href=\"#References\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-14\"><a href=\"#Sources\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">Sources</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Grammar_classes\">Grammar classes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=1\" title=\"Edit section: Grammar classes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Grammatical inference has often been very focused on the problem of learning finite state machines of various types (see the article <a href=\"/wiki/Induction_of_regular_languages\" title=\"Induction of regular languages\">Induction of regular languages</a> for details on these approaches), since there have been efficient algorithms for this problem since the 1980s.\n</p><p>Since the beginning of the century, these approaches have been extended to the problem of inference of <a href=\"/wiki/Context-free_grammars\" class=\"mw-redirect\" title=\"Context-free grammars\">context-free grammars</a> and richer formalisms, such as multiple context-free grammars and parallel multiple context-free grammars.\nOther classes of grammars for which grammatical inference has been studied are contextual grammars and pattern languages.\n</p>\n<h2><span class=\"mw-headline\" id=\"Learning_models\">Learning models</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=2\" title=\"Edit section: Learning models\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The simplest form of learning is where the learning algorithm merely receives a set of examples drawn from the language in question: the aim is to learn the language from examples of it (and, rarely, from counter-examples, that is, example that do not belong to the language).\nHowever, other learning models have been studied. One frequently studied alternative is the case where the learner can ask membership queries as in the exact query learning model or minimally adequate teacher model introduced by Angluin<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup>.\n</p>\n<h2><span class=\"mw-headline\" id=\"Methodologies\">Methodologies</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=3\" title=\"Edit section: Methodologies\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>There is a wide variety of methods for grammatical inference.  Two of the classic sources are <a href=\"#CITEREFFu1977\">Fu (1977)</a> and <a href=\"#CITEREFFu1982\">Fu (1982)</a>. <a href=\"#CITEREFDudaHartStork2001\">Duda, Hart &amp; Stork (2001)</a> also devote a brief section to the problem, and cite a number of references.  The basic trial-and-error method they present is discussed below. For approaches to infer subclasses of <a href=\"/wiki/Regular_languages\" class=\"mw-redirect\" title=\"Regular languages\">regular languages</a> in particular, see <i><a href=\"/wiki/Induction_of_regular_languages\" title=\"Induction of regular languages\">Induction of regular languages</a></i>. A more recent textbook is de la Higuera (2010),<sup id=\"cite_ref-Grammatical_Inference_1-1\" class=\"reference\"><a href=\"#cite_note-Grammatical_Inference-1\">&#91;1&#93;</a></sup> which covers the theory of grammatical inference of regular languages and finite state automata. D'Ulizia, Ferri and Grifoni<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup> provide a survey that explores grammatical inference methods for natural languages.\n</p>\n<h3><span class=\"mw-headline\" id=\"Grammatical_inference_by_trial-and-error\">Grammatical inference by trial-and-error</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=4\" title=\"Edit section: Grammatical inference by trial-and-error\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The method proposed in Section 8.7 of <a href=\"#CITEREFDudaHartStork2001\">Duda, Hart &amp; Stork (2001)</a> suggests successively guessing grammar rules (productions) and testing them against positive and negative observations.  The rule set is expanded so as to be able to generate each positive example, but if a given rule set also generates a negative example, it must be discarded.  This particular approach can be characterized as \"hypothesis testing\" and bears some similarity to Mitchel's <a href=\"/wiki/Version_space\" class=\"mw-redirect\" title=\"Version space\">version space</a> algorithm. The <a href=\"#CITEREFDudaHartStork2001\">Duda, Hart &amp; Stork (2001)</a> text provide a simple example which nicely illustrates the process, but the feasibility of such an unguided trial-and-error approach for more substantial problems is dubious.\n</p>\n<h3><span class=\"mw-headline\" id=\"Grammatical_inference_by_genetic_algorithms\">Grammatical inference by genetic algorithms</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=5\" title=\"Edit section: Grammatical inference by genetic algorithms\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Grammatical induction using <a href=\"/wiki/Evolutionary_algorithm\" title=\"Evolutionary algorithm\">evolutionary algorithms</a> is the process of evolving a representation of the grammar of a target language through some evolutionary process. <a href=\"/wiki/Formal_grammar\" title=\"Formal grammar\">Formal grammars</a> can easily be represented as <a href=\"/wiki/Tree_(data_structure)\" title=\"Tree (data structure)\">tree structures</a> of production rules that can be subjected to evolutionary operators. <a href=\"/wiki/Algorithm\" title=\"Algorithm\">Algorithms</a> of this sort stem from the <a href=\"/wiki/Genetic_programming\" title=\"Genetic programming\">genetic programming</a> paradigm pioneered by <a href=\"/wiki/John_Koza\" title=\"John Koza\">John Koza</a>.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (August 2007)\">citation needed</span></a></i>&#93;</sup> Other early work on simple formal languages used the binary string representation of genetic algorithms, but the inherently hierarchical structure of grammars couched in the <a href=\"/wiki/Extended_Backus%E2%80%93Naur_form\" title=\"Extended Backus\u2013Naur form\">EBNF</a> language made trees a more flexible approach.\n</p><p>Koza represented <a href=\"/wiki/Lisp_(programming_language)\" title=\"Lisp (programming language)\">Lisp</a> programs as trees. He was able to find analogues to the genetic operators within the standard set of tree operators. For example, swapping sub-trees is equivalent to the corresponding process of genetic crossover, where sub-strings of a genetic code are transplanted into an individual of the next generation. Fitness is measured by scoring the output from the <a href=\"/wiki/Grammatical_function\" class=\"mw-redirect\" title=\"Grammatical function\">functions</a> of the Lisp code. Similar analogues between the tree structured lisp representation and the representation of grammars as trees, made the application of genetic programming techniques possible for grammar induction.\n</p><p>In the case of grammar induction, the transplantation of sub-trees corresponds to the swapping of production rules that enable the parsing of phrases from some language. The fitness operator for the grammar is based upon some measure of how well it performed in parsing some group of sentences from the target language. In a tree representation of a grammar, a <a href=\"/wiki/Terminal_symbol\" class=\"mw-redirect\" title=\"Terminal symbol\">terminal symbol</a> of a production rule corresponds to a leaf node of the tree. Its parent nodes corresponds to a non-terminal symbol (e.g. a <a href=\"/wiki/Noun_phrase\" title=\"Noun phrase\">noun phrase</a> or a <a href=\"/wiki/Verb_phrase\" title=\"Verb phrase\">verb phrase</a>) in the rule set. Ultimately, the root node might correspond to a sentence non-terminal.\n</p>\n<h3><span class=\"mw-headline\" id=\"Grammatical_inference_by_greedy_algorithms\">Grammatical inference by greedy algorithms</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=6\" title=\"Edit section: Grammatical inference by greedy algorithms\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Like all <a href=\"/wiki/Greedy_algorithm\" title=\"Greedy algorithm\">greedy algorithms</a>, greedy grammar inference algorithms make, in iterative manner, decisions that seem to be the best at that stage.\nThe decisions made usually deal with things like the creation of new rules, the removal of existing rules, the choice of a rule to be applied or the merging of some existing rules.\nBecause there are several ways to define 'the stage' and 'the best', there are also several greedy grammar inference algorithms.\n</p><p>These <a href=\"/wiki/Context-free_grammar\" title=\"Context-free grammar\">context-free grammar</a> generating algorithms make the decision after every read symbol:\n</p>\n<ul><li><a href=\"/wiki/LZW\" class=\"mw-redirect\" title=\"LZW\">Lempel-Ziv-Welch algorithm</a> creates a context-free grammar in a deterministic way such that it is necessary to store only the start rule of the generated grammar.</li>\n<li><a href=\"/wiki/Sequitur_algorithm\" title=\"Sequitur algorithm\">Sequitur</a> and its modifications.</li></ul>\n<p>These context-free grammar generating algorithms first read the whole given symbol-sequence and then start to make decisions:\n</p>\n<ul><li><a href=\"/wiki/Byte_pair_encoding\" title=\"Byte pair encoding\">Byte pair encoding</a> and its optimizations.</li></ul>\n<h3><span class=\"mw-headline\" id=\"Distributional_learning\">Distributional learning</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=7\" title=\"Edit section: Distributional learning\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>A more recent approach is based on distributional learning. Algorithms using these approaches have been applied to learning <a href=\"/wiki/Context-free_grammars\" class=\"mw-redirect\" title=\"Context-free grammars\">context-free grammars</a> and <a href=\"/wiki/Mildly_context-sensitive_language\" class=\"mw-redirect\" title=\"Mildly context-sensitive language\">mildly context-sensitive languages</a> and have been proven to be correct and efficient for large subclasses of these grammars.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\">&#91;4&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Learning_of_pattern_languages\">Learning of <a href=\"/wiki/Pattern_language_(formal_languages)\" title=\"Pattern language (formal languages)\">pattern languages</a></span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=8\" title=\"Edit section: Learning of pattern languages\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Angluin defines a <i>pattern</i> to be \"a string of constant symbols from \u03a3 and <b>variable symbols</b> from a disjoint set\".\nThe language of such a pattern is the set of all its nonempty ground instances  i.e. all strings resulting from consistent replacement of its variable symbols by nonempty strings of constant symbols.<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\">&#91;note 1&#93;</a></sup>\nA pattern is called <b>descriptive</b> for a finite input set of strings if its language is minimal (with respect to set inclusion) among all pattern languages subsuming the input set.\n</p><p>Angluin gives a polynomial algorithm to compute, for a given input string set, all descriptive patterns in one variable <i>x</i>.<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;note 2&#93;</a></sup>\nTo this end, she builds an automaton representing all possibly relevant patterns; using sophisticated arguments about word lengths, which rely on <i>x</i> being the only variable, the state count can be drastically reduced.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;5&#93;</a></sup>\n</p><p>Erlebach et al. give a more efficient version of Angluin's pattern learning algorithm, as well as a parallelized version.<sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;6&#93;</a></sup>\n</p><p>Arimura et al. show that a language class  obtained from limited unions of patterns can be learned in polynomial time.<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;7&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Pattern_theory\">Pattern theory</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=9\" title=\"Edit section: Pattern theory\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p><a href=\"/wiki/Pattern_theory\" title=\"Pattern theory\">Pattern theory</a>, formulated by <a href=\"/wiki/Ulf_Grenander\" title=\"Ulf Grenander\">Ulf Grenander</a>,<sup id=\"cite_ref-10\" class=\"reference\"><a href=\"#cite_note-10\">&#91;8&#93;</a></sup> is a mathematical <a href=\"/wiki/Formalism_(mathematics)\" class=\"mw-redirect\" title=\"Formalism (mathematics)\">formalism</a> to describe knowledge of the world as patterns. It differs from other approaches to <a href=\"/wiki/Artificial_intelligence\" title=\"Artificial intelligence\">artificial intelligence</a> in that it does not begin by prescribing algorithms and machinery to recognize and classify patterns; rather, it prescribes a vocabulary to articulate and recast the pattern concepts in precise language.\n</p><p>In addition to the new algebraic vocabulary, its statistical approach was novel in its aim to:\n</p>\n<ul><li>Identify the <a href=\"/wiki/Latent_variable\" title=\"Latent variable\">hidden variables</a> of a data set using real world data rather than artificial stimuli, which was commonplace at the time.</li>\n<li>Formulate prior distributions for hidden variables and models for the observed variables that form the vertices of a Gibbs-like graph.</li>\n<li>Study the randomness and variability of these graphs.</li>\n<li>Create the basic classes of stochastic models applied by listing the deformations of the patterns.</li>\n<li>Synthesize (sample) from the models, not just analyze signals with it.</li></ul>\n<p>Broad in its mathematical coverage, pattern theory spans algebra and statistics, as well as local topological and global entropic properties.\n</p>\n<h2><span class=\"mw-headline\" id=\"Applications\">Applications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=10\" title=\"Edit section: Applications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The principle of grammar induction has been applied to other aspects of <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a>, and has been applied (among many other problems) to <a href=\"/wiki/Natural_language_understanding\" title=\"Natural language understanding\">natural language understanding</a>,<sup id=\"cite_ref-11\" class=\"reference\"><a href=\"#cite_note-11\">&#91;9&#93;</a></sup> <a href=\"/wiki/Example-based_translation\" class=\"mw-redirect\" title=\"Example-based translation\">example-based translation</a>,<sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;10&#93;</a></sup> <a href=\"/wiki/Morpheme\" title=\"Morpheme\">morpheme</a> analysis, and place name derivations.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (February 2018)\">citation needed</span></a></i>&#93;</sup> Grammar induction has also been used for <a href=\"/wiki/Lossless_data_compression\" class=\"mw-redirect\" title=\"Lossless data compression\">lossless data compression</a><sup id=\"cite_ref-13\" class=\"reference\"><a href=\"#cite_note-13\">&#91;11&#93;</a></sup> and <a href=\"/wiki/Statistical_inference\" title=\"Statistical inference\">statistical inference</a> via <a href=\"/wiki/Minimum_message_length\" title=\"Minimum message length\">minimum message length</a> (MML) and <a href=\"/wiki/Minimum_description_length\" title=\"Minimum description length\">minimum description length</a> (MDL) principles.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (August 2017)\">citation needed</span></a></i>&#93;</sup> Grammar induction has also been used in some <a href=\"/wiki/Probabilistic_models_of_language_acquisition\" class=\"mw-redirect\" title=\"Probabilistic models of language acquisition\">probabilistic models of language acquisition</a>.<sup id=\"cite_ref-14\" class=\"reference\"><a href=\"#cite_note-14\">&#91;12&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=11\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Artificial_grammar_learning#Artificial_intelligence\" title=\"Artificial grammar learning\">Artificial grammar learning#Artificial intelligence</a></li>\n<li><a href=\"/wiki/Example-based_machine_translation\" title=\"Example-based machine translation\">Example-based machine translation</a></li>\n<li><a href=\"/wiki/Inductive_programming\" title=\"Inductive programming\">Inductive programming</a></li>\n<li><a href=\"/wiki/Kolmogorov_complexity\" title=\"Kolmogorov complexity\">Kolmogorov complexity</a></li>\n<li><a href=\"/wiki/Language_identification_in_the_limit\" title=\"Language identification in the limit\">Language identification in the limit</a></li>\n<li><a href=\"/wiki/Straight-line_grammar\" title=\"Straight-line grammar\">Straight-line grammar</a></li>\n<li><a href=\"/wiki/Syntactic_pattern_recognition\" title=\"Syntactic pattern recognition\">Syntactic pattern recognition</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"Notes\">Notes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=12\" title=\"Edit section: Notes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist\" style=\"list-style-type: decimal;\">\n<div class=\"mw-references-wrap\"><ol class=\"references\">\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\">The language of a pattern with at least two occurrences of the same variable is not regular due to the <a href=\"/wiki/Pumping_lemma_for_regular_languages\" title=\"Pumping lemma for regular languages\">pumping lemma</a>.</span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\"><i>x</i> may occur several times, but no other variable <i>y</i> may occur</span>\n</li>\n</ol></div></div>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=13\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist\" style=\"list-style-type: decimal;\">\n<div class=\"mw-references-wrap mw-references-columns\"><ol class=\"references\">\n<li id=\"cite_note-Grammatical_Inference-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Grammatical_Inference_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Grammatical_Inference_1-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation book\">de la Higuera, Colin (2010). <a rel=\"nofollow\" class=\"external text\" href=\"http://bootcamp.lif.univ-mrs.fr/de-la-higuera.pdf\"><i>Grammatical Inference: Learning Automata and Grammars</i></a> <span style=\"font-size:85%;\">(PDF)</span>. Cambridge: Cambridge University Press.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Grammatical+Inference%3A+Learning+Automata+and+Grammars&amp;rft.place=Cambridge&amp;rft.pub=Cambridge+University+Press&amp;rft.date=2010&amp;rft.aulast=de+la+Higuera&amp;rft.aufirst=Colin&amp;rft_id=http%3A%2F%2Fbootcamp.lif.univ-mrs.fr%2Fde-la-higuera.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Dana Angluin (1987). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20131202232143/http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf\">\"Learning Regular Sets from Queries and Counter-Examples\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i><a href=\"/wiki/Information_and_Control\" class=\"mw-redirect\" title=\"Information and Control\">Information and Control</a></i>. <b>75</b>: 87\u2013106. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/0890-5401%2887%2990052-6\">10.1016/0890-5401(87)90052-6</a>. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf\">the original</a> <span style=\"font-size:85%;\">(PDF)</span> on 2013-12-02.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Information+and+Control&amp;rft.atitle=Learning+Regular+Sets+from+Queries+and+Counter-Examples&amp;rft.volume=75&amp;rft.pages=87-106&amp;rft.date=1987&amp;rft_id=info%3Adoi%2F10.1016%2F0890-5401%2887%2990052-6&amp;rft.au=Dana+Angluin&amp;rft_id=http%3A%2F%2Fwww.cse.iitk.ac.in%2Fusers%2Fchitti%2Fthesis%2Freferences%2FlearningRegSetsFromQueriesAndCounterExamples.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\">D\u2019Ulizia, A., Ferri, F., Grifoni, P. (2011) \"<a rel=\"nofollow\" class=\"external text\" href=\"https://www.academia.edu/download/41900378/A_survey_of_grammatical_inference_method20160202-5760-79hwcu.pdf\">A Survey of Grammatical Inference Methods for Natural Language Learning</a><sup class=\"noprint Inline-Template\"><span style=\"white-space: nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Link_rot\" title=\"Wikipedia:Link rot\"><span title=\"&#160;Dead link since September 2018\">permanent dead link</span></a></i>&#93;</span></sup>\", <i>Artificial Intelligence Review</i>, Vol. 36, No. 1, pp. 1\u201327.</span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\">Clark and Eyraud (2007) <i>Journal of Machine Learning Research</i>; Ryo Yoshinaka (2011) <i>Theoretical Computer Science</i></span>\n</li>\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Dana Angluin (1980). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.sciencedirect.com/science/article/pii/0022000080900410/pdf?md5=c3534f6c086df22fbf814b12984fab5e&amp;pid=1-s2.0-0022000080900410-main.pdf\">\"Finding Patterns Common to a Set of Strings\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Journal of Computer and System Sciences</i>. <b>21</b>: 46\u201362. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/0022-0000%2880%2990041-0\">10.1016/0022-0000(80)90041-0</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Computer+and+System+Sciences&amp;rft.atitle=Finding+Patterns+Common+to+a+Set+of+Strings&amp;rft.volume=21&amp;rft.pages=46-62&amp;rft.date=1980&amp;rft_id=info%3Adoi%2F10.1016%2F0022-0000%2880%2990041-0&amp;rft.au=Dana+Angluin&amp;rft_id=http%3A%2F%2Fwww.sciencedirect.com%2Fscience%2Farticle%2Fpii%2F0022000080900410%2Fpdf%3Fmd5%3Dc3534f6c086df22fbf814b12984fab5e%26pid%3D1-s2.0-0022000080900410-main.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">T. Erlebach; P. Rossmanith; H. Stadtherr; <a href=\"/wiki/Angelika_Steger\" title=\"Angelika Steger\">A. Steger</a>; T. Zeugmann (1997). \"Learning One-Variable Pattern Languages Very Efficiently on Average, in Parallel, and by Asking Queries\".  In M. Li; A. Maruoka. <i>Proc. 8th International Workshop on Algorithmic Learning Theory \u2014 ALT'97</i>. LNAI. <b>1316</b>. Springer. pp.&#160;260\u2013276.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Learning+One-Variable+Pattern+Languages+Very+Efficiently+on+Average%2C+in+Parallel%2C+and+by+Asking+Queries&amp;rft.btitle=Proc.+8th+International+Workshop+on+Algorithmic+Learning+Theory+%E2%80%94+ALT%2797&amp;rft.series=LNAI&amp;rft.pages=260-276&amp;rft.pub=Springer&amp;rft.date=1997&amp;rft.au=T.+Erlebach&amp;rft.au=P.+Rossmanith&amp;rft.au=H.+Stadtherr&amp;rft.au=A.+Steger&amp;rft.au=T.+Zeugmann&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation book\">Hiroki Arimura; Takeshi Shinohara; Setsuko Otsuki (1994). \"Finding Minimal Generalizations for Unions of Pattern Languages and Its Application to Inductive Inference from Positive Data\". <a rel=\"nofollow\" class=\"external text\" href=\"http://ai2-s2-pdfs.s3.amazonaws.com/6a4c/0482e0030b0e5791cf75b0edd9f55fdfc10e.pdf\"><i>Proc. STACS 11</i></a> <span style=\"font-size:85%;\">(PDF)</span>. LNCS. <b>775</b>. Springer. pp.&#160;649\u2013660.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=bookitem&amp;rft.atitle=Finding+Minimal+Generalizations+for+Unions+of+Pattern+Languages+and+Its+Application+to+Inductive+Inference+from+Positive+Data&amp;rft.btitle=Proc.+STACS+11&amp;rft.series=LNCS&amp;rft.pages=649-660&amp;rft.pub=Springer&amp;rft.date=1994&amp;rft.au=Hiroki+Arimura&amp;rft.au=Takeshi+Shinohara&amp;rft.au=Setsuko+Otsuki&amp;rft_id=http%3A%2F%2Fai2-s2-pdfs.s3.amazonaws.com%2F6a4c%2F0482e0030b0e5791cf75b0edd9f55fdfc10e.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span><sup class=\"noprint Inline-Template\"><span style=\"white-space: nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Link_rot\" title=\"Wikipedia:Link rot\"><span title=\"&#160;Dead link since February 2018\">dead link</span></a></i>&#93;</span></sup></span>\n</li>\n<li id=\"cite_note-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-10\">^</a></b></span> <span class=\"reference-text\">Grenander, Ulf, and Michael I. Miller. <i><a rel=\"nofollow\" class=\"external text\" href=\"http://www.ulb.tu-darmstadt.de/tocs/185410162.pdf\">Pattern theory: from representation to inference</a></i>.<sup class=\"noprint Inline-Template\"><span style=\"white-space: nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Link_rot\" title=\"Wikipedia:Link rot\"><span title=\"&#160;Dead link since March 2018\">dead link</span></a></i>&#93;</span></sup> Vol. 1. Oxford: Oxford university press, 2007.</span>\n</li>\n<li id=\"cite_note-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-11\">^</a></b></span> <span class=\"reference-text\">Miller, Scott, et al. \"<a rel=\"nofollow\" class=\"external text\" href=\"http://www.aclweb.org/anthology/P94-1004\">Hidden understanding models of natural language</a>.\" Proceedings of the 32nd annual meeting on Association for Computational Linguistics. Association for Computational Linguistics, 1994.</span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\">Brown, Ralf D. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://pdfs.semanticscholar.org/c537/ac8bac9d83651e0ce6b37333034a5f572e39.pdf#page=5\">Transfer-rule induction for example-based translation</a>.\" Proceedings of the MT Summit VIII Workshop on Example-Based Machine Translation. 2001.</span>\n</li>\n<li id=\"cite_note-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-13\">^</a></b></span> <span class=\"reference-text\">Cherniavsky, Neva, and Richard Ladner. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://pdfs.semanticscholar.org/1be9/0a2f40d10acd17d5910eb21fb3b4a117d08b.pdf\">Grammar-based compression of DNA sequences</a>.\" DIMACS Working Group on The Burrows-Wheeler Transform 21 (2004).</span>\n</li>\n<li id=\"cite_note-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-14\">^</a></b></span> <span class=\"reference-text\">Chater, Nick, and Christopher D. Manning. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://www.stanford.edu/class/linguist1/Rdgs/chater.pdf\">Probabilistic models of language processing and acquisition</a>.\" Trends in cognitive sciences 10.7 (2006): 335-344.</span>\n</li>\n</ol></div></div>\n<h2><span class=\"mw-headline\" id=\"Sources\">Sources</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Grammar_induction&amp;action=edit&amp;section=14\" title=\"Edit section: Sources\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><cite id=\"CITEREFDudaHartStork2001\" class=\"citation\">Duda, Richard O.; Hart, Peter E.; Stork, David G. (2001), <a rel=\"nofollow\" class=\"external text\" href=\"http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html\"><i>Pattern Classification</i></a> (2 ed.), <a href=\"/wiki/New_York_City\" title=\"New York City\">New York</a>: John Wiley &amp; Sons</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Pattern+Classification&amp;rft.place=New+York&amp;rft.edition=2&amp;rft.pub=John+Wiley+%26+Sons&amp;rft.date=2001&amp;rft.aulast=Duda&amp;rft.aufirst=Richard+O.&amp;rft.au=Hart%2C+Peter+E.&amp;rft.au=Stork%2C+David+G.&amp;rft_id=http%3A%2F%2Fwww.wiley.com%2FWileyCDA%2FWileyTitle%2FproductCd-0471056693.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFFu1982\" class=\"citation\">Fu, King Sun (1982), <i>Syntactic Pattern Recognition and Applications</i>, <a href=\"/wiki/Englewood_Cliffs,_NJ\" class=\"mw-redirect\" title=\"Englewood Cliffs, NJ\">Englewood Cliffs, NJ</a>: Prentice-Hall</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Syntactic+Pattern+Recognition+and+Applications&amp;rft.place=Englewood+Cliffs%2C+NJ&amp;rft.pub=Prentice-Hall&amp;rft.date=1982&amp;rft.aulast=Fu&amp;rft.aufirst=King+Sun&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFFu1977\" class=\"citation\">Fu, King Sun (1977), <i>Syntactic Pattern Recognition, Applications</i>, <a href=\"/wiki/Berlin\" title=\"Berlin\">Berlin</a>: Springer-Verlag</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Syntactic+Pattern+Recognition%2C+Applications&amp;rft.place=Berlin&amp;rft.pub=Springer-Verlag&amp;rft.date=1977&amp;rft.aulast=Fu&amp;rft.aufirst=King+Sun&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFHorning1969\" class=\"citation\">Horning, James Jay (1969), <a rel=\"nofollow\" class=\"external text\" href=\"http://proquest.umi.com/pqdlink?Ver=1&amp;Exp=05-16-2013&amp;FMT=7&amp;DID=757518381&amp;RQT=309&amp;attempt=1&amp;cfc=1\"><i>A Study of Grammatical Inference</i></a> (Ph.D. Thesis ed.), <a href=\"/wiki/Stanford\" class=\"mw-redirect\" title=\"Stanford\">Stanford</a>: Stanford University Computer Science Department</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=A+Study+of+Grammatical+Inference&amp;rft.place=Stanford&amp;rft.edition=Ph.D.+Thesis&amp;rft.pub=Stanford+University+Computer+Science+Department&amp;rft.date=1969&amp;rft.aulast=Horning&amp;rft.aufirst=James+Jay&amp;rft_id=http%3A%2F%2Fproquest.umi.com%2Fpqdlink%3FVer%3D1%26Exp%3D05-16-2013%26FMT%3D7%26DID%3D757518381%26RQT%3D309%26attempt%3D1%26cfc%3D1&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFGold1967\" class=\"citation\">Gold, E. Mark (1967), <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20160828171937/http://groups.lis.illinois.edu/amag/langev/paper/gold67limit.html\"><i>Language Identification in the Limit</i></a>, <b>10</b>, <a href=\"/wiki/Information_and_Control\" class=\"mw-redirect\" title=\"Information and Control\">Information and Control</a>, pp.&#160;447\u2013474, archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://groups.lis.illinois.edu/amag/langev/paper/gold67limit.html\">the original</a> on 2016-08-28<span class=\"reference-accessdate\">, retrieved <span class=\"nowrap\">2016-09-04</span></span></cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Language+Identification+in+the+Limit&amp;rft.pages=447-474&amp;rft.pub=Information+and+Control&amp;rft.date=1967&amp;rft.aulast=Gold&amp;rft.aufirst=E.+Mark&amp;rft_id=http%3A%2F%2Fgroups.lis.illinois.edu%2Famag%2Flangev%2Fpaper%2Fgold67limit.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li>\n<li><cite id=\"CITEREFGold1967\" class=\"citation\">Gold, E. Mark (1967), <a rel=\"nofollow\" class=\"external text\" href=\"http://web.mit.edu/~6.863/www/spring2009/readings/gold67limit.pdf\"><i>Language Identification in the Limit</i></a> <span style=\"font-size:85%;\">(PDF)</span>, <b>10</b>, <a href=\"/wiki/Information_and_Control\" class=\"mw-redirect\" title=\"Information and Control\">Information and Control</a>, pp.&#160;447\u2013474</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=book&amp;rft.btitle=Language+Identification+in+the+Limit&amp;rft.pages=447-474&amp;rft.pub=Information+and+Control&amp;rft.date=1967&amp;rft.aulast=Gold&amp;rft.aufirst=E.+Mark&amp;rft_id=http%3A%2F%2Fweb.mit.edu%2F~6.863%2Fwww%2Fspring2009%2Freadings%2Fgold67limit.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AGrammar+induction\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></li></ul>\n\n<!-- \nNewPP limit report\nParsed by mw2226\nCached time: 20180919031043\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.284 seconds\nReal time usage: 0.348 seconds\nPreprocessor visited node count: 1788/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 61474/2097152 bytes\nTemplate argument size: 3941/2097152 bytes\nHighest expansion depth: 12/40\nExpensive parser function count: 6/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 11585/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.132/10.000 seconds\nLua memory usage: 4.89 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  278.012      1 -total\n 33.05%   91.896      2 Template:Reflist\n 24.57%   68.317      1 Template:Machine_learning_bar\n 23.75%   66.024      6 Template:Fix\n 23.33%   64.851      1 Template:Sidebar_with_collapsible_lists\n 23.06%   64.098      3 Template:Citation_needed\n 15.55%   43.238      3 Template:Cite_book\n 14.91%   41.438     12 Template:Category_handler\n 10.43%   28.996      6 Template:Citation\n  6.92%   19.231      3 Template:Delink\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:4375576-0!canonical and timestamp 20180919031042 and revision id 859446382\n -->\n</div>"},"langlinks":[{"lang":"pl","url":"https://pl.wikipedia.org/wiki/Inferencja_gramatyki","langname":"Polish","autonym":"polski","*":"Inferencja gramatyki"}],"categories":[{"sortkey":"","hidden":"","*":"All_articles_with_dead_external_links"},{"sortkey":"","hidden":"","*":"Articles_with_dead_external_links_from_September_2018"},{"sortkey":"","hidden":"","*":"Articles_with_permanently_dead_external_links"},{"sortkey":"","hidden":"","*":"Articles_with_dead_external_links_from_February_2018"},{"sortkey":"","hidden":"","*":"Articles_with_dead_external_links_from_March_2018"},{"sortkey":"","hidden":"","*":"All_articles_with_unsourced_statements"},{"sortkey":"","hidden":"","*":"Articles_with_unsourced_statements_from_August_2007"},{"sortkey":"","hidden":"","*":"Articles_with_unsourced_statements_from_February_2018"},{"sortkey":"","hidden":"","*":"Articles_with_unsourced_statements_from_August_2017"},{"sortkey":"","*":"Genetic_programming"},{"sortkey":"","*":"Natural_language_processing"},{"sortkey":"","*":"Computational_linguistics"},{"sortkey":"","*":"Grammar"},{"sortkey":"","*":"Inference"},{"sortkey":"","*":"Machine_learning"}],"links":[{"ns":14,"exists":"","*":"Category:Articles with unsourced statements from August 2007"},{"ns":14,"exists":"","*":"Category:Articles with unsourced statements from February 2018"},{"ns":14,"exists":"","*":"Category:Articles with unsourced statements from August 2017"},{"ns":14,"exists":"","*":"Category:Articles with dead external links from September 2018"},{"ns":14,"exists":"","*":"Category:Articles with dead external links from February 2018"},{"ns":14,"exists":"","*":"Category:Articles with dead external links from March 2018"},{"ns":10,"exists":"","*":"Template:Machine learning bar"},{"ns":0,"exists":"","*":"Algorithm"},{"ns":0,"exists":"","*":"Angelika Steger"},{"ns":0,"exists":"","*":"Anomaly detection"},{"ns":0,"exists":"","*":"Artificial grammar learning"},{"ns":0,"exists":"","*":"Artificial intelligence"},{"ns":0,"exists":"","*":"Artificial neural network"},{"ns":0,"exists":"","*":"Association rule learning"},{"ns":0,"exists":"","*":"Autoencoder"},{"ns":0,"exists":"","*":"Automated machine learning"},{"ns":0,"exists":"","*":"BIRCH"},{"ns":0,"exists":"","*":"Bayesian network"},{"ns":0,"exists":"","*":"Berlin"},{"ns":0,"exists":"","*":"Bias-variance dilemma"},{"ns":0,"exists":"","*":"Boosting (machine learning)"},{"ns":0,"exists":"","*":"Bootstrap aggregating"},{"ns":0,"exists":"","*":"Byte pair encoding"},{"ns":0,"exists":"","*":"CURE data clustering algorithm"},{"ns":0,"exists":"","*":"Canonical correlation analysis"},{"ns":0,"exists":"","*":"Cluster analysis"},{"ns":0,"exists":"","*":"Computational learning theory"},{"ns":0,"exists":"","*":"Conditional random field"},{"ns":0,"exists":"","*":"Conference on Neural Information Processing Systems"},{"ns":0,"exists":"","*":"Context-free grammar"},{"ns":0,"exists":"","*":"Context-free grammars"},{"ns":0,"exists":"","*":"Convolutional neural network"},{"ns":0,"exists":"","*":"DBSCAN"},{"ns":0,"exists":"","*":"Data mining"},{"ns":0,"exists":"","*":"Decision tree learning"},{"ns":0,"exists":"","*":"Deep learning"},{"ns":0,"exists":"","*":"Digital object identifier"},{"ns":0,"exists":"","*":"Dimensionality reduction"},{"ns":0,"exists":"","*":"Empirical risk minimization"},{"ns":0,"exists":"","*":"Englewood Cliffs, NJ"},{"ns":0,"exists":"","*":"Ensemble learning"},{"ns":0,"exists":"","*":"Evolutionary algorithm"},{"ns":0,"exists":"","*":"Example-based machine translation"},{"ns":0,"exists":"","*":"Example-based translation"},{"ns":0,"exists":"","*":"Expectation\u2013maximization algorithm"},{"ns":0,"exists":"","*":"Extended Backus\u2013Naur form"},{"ns":0,"exists":"","*":"Factor analysis"},{"ns":0,"exists":"","*":"Feature engineering"},{"ns":0,"exists":"","*":"Feature learning"},{"ns":0,"exists":"","*":"Finite state machine"},{"ns":0,"exists":"","*":"Formal grammar"},{"ns":0,"exists":"","*":"Formalism (mathematics)"},{"ns":0,"exists":"","*":"Gated recurrent unit"},{"ns":0,"exists":"","*":"Genetic programming"},{"ns":0,"exists":"","*":"Glossary of artificial intelligence"},{"ns":0,"exists":"","*":"Grammatical function"},{"ns":0,"exists":"","*":"Graphical model"},{"ns":0,"exists":"","*":"Greedy algorithm"},{"ns":0,"exists":"","*":"Hidden Markov model"},{"ns":0,"exists":"","*":"Hierarchical clustering"},{"ns":0,"exists":"","*":"Independent component analysis"},{"ns":0,"exists":"","*":"Induction of regular languages"},{"ns":0,"exists":"","*":"Inductive programming"},{"ns":0,"exists":"","*":"Information and Control"},{"ns":0,"exists":"","*":"International Conference on Machine Learning"},{"ns":0,"exists":"","*":"John Koza"},{"ns":0,"exists":"","*":"Journal of Machine Learning Research"},{"ns":0,"exists":"","*":"K-means clustering"},{"ns":0,"exists":"","*":"K-nearest neighbors algorithm"},{"ns":0,"exists":"","*":"K-nearest neighbors classification"},{"ns":0,"exists":"","*":"Kolmogorov complexity"},{"ns":0,"exists":"","*":"LZW"},{"ns":0,"exists":"","*":"Language identification in the limit"},{"ns":0,"exists":"","*":"Latent variable"},{"ns":0,"exists":"","*":"Learning to rank"},{"ns":0,"exists":"","*":"Linear discriminant analysis"},{"ns":0,"exists":"","*":"Linear regression"},{"ns":0,"exists":"","*":"Lisp (programming language)"},{"ns":0,"exists":"","*":"List of datasets for machine-learning research"},{"ns":0,"exists":"","*":"Local outlier factor"},{"ns":0,"exists":"","*":"Logistic regression"},{"ns":0,"exists":"","*":"Long short-term memory"},{"ns":0,"exists":"","*":"Lossless data compression"},{"ns":0,"exists":"","*":"Machine Learning (journal)"},{"ns":0,"exists":"","*":"Machine learning"},{"ns":0,"exists":"","*":"Mean-shift"},{"ns":0,"exists":"","*":"Mildly context-sensitive language"},{"ns":0,"exists":"","*":"Minimum description length"},{"ns":0,"exists":"","*":"Minimum message length"},{"ns":0,"exists":"","*":"Morpheme"},{"ns":0,"exists":"","*":"Multilayer perceptron"},{"ns":0,"exists":"","*":"Naive Bayes classifier"},{"ns":0,"exists":"","*":"Natural language processing"},{"ns":0,"exists":"","*":"Natural language understanding"},{"ns":0,"exists":"","*":"New York City"},{"ns":0,"exists":"","*":"Non-negative matrix factorization"},{"ns":0,"exists":"","*":"Noun phrase"},{"ns":0,"exists":"","*":"OPTICS algorithm"},{"ns":0,"exists":"","*":"Occam learning"},{"ns":0,"exists":"","*":"Online machine learning"},{"ns":0,"exists":"","*":"Outline of machine learning"},{"ns":0,"exists":"","*":"Pattern language (formal languages)"},{"ns":0,"exists":"","*":"Pattern theory"},{"ns":0,"exists":"","*":"Perceptron"},{"ns":0,"exists":"","*":"Principal component analysis"},{"ns":0,"exists":"","*":"Probabilistic models of language acquisition"},{"ns":0,"exists":"","*":"Probably approximately correct learning"},{"ns":0,"exists":"","*":"Productions (computer science)"},{"ns":0,"exists":"","*":"Pumping lemma for regular languages"},{"ns":0,"exists":"","*":"Q-learning"},{"ns":0,"exists":"","*":"Random forest"},{"ns":0,"exists":"","*":"Recurrent neural network"},{"ns":0,"exists":"","*":"Regression analysis"},{"ns":0,"exists":"","*":"Regular languages"},{"ns":0,"exists":"","*":"Reinforcement learning"},{"ns":0,"exists":"","*":"Relevance vector machine"},{"ns":0,"exists":"","*":"Restricted Boltzmann machine"},{"ns":0,"exists":"","*":"Self-organizing map"},{"ns":0,"exists":"","*":"Semi-supervised learning"},{"ns":0,"exists":"","*":"Sequitur algorithm"},{"ns":0,"exists":"","*":"Stanford"},{"ns":0,"exists":"","*":"State\u2013action\u2013reward\u2013state\u2013action"},{"ns":0,"exists":"","*":"Statistical classification"},{"ns":0,"exists":"","*":"Statistical inference"},{"ns":0,"exists":"","*":"Statistical learning theory"},{"ns":0,"exists":"","*":"Straight-line grammar"},{"ns":0,"exists":"","*":"Structured prediction"},{"ns":0,"exists":"","*":"Supervised learning"},{"ns":0,"exists":"","*":"Support vector machine"},{"ns":0,"exists":"","*":"Syntactic pattern recognition"},{"ns":0,"exists":"","*":"T-distributed stochastic neighbor embedding"},{"ns":0,"exists":"","*":"Temporal difference learning"},{"ns":0,"exists":"","*":"Terminal symbol"},{"ns":0,"exists":"","*":"Tree (data structure)"},{"ns":0,"exists":"","*":"U-Net"},{"ns":0,"exists":"","*":"Ulf Grenander"},{"ns":0,"exists":"","*":"Unsupervised learning"},{"ns":0,"exists":"","*":"Vapnik\u2013Chervonenkis theory"},{"ns":0,"exists":"","*":"Verb phrase"},{"ns":0,"exists":"","*":"Version space"},{"ns":4,"exists":"","*":"Wikipedia:Citation needed"},{"ns":4,"exists":"","*":"Wikipedia:Link rot"},{"ns":11,"exists":"","*":"Template talk:Machine learning bar"},{"ns":100,"exists":"","*":"Portal:Machine learning"}],"templates":[{"ns":10,"exists":"","*":"Template:Machine learning bar"},{"ns":10,"exists":"","*":"Template:Sidebar with collapsible lists"},{"ns":10,"exists":"","*":"Template:Longitem"},{"ns":10,"exists":"","*":"Template:Nobold"},{"ns":10,"exists":"","*":"Template:Small"},{"ns":10,"exists":"","*":"Template:\u2022"},{"ns":10,"exists":"","*":"Template:Portal-inline"},{"ns":10,"exists":"","*":"Template:Harvtxt"},{"ns":10,"exists":"","*":"Template:Harvard citation text"},{"ns":10,"exists":"","*":"Template:Citation needed"},{"ns":10,"exists":"","*":"Template:Fix"},{"ns":10,"exists":"","*":"Template:Category handler"},{"ns":10,"exists":"","*":"Template:Fix/category"},{"ns":10,"exists":"","*":"Template:Delink"},{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":10,"exists":"","*":"Template:Cite book"},{"ns":10,"exists":"","*":"Template:Cite journal"},{"ns":10,"exists":"","*":"Template:Dead link"},{"ns":10,"exists":"","*":"Template:Citation"},{"ns":828,"exists":"","*":"Module:Sidebar"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Navbar"},{"ns":828,"exists":"","*":"Module:Portal"},{"ns":828,"exists":"","*":"Module:Portal/images/m"},{"ns":828,"exists":"","*":"Module:Portal/images/aliases"},{"ns":828,"exists":"","*":"Module:Footnotes"},{"ns":828,"exists":"","*":"Module:Unsubst"},{"ns":828,"exists":"","*":"Module:Category handler"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Category handler/data"},{"ns":828,"exists":"","*":"Module:Category handler/config"},{"ns":828,"exists":"","*":"Module:Category handler/shared"},{"ns":828,"exists":"","*":"Module:Category handler/blacklist"},{"ns":828,"exists":"","*":"Module:Namespace detect/data"},{"ns":828,"exists":"","*":"Module:Namespace detect/config"},{"ns":828,"exists":"","*":"Module:Delink"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"},{"ns":828,"exists":"","*":"Module:Citation/CS1"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Configuration"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Whitelist"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Utilities"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Date validation"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Identifiers"},{"ns":828,"exists":"","*":"Module:Citation/CS1/COinS"}],"images":["Kernel_Machine.svg","Portal-puzzle.svg"],"externallinks":["http://bootcamp.lif.univ-mrs.fr/de-la-higuera.pdf","https://web.archive.org/web/20131202232143/http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf","//doi.org/10.1016/0890-5401(87)90052-6","http://www.cse.iitk.ac.in/users/chitti/thesis/references/learningRegSetsFromQueriesAndCounterExamples.pdf","https://www.academia.edu/download/41900378/A_survey_of_grammatical_inference_method20160202-5760-79hwcu.pdf","http://www.sciencedirect.com/science/article/pii/0022000080900410/pdf?md5=c3534f6c086df22fbf814b12984fab5e&pid=1-s2.0-0022000080900410-main.pdf","//doi.org/10.1016/0022-0000(80)90041-0","http://ai2-s2-pdfs.s3.amazonaws.com/6a4c/0482e0030b0e5791cf75b0edd9f55fdfc10e.pdf","http://www.ulb.tu-darmstadt.de/tocs/185410162.pdf","http://www.aclweb.org/anthology/P94-1004","https://pdfs.semanticscholar.org/c537/ac8bac9d83651e0ce6b37333034a5f572e39.pdf#page=5","https://pdfs.semanticscholar.org/1be9/0a2f40d10acd17d5910eb21fb3b4a117d08b.pdf","https://www.stanford.edu/class/linguist1/Rdgs/chater.pdf","https://arxiv.org/list/cs.LG/recent","http://www.wiley.com/WileyCDA/WileyTitle/productCd-0471056693.html","http://proquest.umi.com/pqdlink?Ver=1&Exp=05-16-2013&FMT=7&DID=757518381&RQT=309&attempt=1&cfc=1","https://web.archive.org/web/20160828171937/http://groups.lis.illinois.edu/amag/langev/paper/gold67limit.html","http://groups.lis.illinois.edu/amag/langev/paper/gold67limit.html","http://web.mit.edu/~6.863/www/spring2009/readings/gold67limit.pdf"],"sections":[{"toclevel":1,"level":"2","line":"Grammar classes","number":"1","index":"1","fromtitle":"Grammar_induction","byteoffset":888,"anchor":"Grammar_classes"},{"toclevel":1,"level":"2","line":"Learning models","number":"2","index":"2","fromtitle":"Grammar_induction","byteoffset":1548,"anchor":"Learning_models"},{"toclevel":1,"level":"2","line":"Methodologies","number":"3","index":"3","fromtitle":"Grammar_induction","byteoffset":2617,"anchor":"Methodologies"},{"toclevel":2,"level":"3","line":"Grammatical inference by trial-and-error","number":"3.1","index":"4","fromtitle":"Grammar_induction","byteoffset":3743,"anchor":"Grammatical_inference_by_trial-and-error"},{"toclevel":2,"level":"3","line":"Grammatical inference by genetic algorithms","number":"3.2","index":"5","fromtitle":"Grammar_induction","byteoffset":4492,"anchor":"Grammatical_inference_by_genetic_algorithms"},{"toclevel":2,"level":"3","line":"Grammatical inference by greedy algorithms","number":"3.3","index":"6","fromtitle":"Grammar_induction","byteoffset":6568,"anchor":"Grammatical_inference_by_greedy_algorithms"},{"toclevel":2,"level":"3","line":"Distributional learning","number":"3.4","index":"7","fromtitle":"Grammar_induction","byteoffset":7574,"anchor":"Distributional_learning"},{"toclevel":2,"level":"3","line":"Learning of pattern languages","number":"3.5","index":"8","fromtitle":"Grammar_induction","byteoffset":8017,"anchor":"Learning_of_pattern_languages"},{"toclevel":2,"level":"3","line":"Pattern theory","number":"3.6","index":"9","fromtitle":"Grammar_induction","byteoffset":10732,"anchor":"Pattern_theory"},{"toclevel":1,"level":"2","line":"Applications","number":"4","index":"10","fromtitle":"Grammar_induction","byteoffset":12170,"anchor":"Applications"},{"toclevel":1,"level":"2","line":"See also","number":"5","index":"11","fromtitle":"Grammar_induction","byteoffset":13808,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"Notes","number":"6","index":"12","fromtitle":"Grammar_induction","byteoffset":14083,"anchor":"Notes"},{"toclevel":1,"level":"2","line":"References","number":"7","index":"13","fromtitle":"Grammar_induction","byteoffset":14117,"anchor":"References"},{"toclevel":1,"level":"2","line":"Sources","number":"8","index":"14","fromtitle":"Grammar_induction","byteoffset":14145,"anchor":"Sources"}],"parsewarnings":[],"displaytitle":"Grammar induction","iwlinks":[],"properties":[{"name":"wikibase_item","*":"Q5593673"}]}}