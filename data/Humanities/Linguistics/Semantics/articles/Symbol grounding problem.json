{"parse":{"title":"Symbol grounding problem","pageid":3446949,"revid":851685979,"text":{"*":"<div class=\"mw-parser-output\"><table class=\"plainlinks metadata ambox ambox-content ambox-multiple_issues compact-ambox\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/40px-Ambox_important.svg.png\" width=\"40\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/60px-Ambox_important.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/b/b4/Ambox_important.svg/80px-Ambox_important.svg.png 2x\" data-file-width=\"40\" data-file-height=\"40\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\"><div class=\"mw-collapsible\" style=\"width:95%; margin: 0.2em 0;\"><b>This article has multiple issues.</b> Please help <b><a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Symbol_grounding_problem&amp;action=edit\">improve it</a></b> or discuss these issues on the <b><a href=\"/wiki/Talk:Symbol_grounding_problem\" title=\"Talk:Symbol grounding problem\">talk page</a></b>. <small><i>(<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove these template messages</a>)</i></small>\n<div class=\"mw-collapsible-content\" style=\"margin-top: 0.3em;\">\n      <table class=\"plainlinks metadata ambox ambox-content ambox-COI\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"Unbalanced scales.svg\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/45px-Unbalanced_scales.svg.png\" width=\"45\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/68px-Unbalanced_scales.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/90px-Unbalanced_scales.svg.png 2x\" data-file-width=\"400\" data-file-height=\"354\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\"><b> A major contributor to this article appears to have a <a href=\"/wiki/Wikipedia:Conflict_of_interest\" title=\"Wikipedia:Conflict of interest\">close connection</a> with its subject.</b><span class=\"hide-when-compact\"> It may require <a href=\"/wiki/Wikipedia:Cleanup\" title=\"Wikipedia:Cleanup\">cleanup</a> to comply with Wikipedia's content policies, particularly <a href=\"/wiki/Wikipedia:Neutral_point_of_view\" title=\"Wikipedia:Neutral point of view\">neutral point of view</a>. Please discuss further on the <a href=\"/wiki/Talk:Symbol_grounding_problem\" title=\"Talk:Symbol grounding problem\">talk page</a>.</span>  <small><i>(September 2014)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<table class=\"plainlinks metadata ambox ambox-style ambox-More_footnotes\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/40px-Text_document_with_red_question_mark.svg.png\" width=\"40\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/60px-Text_document_with_red_question_mark.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/a/a4/Text_document_with_red_question_mark.svg/80px-Text_document_with_red_question_mark.svg.png 2x\" data-file-width=\"48\" data-file-height=\"48\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article includes a <a href=\"/wiki/Wikipedia:Citing_sources\" title=\"Wikipedia:Citing sources\">list of references</a>, but <b>its sources remain unclear</b> because it has <b>insufficient <a href=\"/wiki/Wikipedia:Citing_sources#Inline_citations\" title=\"Wikipedia:Citing sources\">inline citations</a></b>.<span class=\"hide-when-compact\"> Please help to <a href=\"/wiki/Wikipedia:WikiProject_Fact_and_Reference_Check\" title=\"Wikipedia:WikiProject Fact and Reference Check\">improve</a> this article by <a href=\"/wiki/Wikipedia:When_to_cite\" title=\"Wikipedia:When to cite\">introducing</a> more precise citations.</span>  <small><i>(March 2013)</i></small><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<table class=\"plainlinks metadata ambox ambox-content ambox-unbalanced\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><div style=\"width:52px\"><img alt=\"Unbalanced scales.svg\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/45px-Unbalanced_scales.svg.png\" width=\"45\" height=\"40\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/68px-Unbalanced_scales.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/f/fe/Unbalanced_scales.svg/90px-Unbalanced_scales.svg.png 2x\" data-file-width=\"400\" data-file-height=\"354\" /></div></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This article <b>may be <a href=\"/wiki/Wikipedia:Neutral_point_of_view#Undue_weight\" title=\"Wikipedia:Neutral point of view\">unbalanced</a> towards certain viewpoints</b>.<span class=\"hide-when-compact\"> Please <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Symbol_grounding_problem&amp;action=edit\">improve the article</a> by adding information on neglected viewpoints, or discuss the issue on the <a href=\"/wiki/Talk:Symbol_grounding_problem\" title=\"Talk:Symbol grounding problem\">talk page</a>.</span>  <small><i>(December 2010)</i></small></div></td></tr></tbody></table>\n    </div>\n</div><small class=\"hide-when-compact\"><i> (<a href=\"/wiki/Help:Maintenance_template_removal\" title=\"Help:Maintenance template removal\">Learn how and when to remove this template message</a>)</i></small></div></td></tr></tbody></table>\n<p>The <b>symbol grounding problem</b> is related to the problem of how <a href=\"/wiki/Word\" title=\"Word\">words</a>  (<a href=\"/wiki/Symbol\" title=\"Symbol\">symbols</a>) get their <a href=\"/wiki/Meaning_(psychology)\" title=\"Meaning (psychology)\">meanings</a>, and hence to the problem of what meaning itself really is. The problem of meaning is in turn related to the problem of <a href=\"/wiki/Consciousness\" title=\"Consciousness\">consciousness</a>, or how it is that <a href=\"/wiki/Mental_state\" title=\"Mental state\">mental states</a> are meaningful.\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Background\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Background</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-2\"><a href=\"#Referents\"><span class=\"tocnumber\">1.1</span> <span class=\"toctext\">Referents</span></a></li>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"#Referential_process\"><span class=\"tocnumber\">1.2</span> <span class=\"toctext\">Referential process</span></a></li>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Grounding_process\"><span class=\"tocnumber\">1.3</span> <span class=\"toctext\">Grounding process</span></a></li>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"#Requirements_for_symbol_grounding\"><span class=\"tocnumber\">1.4</span> <span class=\"toctext\">Requirements for symbol grounding</span></a>\n<ul>\n<li class=\"toclevel-3 tocsection-6\"><a href=\"#Capacity_to_pick_out_referents\"><span class=\"tocnumber\">1.4.1</span> <span class=\"toctext\">Capacity to pick out referents</span></a></li>\n<li class=\"toclevel-3 tocsection-7\"><a href=\"#Consciousness\"><span class=\"tocnumber\">1.4.2</span> <span class=\"toctext\">Consciousness</span></a></li>\n</ul>\n</li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-8\"><a href=\"#Formulation\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Formulation</span></a></li>\n<li class=\"toclevel-1 tocsection-9\"><a href=\"#Functionalism\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Functionalism</span></a></li>\n<li class=\"toclevel-1 tocsection-10\"><a href=\"#Searle&#39;s_Chinese_room_argument\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Searle's Chinese room argument</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-11\"><a href=\"#Implications\"><span class=\"tocnumber\">4.1</span> <span class=\"toctext\">Implications</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-12\"><a href=\"#Brentano&#39;s_notion_of_intentionality\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Brentano's notion of intentionality</span></a></li>\n<li class=\"toclevel-1 tocsection-13\"><a href=\"#See_also\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-14\"><a href=\"#Notes\"><span class=\"tocnumber\">7</span> <span class=\"toctext\">Notes</span></a></li>\n<li class=\"toclevel-1 tocsection-15\"><a href=\"#References\"><span class=\"tocnumber\">8</span> <span class=\"toctext\">References</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Background\">Background</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=1\" title=\"Edit section: Background\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<h3><span class=\"mw-headline\" id=\"Referents\">Referents</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=2\" title=\"Edit section: Referents\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p><a href=\"/wiki/Gottlob_Frege\" title=\"Gottlob Frege\">Gottlob Frege</a> distinguished a referent, the thing that a word refers to, and the word's  meaning. This is most clearly illustrated using the proper names of concrete individuals, but it is also true of names of kinds of things and of abstract properties: (1) \"Tony Blair\", (2) \"the prime minister of the UK during the year 2004\", and (3) \"Cherie Blair's husband\" all have the same referent, but not the same meaning.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup>\n</p><p>Some have suggested that the meaning of a (referring) word is the rule or features that one must use in order to successfully pick out its referent. In that respect, (2) and (3) come closer to wearing their meanings on their sleeves, because they are explicitly stating a rule for picking out their referents: \"Find whoever was prime minister of the UK during the year 2004\", or \"find whoever is Cherie's current husband\". But that does not settle the matter, because there's still the problem of the meaning of the components of that rule (\"UK,\" \"during\", \"current\", \"PM\", \"Cherie\", \"husband\"), and how to pick <i>them</i> out.\n</p><p>The phrase \"Tony Blair\" (or better still, just \"Tony\") does not have this recursive component problem, because it points straight to its referent, but how? If the meaning is the rule for picking out the referent, what is that rule, when we come down to non-decomposable components like proper names of individuals (or names of <i>kinds</i>, as in \"an unmarried man\" is a \"bachelor\")?\n</p>\n<h3><span class=\"mw-headline\" id=\"Referential_process\">Referential process</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=3\" title=\"Edit section: Referential process\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Humans are able to pick out the intended referents of words<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (October 2016)\">citation needed</span></a></i>&#93;</sup>, such as \"Tony Blair\" or \"bachelor,\" but this process need not be explicit.  It is probably an unreasonable expectation to know the explicit <a href=\"/wiki/Philosophical_Investigations#Rules\" title=\"Philosophical Investigations\">rule</a> for picking out the intended referents<sup class=\"noprint Inline-Template\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Please_clarify\" title=\"Wikipedia:Please clarify\"><span title=\" (October 2016)\">why?</span></a></i>&#93;</sup>.\n</p><p>So if we take a word's meaning to be the means of picking out its referent, then meanings are in our brains. That is meaning in the <i>narrow</i> sense. If we use \"meaning\" in a <i>wider</i> sense, then we may want to say that meanings include both the referents themselves and the means of picking them out. So if a word (say, \"Tony-Blair\") is located inside an entity (e.g., oneself) that can use the word and pick out its referent, then the word's wide meaning consists of both the means that that entity uses to pick out its referent, and the referent itself: a wide causal nexus between (1) a head, (2) a word inside it, (3) an object outside it, and (4) whatever \"processing\" is required in order to successfully connect the inner word to the outer object.\n</p><p>But what if the \"entity\" in which a word is located is not a head but a piece of paper (or a computer screen)? What is its meaning then? Surely all the (referring) words on this screen, for example, have meanings, just as they have referents.\n</p><p>In the 19th century, the semiotician <a href=\"/wiki/Charles_Saunders_Peirce\" class=\"mw-redirect\" title=\"Charles Saunders Peirce\">Charles Saunders Peirce</a> suggested what some think is a similar model: according to his triadic sign model, meaning requires (1) an interpreter, (2) a sign or representamen, (3) an object, and is (4) the virtual product of an endless regress and progress called <a href=\"/wiki/Semiosis\" title=\"Semiosis\">Semiosis</a>.<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup> Some have interpreted Peirce as addressing the problem of grounding, feelings, and intentionality for the understanding of semiotic processes.<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup> In recent years, Peirce's theory of signs has been rediscovered by an increasing number of artificial intelligence researchers in the context of symbol grounding problem.<sup id=\"cite_ref-4\" class=\"reference\"><a href=\"#cite_note-4\">&#91;4&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Grounding_process\">Grounding process</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=4\" title=\"Edit section: Grounding process\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<table class=\"plainlinks metadata ambox mbox-small-left ambox-content\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><a href=\"/wiki/File:Wiki_letter_w_cropped.svg\" class=\"image\"><img alt=\"[icon]\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/20px-Wiki_letter_w_cropped.svg.png\" width=\"20\" height=\"14\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/30px-Wiki_letter_w_cropped.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/40px-Wiki_letter_w_cropped.svg.png 2x\" data-file-width=\"44\" data-file-height=\"31\" /></a></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This section <b>needs expansion</b>. <small>You can help by <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=\">adding to it</a>.</small>  <small><i>(October 2016)</i></small></div></td></tr></tbody></table>\n<p>There would be no connection at all between written symbols and any intended referents if there were no minds mediating those intentions, via their own internal means of picking out those intended referents.\n</p><p>So the meaning of a word on a page is \"ungrounded.\"<sup id=\"cite_ref-5\" class=\"reference\"><a href=\"#cite_note-5\">&#91;5&#93;</a></sup> Nor would looking it up in a dictionary help: If one tried to look up the meaning of a word one did not understand in a <a href=\"/wiki/Dictionary\" title=\"Dictionary\">dictionary</a> of a language one did not already understand, one would just cycle endlessly from one meaningless definition to another. One's search for meaning would be ungrounded.\n</p><p>In contrast, the meaning of the words in one's head\u2014those words one <i>does</i> understand\u2014are \"grounded\"<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (October 2016)\">citation needed</span></a></i>&#93;</sup>. That mental grounding of the meanings of words mediates between the words on any external page one reads (and understands) and the external objects to which those words refer.<sup id=\"cite_ref-6\" class=\"reference\"><a href=\"#cite_note-6\">&#91;6&#93;</a></sup><sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;7&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Requirements_for_symbol_grounding\">Requirements for symbol grounding</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=5\" title=\"Edit section: Requirements for symbol grounding\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Another symbol system is <a href=\"/wiki/Natural_language\" title=\"Natural language\">natural language</a> (Fodor 1975). On paper or in a computer, language, too, is just a formal symbol system, manipulable by rules based on the arbitrary shapes of words. But in the brain, meaningless strings of squiggles become meaningful thoughts. Harnad has suggested two properties that might be required to make this difference:\n</p>\n<ul><li>capacity to pick referents</li>\n<li>consciousness</li></ul>\n<h4><span class=\"mw-headline\" id=\"Capacity_to_pick_out_referents\">Capacity to pick out referents</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=6\" title=\"Edit section: Capacity to pick out referents\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>One property that static paper or, usually, even a dynamic computer lack is that the brain possess is the capacity to pick out symbols' referents. This is what we were discussing earlier, and it is what the hitherto undefined term \"grounding\" refers to. A symbol system alone, whether static or dynamic, cannot have this capacity (any more than a book can), because picking out referents is not just a computational (implementation-independent) property; it is a dynamical (implementation-dependent) property.\n</p><p>To be grounded, the symbol system would have to be augmented with nonsymbolic, sensorimotor capacities\u2014the capacity to interact autonomously with that world of objects, events, actions, properties and states that its symbols are systematically interpretable (by us) as referring to. It would have to be able to pick out the referents of its symbols, and its sensorimotor interactions with the world would have to fit coherently with the symbols' interpretations.\n</p><p>The symbols, in other words, need to be connected directly to (i.e., grounded in) their referents; the connection must not be dependent only on the connections made by the brains of external interpreters like us. Just the symbol system alone, without this capacity for direct grounding, is not a viable candidate for being whatever it is that is really going on in our brains when we think meaningful thoughts (Cangelosi &amp; Harnad 2001).\n</p><p>Meaning as the ability to recognize instances (of objects) or perform actions is specifically treated in the paradigm called \"Procedural Semantics\", described in a number of papers including \"Procedural Semantics\" by Philip N.  Johnson-Laird <sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;8&#93;</a></sup> and expanded by William A. Woods in \"Meaning and Links\".<sup id=\"cite_ref-9\" class=\"reference\"><a href=\"#cite_note-9\">&#91;9&#93;</a></sup>  A brief summary in Woods' paper reads: \"The idea of procedural semantics is that the semantics of natural language sentences can be characterized in a formalism whose meanings are defined by abstract procedures that a computer (or a person) can either execute or reason about. In this theory the meaning of a noun is a procedure for recognizing or generating instances, the meaning of a proposition is a procedure for determining if it is true or false, and the meaning of an action is the ability to do the action or to tell if it has been done.\"\n</p>\n<h4><span class=\"mw-headline\" id=\"Consciousness\">Consciousness</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=7\" title=\"Edit section: Consciousness\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h4>\n<p>The necessity of groundedness, in other words, takes us from the level of the pen-pal <a href=\"/wiki/Turing_test\" title=\"Turing test\">Turing test</a>, which is purely symbolic (computational), to the robotic Turing test, which is hybrid symbolic/sensorimotor (Harnad 2000, 2007). Meaning is grounded in the robotic capacity to detect, categorize, identify, and act upon the things that words and sentences refer to (see entries for <a href=\"/wiki/Affordance\" title=\"Affordance\">Affordance</a> and for <a href=\"/wiki/Categorical_perception\" title=\"Categorical perception\">Categorical perception</a>). On the other hand, if the symbols (words and sentences) refer to the very bits of '0' and '1', directly connected to their electronic implementations, which a (any?) computer system can readily manipulate (thus detect, categorize, identify and act upon), then even non-robotic computer systems could be said to be \"sensorimotor\" and hence able to \"ground\" symbols in this narrow domain. \n</p><p>To categorize is to do the right thing with the right <i>kind</i> of thing. The categorizer must be able to detect the sensorimotor features of the members of the category that reliably distinguish them from the nonmembers. These feature-detectors must either be inborn or learned. The learning can be based on trial and error induction, guided by feedback from the consequences of correct and incorrect categorization; or, in our own linguistic species, the learning can also be based on verbal descriptions or definitions. The description or definition of a new category, however, can only convey the category and ground its name if the words in the definition are themselves already grounded category names (Blondin-Mass\u00e9 et al. 2008). So ultimately grounding has to be sensorimotor, to avoid infinite regress (Harnad 2005).\n</p><p>But if groundedness is a necessary condition for meaning, is it a sufficient one? Not necessarily, for it is possible that even a robot that could pass the Turing test, \"living\" amongst the rest of us indistinguishably for a lifetime, would fail to have in its head what Searle has in his: It could be a <a href=\"/wiki/Philosophical_zombie\" title=\"Philosophical zombie\">zombie</a>, with no one home, feeling feelings, meaning meanings (Harnad 1995). However, it is possible that different interpreters (including different intelligent species of animals) would have different mechanisms for producing meaning in their systems, thus one cannot require that a system different from a human \"experiences\" meaning in the same way that a human does, and vice-versa. \n</p><p>Harnad thus points at consciousness as a second property. The problem of discovering the causal mechanism for successfully picking out the referent of a category name can in principle be solved by cognitive science. But the problem of explaining how consciousness could play an \"independent\" role in doing so is probably insoluble, except on pain of <a href=\"/wiki/Psychokinesis\" title=\"Psychokinesis\">telekinetic</a> <a href=\"/wiki/Dualism_(philosophy_of_mind)\" class=\"mw-redirect\" title=\"Dualism (philosophy of mind)\">dualism</a>. Perhaps symbol grounding (i.e., robotic TT capacity) is enough to ensure that conscious meaning is present, but then again, perhaps not. In either case, there is no way we can hope to be any the wiser\u2014and that is Turing's methodological point (Harnad 2001b, 2003, 2006).\n</p>\n<h2><span class=\"mw-headline\" id=\"Formulation\">Formulation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=8\" title=\"Edit section: Formulation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>To answer this question we have to formulate the symbol grounding problem itself <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/3106/\">(Harnad 1990)</a>:\n</p>\n<table class=\"plainlinks metadata ambox mbox-small-left ambox-content\" role=\"presentation\"><tbody><tr><td class=\"mbox-image\"><a href=\"/wiki/File:Wiki_letter_w_cropped.svg\" class=\"image\"><img alt=\"[icon]\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/20px-Wiki_letter_w_cropped.svg.png\" width=\"20\" height=\"14\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/30px-Wiki_letter_w_cropped.svg.png 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/1/1c/Wiki_letter_w_cropped.svg/40px-Wiki_letter_w_cropped.svg.png 2x\" data-file-width=\"44\" data-file-height=\"31\" /></a></td><td class=\"mbox-text\"><div class=\"mbox-text-span\">This section <b>needs expansion</b>. <small>You can help by <a class=\"external text\" href=\"//en.wikipedia.org/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=\">adding to it</a>.</small>  <small><i>(October 2016)</i></small></div></td></tr></tbody></table>\n<h2><span class=\"mw-headline\" id=\"Functionalism\">Functionalism</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=9\" title=\"Edit section: Functionalism\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>There is a school of thought according to which the computer is more like the brain\u2014or rather, the brain is more like the computer: According to this view (called \"<a href=\"/wiki/Computational_theory_of_mind\" title=\"Computational theory of mind\">computationalism</a>\", a variety of <a href=\"/wiki/Functionalism_(philosophy_of_mind)\" title=\"Functionalism (philosophy of mind)\">functionalism</a>), the future theory explaining how the brain picks out its referents (the theory that cognitive neuroscience may eventually arrive at) will be a purely computational one (Pylyshyn 1984). A computational theory is a theory at the software level. It is essentially a computer algorithm: a set of rules for manipulating symbols. And the algorithm is \"implementation-independent.\" That means that whatever it is that an algorithm is doing, it will do the same thing no matter what hardware it is executed on. The physical details of the <a href=\"/wiki/Dynamical_system\" title=\"Dynamical system\">dynamical system</a> implementing the computation are irrelevant to the computation itself, which is purely formal; any hardware that can run the computation will do, and all physical implementations of that particular computer algorithm are equivalent, computationally.\n</p><p>A computer can execute any computation. Hence once computationalism finds a proper computer algorithm, one that our brain could be running when there is meaning transpiring in our heads, meaning will be transpiring in that computer too, when it implements that algorithm.\n</p><p>How would we know that we have a proper computer algorithm? It would have to be able to pass the <a href=\"/wiki/Turing_test\" title=\"Turing test\">Turing test</a>. That means it would have to be capable of corresponding with any human being as a pen-pal, for a lifetime, without ever being in any way distinguishable from a real human pen-pal.\n</p>\n<h2><span id=\"Searle.27s_Chinese_room_argument\"></span><span class=\"mw-headline\" id=\"Searle's_Chinese_room_argument\">Searle's Chinese room argument</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=10\" title=\"Edit section: Searle&#039;s Chinese room argument\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Main article: <a href=\"/wiki/Chinese_room\" title=\"Chinese room\">Chinese room</a></div>\n<p><a href=\"/wiki/John_Searle\" title=\"John Searle\">John Searle</a> formulated the \"<a href=\"/wiki/Chinese_room\" title=\"Chinese room\">Chinese room</a> argument\" in order to disprove computationalism<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (October 2016)\">citation needed</span></a></i>&#93;</sup>. The Chinese room argument is based on a thought experiment: in it, Searle stated that if the Turing test were conducted in Chinese, then he himself, Searle (who does not understand Chinese), could execute a program that implements the same algorithm that the computer was using without knowing what any of the words he was manipulating meant. \n</p><p>At first glance, it would seem that if there's no meaning going on inside Searle's head when he is implementing that program, then there's no meaning going on inside the computer when it is the one implementing the algorithm either, computation being implementation-independent. But on a closer look, for a person to execute the same program that a computer would, at very least it would  have to have access to a similar bank of memory that the computer has (most likely externally stored). This means that the new computational system that executes the same algorithm is no longer just Searle's original head, but that plus the memory bank (and possibly other devices). \nIn particular, this additional memory could store a digital representation of the intended referent of different words (like images, sounds, even video sequences), that the algorithm would use as a model of, and to derive features assotiated with, the intended referent. The \"meaning\" then is not to be searched in just Searle's original brain, but in the overall system needed to process the algorithm. (Just like when Searle is reading English words, the meaning is not to be located in isolated logical processing areas of the brain, but probably in the overall brain, likely including specific long-term memory areas). \nThus, Searle's not perceiving any meaning in his head alone when simulating the work of a computer, does not imply lack of meaning in the overall system, and thus in the actual computer system passing an advanced Turing test.\n</p>\n<h3><span class=\"mw-headline\" id=\"Implications\">Implications</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=11\" title=\"Edit section: Implications\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>How does Searle know that there is no meaning going on in his head when he is executing such a Turing-test-passing program? Exactly the same way he knows whether there is or is not meaning going on inside his head under any other conditions: He <i>understands</i> the words of English, whereas the Chinese symbols that he is manipulating according to the algorithm's rules mean nothing whatsoever to him (and there is no one else in his head for them to mean anything to). However, the complete system that is manipulating those Chinese symbols - which is not just Searle's brain, as explained in the previous section - may have the ability to extract meaning from those symbols, in the sense of being able to use  internal (memory) models of the intended referents, pick out the intended referents of those symbols, and generally identifying and using their features appropriately.\n</p><p>Note that in pointing out that the Chinese words would be meaningless to him under those conditions, Searle has appealed to consciousness. Otherwise one could argue that there <i>would</i> be meaning going on in Searle's  head under those conditions, but that Searle himself would simply not be conscious of it. That is called the <a rel=\"nofollow\" class=\"external text\" href=\"http://plato.stanford.edu/entries/chinese-room/#4.1\">\"Systems Reply\"</a> to Searle's Chinese Room Argument, and Searle <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/4023/\">rejects</a> the Systems Reply as being merely a reiteration, in the face of negative evidence, of the very thesis (computationalism) that is on trial in his thought-experiment: \"Are words in a running computation like the ungrounded words on a page, meaningless without the mediation of brains, or are they like the grounded words in brains?\"\n</p><p>In this either/or question, the (still undefined) word \"ungrounded\" has implicitly relied on the difference between inert words on a page and consciously meaningful words in our heads. And Searle is asserting that under these conditions (the Chinese Turing test), the words in his head would not be consciously meaningful, hence they would still be as ungrounded as the inert words on a page.\n</p><p>So if Searle is right, that (1) both the words on a page and those in any running computer program (including a Turing-test-passing computer program) are meaningless in and of themselves, and hence that (2) whatever it is that the brain is doing to generate meaning can't be just implementation-independent computation, then what <i>is</i> the brain doing to generate meaning <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/4023/\">(Harnad 2001a)</a>?\n</p>\n<h2><span id=\"Brentano.27s_notion_of_intentionality\"></span><span class=\"mw-headline\" id=\"Brentano's_notion_of_intentionality\">Brentano's notion of intentionality</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=12\" title=\"Edit section: Brentano&#039;s notion of intentionality\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>\"<a href=\"/wiki/Intentionality\" title=\"Intentionality\">Intentionality</a>\" has been called the \"mark of the mental\" because of some observations by the philosopher <a href=\"/wiki/Franz_Brentano\" title=\"Franz Brentano\">Brentano</a> to the effect that mental states always have an inherent, intended (mental) object or content toward which they are \"directed\": One sees something, wants something, believes something, desires something, understands something, means something etc., and that object is always something that one has <i>in mind</i>. Having a mental object is part of having anything in mind. Hence it is the mark of the mental. There are no \"free-floating\" mental states that do not also have a mental object. Even hallucinations and imaginings have an object, and even feeling depressed feels like something. Nor is the object the \"external\" physical object, when there is one. One may see a real chair, but the \"intentional\" object of one's \"intentional state\" is the mental chair one has in mind. (Yet another term for intentionality has been \"aboutness\" or \"representationality\": thoughts are always <i>about</i> something; they are (mental) \"representations\" <i>of</i>  something; but that something is what it is that the thinker has in mind, not whatever external object may or may not correspond to it.)\n</p><p>If this all sounds like skating over the surface of a problem rather than a real break-through, then the foregoing description has had its intended effect: No, the problem of intentionality is not the symbol grounding problem; nor is grounding symbols the solution to the problem of intentionality. The symbols inside an autonomous dynamical symbol system that is able to pass the robotic Turing test are grounded, in that, unlike in the case of an ungrounded symbol system, they do not depend on the mediation of the mind of an external interpreter to connect them to the external objects that they are interpretable (by the interpreter) as being \"about\"; the connection is autonomous, direct, and unmediated. But <i>grounding is not meaning</i>. Grounding is an input/output performance function. Grounding connects the sensory inputs from external objects to internal symbols and states occurring within an autonomous sensorimotor system, guiding the system's resulting processing and output.\n</p><p>Meaning, in contrast, is something mental. But to try to put a halt to the name-game of proliferating nonexplanatory synonyms for the mind/body problem without solving it (or, worse, implying that there is more than one mind/body problem), let us cite just one more thing that requires no further explication: <i>feeling</i>. The only thing that distinguishes an internal state that merely has grounding from one that has meaning is that it <i>feels like something</i> to be in the meaning state, whereas it does not feel like anything to be in the merely grounded functional state. Grounding is a functional matter; feeling is a felt matter. And that is the real source of Brentano's vexed peekaboo relation between \"intentionality\" and its internal \"intentional object\":  All mental states, in addition to being the functional states of an autonomous dynamical system, are also feeling states: Feelings are not merely \"functed,\" as all other physical states are; feelings are also felt.\n</p><p>Hence feeling (<a href=\"/wiki/Sentience\" title=\"Sentience\">sentience</a>) is the real mark of the mental. But the symbol grounding problem is not the same as the mind/body problem, let alone a solution to it. The mind/body problem is actually the feeling/function problem: Symbol-grounding touches only its functional component.  This does not detract from the importance of the symbol grounding problem, but just reflects that it is a keystone piece to the bigger puzzle called the mind.\n</p><p>The neuroscientist <a href=\"/wiki/Antonio_Damasio\" title=\"Antonio Damasio\">Antonio Damasio</a> investigates this marking function of feelings and emotions in his <a href=\"/wiki/Somatic_marker_hypothesis\" title=\"Somatic marker hypothesis\">Somatic marker hypothesis</a>. Damasio adds the notion of biologic <a href=\"/wiki/Homeostasis\" title=\"Homeostasis\">homeostasis</a> to this discussion, presenting it as an automated bodily regulation process providing intentionality to a mind via emotions. Homeostasis is the mechanism that keeps all bodily processes in healthy balance. All of our actions and perceptions will be automatically \"evaluated\" by our body hardware according to their contribution to homeostasis. This gives us an implicit orientation on how to survive. Such bodily or somatic evaluations can come to our mind in the form of conscious and non-conscious feelings (\"gut feelings\") and lead our decision-making process. The meaning of a word can be roughly conceptualized as the sum of its associations and their expected contribution to homeostasis, where associations are reconstructions of sensomotor perceptions that appeared in contiguity with the word. Yet, the Somatic marker hypothesis is still hotly debated and critics claim that it has failed to clearly demonstrate how these processes interact at a psychological and evolutionary level. The recurrent question that the Somatic marker hypothesis does not address remains: how and why does homeostasis (as in any <a href=\"/wiki/Servomechanism\" title=\"Servomechanism\">servomechanism</a> such as a thermostat and furnace) become <i>felt</i> homeostasis?\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=13\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div>\n\n&#32;\n<table class=\"multicol\" role=\"presentation\" style=\"border-collapse: collapse; padding: 0; border: 0; background:transparent; width:100%;\"><tbody><tr>\n<td style=\"text-align: left; vertical-align: top;\">\n<ul><li><a href=\"/wiki/Binding_problem\" title=\"Binding problem\">Binding problem</a></li>\n<li><a href=\"/wiki/Categorical_perception\" title=\"Categorical perception\">Categorical perception</a></li>\n<li><a href=\"/wiki/Communicative_action\" title=\"Communicative action\">Communicative action</a></li>\n<li><a href=\"/wiki/Consciousness\" title=\"Consciousness\">Consciousness</a></li>\n<li><a href=\"/wiki/Formal_language\" title=\"Formal language\">Formal language</a></li>\n<li><a href=\"/wiki/Formal_system\" title=\"Formal system\">Formal system</a></li>\n<li><a href=\"/wiki/Frame_problem\" title=\"Frame problem\">Frame problem</a></li></ul>\n<p>&#32;\n</p>\n</td>\n<td style=\"text-align: left; vertical-align: top;\">\n<ul><li><a href=\"/wiki/Hermeneutics\" title=\"Hermeneutics\">Hermeneutics</a></li>\n<li><a href=\"/wiki/Semantics\" title=\"Semantics\">Interpretation</a></li>\n<li><a href=\"/wiki/Physical_symbol_system\" title=\"Physical symbol system\">Physical symbol system</a></li>\n<li><a href=\"/wiki/Pragmatics\" title=\"Pragmatics\">Pragmatics</a></li>\n<li><a href=\"/wiki/Semantics\" title=\"Semantics\">Semantics</a></li>\n<li><a href=\"/wiki/Semiosis\" title=\"Semiosis\">Semiosis</a></li></ul>\n<p>&#32;\n</p>\n</td>\n<td style=\"text-align: left; vertical-align: top;\">\n<ul><li><a href=\"/wiki/Semiotics\" title=\"Semiotics\">Semiotics</a></li>\n<li><a href=\"/wiki/Sign\" title=\"Sign\">Sign</a></li>\n<li><a href=\"/wiki/Sign_relation\" title=\"Sign relation\">Sign relation</a></li>\n<li><a href=\"/wiki/Situated_cognition\" title=\"Situated cognition\">Situated cognition</a></li>\n<li><a href=\"/wiki/Syntax\" title=\"Syntax\">Syntax</a></li>\n<li><a href=\"/wiki/Turing_machine\" title=\"Turing machine\">Turing machine</a></li></ul>\n<p>&#32;\n</p>\n</td></tr></tbody></table></div>\n<h2><span class=\"mw-headline\" id=\"Notes\">Notes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=14\" title=\"Edit section: Notes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist\" style=\"list-style-type: decimal;\">\n<div class=\"mw-references-wrap\"><ol class=\"references\">\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\">Although this article draws in places upon Frege's view of semantics,  it is very anti-Fregean in stance. Frege was a fierce critic of psychological accounts that attempt to explain meaning in terms of mental states.</span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\">Peirce, Charles S. The philosophy of Peirce: selected writings. New York: AMS Press, 1978.</span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\">Semeiosis and Intentionality T. L. Short Transactions of the Charles S. Peirce Society Vol. 17, No. 3 (Summer, 1981), pp. 197-223</span>\n</li>\n<li id=\"cite_note-4\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-4\">^</a></b></span> <span class=\"reference-text\">C.S. Peirce and artificial intelligence: historical heritage and (new) theoretical stakes; Pierre Steiner; SAPERE - Special Issue on Philosophy and Theory of AI 5:265-276 (2013)</span>\n</li>\n<li id=\"cite_note-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-5\">^</a></b></span> <span class=\"reference-text\">Or, \"imputed\" as read below the dotted baseline of the <a href=\"/wiki/Triangle_of_reference\" title=\"Triangle of reference\">triangle of reference</a> since 1923.</span>\n</li>\n<li id=\"cite_note-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-6\">^</a></b></span> <span class=\"reference-text\">This is the <a href=\"/wiki/Causal_theory_of_reference\" title=\"Causal theory of reference\">causal</a>, <a href=\"/w/index.php?title=Contextual_theory_of_reference&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"Contextual theory of reference (page does not exist)\">contextual theory of reference</a> that <a href=\"/wiki/C._K._Ogden\" class=\"mw-redirect\" title=\"C. K. Ogden\">Ogden</a> &amp; <a href=\"/wiki/I._A._Richards\" title=\"I. A. Richards\">Richards</a> packed in <i><a href=\"/wiki/The_Meaning_of_Meaning\" title=\"The Meaning of Meaning\">The Meaning of Meaning</a></i> (1923).</span>\n</li>\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\">Cf. <a href=\"/wiki/Semantic_externalism\" title=\"Semantic externalism\">semantic externalism</a> as claimed in \"The Meaning of 'Meaning'\" of <i>Mind, Language and Reality</i> (1975) by <a href=\"/wiki/Hilary_Putnam\" title=\"Hilary Putnam\">Putnam</a> who argues: \"Meanings just ain't in the head.\" Now he and <a href=\"/wiki/Michael_Dummett\" title=\"Michael Dummett\">Dummett</a> seem to favor <a href=\"/wiki/Anti-realism\" title=\"Anti-realism\">anti-realism</a> in favor of <a href=\"/wiki/Intuitionism\" title=\"Intuitionism\">intuitionism</a>, <a href=\"/wiki/Psychologism\" title=\"Psychologism\">psychologism</a>, <a href=\"/wiki/Constructivist_epistemology\" title=\"Constructivist epistemology\">constructivism</a> and <a href=\"/wiki/Contextualism\" title=\"Contextualism\">contextualism</a>.</span>\n</li>\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\">Philip N.  Johnson-Laird  \"Procedural Semantics\" (Cognition, 5  (1977) 189; see <a rel=\"nofollow\" class=\"external free\" href=\"http://www.nyu.edu/gsas/dept/philo/courses/mindsandmachines/Papers/procedural.pdf\">http://www.nyu.edu/gsas/dept/philo/courses/mindsandmachines/Papers/procedural.pdf</a>)</span>\n</li>\n<li id=\"cite_note-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-9\">^</a></b></span> <span class=\"reference-text\">William A. Woods. \"Meaning and Links\" (AI Magazine Volume 28 Number 4 (2007); see <a rel=\"nofollow\" class=\"external free\" href=\"http://www.aaai.org/ojs/index.php/aimagazine/article/view/2069/2056\">http://www.aaai.org/ojs/index.php/aimagazine/article/view/2069/2056</a>)</span>\n</li>\n</ol></div></div>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Symbol_grounding_problem&amp;action=edit&amp;section=15\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<dl><dd><b>Note:</b> <i>This article is based on an entry originally published in Nature/Macmillan Encyclopedia of Cognitive Science that has since been revised by the author and the Wikipedia community.</i></dd></dl>\n<ul><li>Blondin Masse, A, G. Chicoisne, Y. Gargouri, S. Harnad, O. Picard, O. Marcotte (2008) <a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/abs/0806.3710\">How Is Meaning Grounded in Dictionary Definitions?</a> <i>TextGraphs-3 Workshop, 22nd International Conference on Computational Linguistics, Coling 2008</i>, Manchester, 18\u201322 August 2008</li>\n<li>Cangelosi, A. &amp; Harnad, S. (2001) <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/2036/\">The Adaptive Advantage of Symbolic Theft Over Sensorimotor Toil: Grounding Language in Perceptual Categories.</a> <i>Evolution of Communication</i> 4(1)  117-142.</li>\n<li>Cangelosi, A.; Greco, A.; Harnad, S. <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/2132/\">From robotic toil to symbolic theft: grounding transfer from entry-level to higher-level categories.</a> <i>Connection Science</i>12(2) 143-62.</li>\n<li><a href=\"/wiki/Jerry_Fodor\" title=\"Jerry Fodor\">Fodor, J. A.</a> (1975) <i>The language of thought</i>. New York: Thomas Y. Crowell</li>\n<li>Frege, G. (1952/1892). On sense and reference. In P. Geach and M. Black, Eds., <i>Translations of the Philosophical Writings of Gottlob Frege</i>. Oxford: Blackwell</li>\n<li>Harnad, S. (1990) <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/3106/\">The Symbol Grounding Problem.</a> <i>Physica D</i> 42: 335-346.</li>\n<li>Harnad, S. (1992) <a rel=\"nofollow\" class=\"external text\" href=\"http://eprints.ecs.soton.ac.uk/6464/\">There Is Only One Mind/Body Problem</a>. Symposium on the Perception of Intentionality, XXV World Congress of Psychology, Brussels, Belgium, July 1992 <i>International Journal of Psychology</i> 27: 521</li>\n<li>Harnad, S. (1994) <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/1592/\">Computation Is Just Interpretable Symbol Manipulation: Cognition Isn't.</a> <i>Minds and Machines</i> 4:379-390 (Special Issue on \"What Is Computation\")</li>\n<li>Harnad, S. (1995) <a rel=\"nofollow\" class=\"external text\" href=\"http://eprints.ecs.soton.ac.uk/3347/\">Why and How We Are Not Zombies.</a> <i>Journal of Consciousness Studies</i> 1: 164-167.</li>\n<li>Harnad, S. (2000) <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/2615/\">Minds, Machines and Turing: The Indistinguishability of Indistinguishables</a>. <i>Journal of Logic, Language, and Information</i> 9(4): 425-445. (Special Issue on \"Alan Turing and Artificial Intelligence\")</li>\n<li>Harnad, S. (2001a) <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/4023/\">Minds, Machines and Searle II: What's Wrong and Right About Searle's Chinese Room Argument?</a> In: M. Bishop &amp; J. Preston (eds.) <i>Essays on Searle's Chinese Room Argument</i>. Oxford University Press.</li>\n<li>Harnad, S. (2001b) <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/1624/\">No Easy Way Out.</a> <i>The Sciences</i> 41(2) 36-42.</li>\n<li>Harnad, Stevan (2001a) <a rel=\"nofollow\" class=\"external text\" href=\"http://eprints.ecs.soton.ac.uk/5943/\">Explaining the Mind: Problems, Problems</a>. <i>The Sciences</i> 41: 36-42.</li>\n<li>Harnad, Stevan (2001b) <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.org/2130/\">The Mind/Body Problem is the Feeling/Function Problem: Harnad on Dennett on Chalmers</a>. Technical Report. Department of Electronics and Computer Sciences. University of Southampton.</li>\n<li>Harnad, S. (2003) <a rel=\"nofollow\" class=\"external text\" href=\"http://eprints.ecs.soton.ac.uk/7718/\">Can a Machine Be Conscious? How?.</a> <i>Journal of Consciousness Studies</i> 10(4-5): 69-75.</li>\n<li>Harnad, S. (2005) <a rel=\"nofollow\" class=\"external text\" href=\"http://eprints.ecs.soton.ac.uk/11725/\">To Cognize is to Categorize: Cognition is categorization.</a> in Lefebvre, C. and Cohen, H., Eds. <i>Handbook of Categorization</i>. Elsevier.</li>\n<li>Harnad, S. (2007) <a rel=\"nofollow\" class=\"external text\" href=\"http://eprints.ecs.soton.ac.uk/7741/\">The Annotation Game: On Turing (1950) on Computing, Machinery and Intelligence.</a> In: Epstein, Robert &amp; Peters, Grace (Eds.) <i>The Turing Test Sourcebook: Philosophical and Methodological Issues in the Quest for the Thinking Computer</i>. Kluwer</li>\n<li>Harnad, S. (2006) <a rel=\"nofollow\" class=\"external text\" href=\"http://eprints.ecs.soton.ac.uk/12092/\">Cohabitation: Computation at 70 Cognition at 20.</a> In Dedrick, D., Eds. <i>Essays in Honour of Zenon Pylyshyn</i>.</li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.macdorman.com\">MacDorman, Karl F.</a> (1999). Grounding symbols through sensorimotor integration. Journal of the Robotics Society of Japan, 17(1), 20-24. <a rel=\"nofollow\" class=\"external text\" href=\"http://www.macdorman.com/kfm/writings/pubs/MacDorman1999GroundingSymbolsSMInteg.pdf\">Online version</a></li>\n<li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.macdorman.com\">MacDorman, Karl F.</a> (2007). Life after the symbol system metaphor. Interaction Studies, 8(1), 143-158. <a rel=\"nofollow\" class=\"external text\" href=\"http://www.macdorman.com/kfm/writings/pubs/MacDorman2007SymbolSystemMetaphor.pdf\">Online version</a></li>\n<li>Pylyshyn, Z. W. (1984) <i>Computation and cognition</i>. Cambridge MA: MIT/Bradford</li>\n<li>Searle, John. R. (1980) <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20150923204614/http://www.class.uh.edu/phil/garson/MindsBrainsandPrograms.pdf\">Minds, brains, and programs.</a> <i>Behavioral and Brain Sciences</i> 3(3): 417-457</li>\n<li>Taddeo, Mariarosaria &amp; <a href=\"/wiki/Luciano_Floridi\" title=\"Luciano Floridi\">Floridi, Luciano</a> (2005). The symbol grounding problem: A critical review of fifteen years of research. <i><a href=\"/wiki/Journal_of_Experimental_and_Theoretical_Artificial_Intelligence\" title=\"Journal of Experimental and Theoretical Artificial Intelligence\">Journal of Experimental and Theoretical Artificial Intelligence</a>, 17</i>(4), 419-445. <a rel=\"nofollow\" class=\"external text\" href=\"http://web.comlab.ox.ac.uk/oucl/research/areas/ieg/research_reports/ieg_rr210605.pdf#search=%22taddeo%20symbol%20grounding%20problem%2\">Online version</a></li>\n<li>Turing, A.M. (1950) <a rel=\"nofollow\" class=\"external text\" href=\"http://cogprints.ecs.soton.ac.uk/archive/00000499/\">Computing Machinery and Intelligence.</a> <i>Mind</i> 49 433-460 [Reprinted in <i>Minds and machines</i>. A. Anderson (ed.), Engelwood Cliffs NJ: Prentice Hall, 1964.]</li></ul>\n<p class=\"mw-empty-elt\">\n</p>\n<!-- \nNewPP limit report\nParsed by mw2237\nCached time: 20180915014441\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.188 seconds\nReal time usage: 0.249 seconds\nPreprocessor visited node count: 1314/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 41075/2097152 bytes\nTemplate argument size: 10283/2097152 bytes\nHighest expansion depth: 16/40\nExpensive parser function count: 7/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 3771/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.064/10.000 seconds\nLua memory usage: 2.15 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  184.281      1 -total\n 61.62%  113.553      6 Template:Ambox\n 43.77%   80.652      1 Template:Multiple_issues\n 21.92%   40.402      3 Template:Citation_needed\n 21.64%   39.871      4 Template:Fix\n 12.43%   22.900      1 Template:COI\n 11.00%   20.280      1 Template:More_footnotes\n 10.02%   18.463      4 Template:Delink\n  9.37%   17.270      7 Template:Category_handler\n  8.26%   15.219      1 Template:Use_dmy_dates\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:3446949-0!canonical and timestamp 20180915014441 and revision id 851685979\n -->\n</div>"},"langlinks":[{"lang":"cs","url":"https://cs.wikipedia.org/wiki/Probl%C3%A9m_ukotven%C3%AD_symbol%C5%AF","langname":"Czech","autonym":"\u010de\u0161tina","*":"Probl\u00e9m ukotven\u00ed symbol\u016f"},{"lang":"et","url":"https://et.wikipedia.org/wiki/S%C3%BCmbolite_p%C3%B5hinemise_probleem","langname":"Estonian","autonym":"eesti","*":"S\u00fcmbolite p\u00f5hinemise probleem"},{"lang":"fr","url":"https://fr.wikipedia.org/wiki/Fondement_des_symboles","langname":"French","autonym":"fran\u00e7ais","*":"Fondement des symboles"},{"lang":"hu","url":"https://hu.wikipedia.org/wiki/A_szimb%C3%B3lum-lehorgonyoz%C3%A1s_probl%C3%A9m%C3%A1ja","langname":"Hungarian","autonym":"magyar","*":"A szimb\u00f3lum-lehorgonyoz\u00e1s probl\u00e9m\u00e1ja"},{"lang":"ja","url":"https://ja.wikipedia.org/wiki/%E3%82%B7%E3%83%B3%E3%83%9C%E3%83%AB%E3%82%B0%E3%83%A9%E3%82%A6%E3%83%B3%E3%83%87%E3%82%A3%E3%83%B3%E3%82%B0%E5%95%8F%E9%A1%8C","langname":"Japanese","autonym":"\u65e5\u672c\u8a9e","*":"\u30b7\u30f3\u30dc\u30eb\u30b0\u30e9\u30a6\u30f3\u30c7\u30a3\u30f3\u30b0\u554f\u984c"},{"lang":"fi","url":"https://fi.wikipedia.org/wiki/Symbolin_maadoittamisen_ongelma","langname":"Finnish","autonym":"suomi","*":"Symbolin maadoittamisen ongelma"}],"categories":[{"sortkey":"Symbol Grounding","hidden":"","*":"Wikipedia_articles_with_possible_conflicts_of_interest_from_September_2014"},{"sortkey":"Symbol Grounding","hidden":"","*":"Articles_lacking_in-text_citations_from_March_2013"},{"sortkey":"Symbol Grounding","hidden":"","*":"All_articles_lacking_in-text_citations"},{"sortkey":"Symbol Grounding","hidden":"","*":"Articles_needing_more_viewpoints_from_December_2010"},{"sortkey":"Symbol Grounding","hidden":"","*":"Articles_with_multiple_maintenance_issues"},{"sortkey":"Symbol Grounding","hidden":"","*":"All_articles_with_unsourced_statements"},{"sortkey":"Symbol Grounding","hidden":"","*":"Articles_with_unsourced_statements_from_October_2016"},{"sortkey":"Symbol Grounding","hidden":"","*":"Wikipedia_articles_needing_clarification_from_October_2016"},{"sortkey":"Symbol Grounding","hidden":"","*":"Articles_to_be_expanded_from_October_2016"},{"sortkey":"Symbol Grounding","hidden":"","*":"All_articles_to_be_expanded"},{"sortkey":"Symbol Grounding","hidden":"","*":"Articles_using_small_message_boxes"},{"sortkey":"Symbol Grounding","hidden":"","*":"Use_dmy_dates_from_December_2010"},{"sortkey":"Symbol Grounding","*":"Cognitive_science"},{"sortkey":"Symbol Grounding","*":"Symbolism"},{"sortkey":"Symbol Grounding","*":"Arguments_in_philosophy_of_mind"},{"sortkey":"Symbol Grounding","*":"Semantics"},{"sortkey":"Symbol Grounding","*":"Philosophy_of_language"}],"links":[{"ns":14,"exists":"","*":"Category:Wikipedia articles with possible conflicts of interest from September 2014"},{"ns":14,"exists":"","*":"Category:Articles lacking in-text citations from March 2013"},{"ns":14,"exists":"","*":"Category:Articles needing more viewpoints from December 2010"},{"ns":14,"exists":"","*":"Category:Articles with unsourced statements from October 2016"},{"ns":14,"exists":"","*":"Category:Wikipedia articles needing clarification from October 2016"},{"ns":14,"exists":"","*":"Category:Articles to be expanded from October 2016"},{"ns":14,"exists":"","*":"Category:Use dmy dates from December 2010"},{"ns":0,"exists":"","*":"Affordance"},{"ns":0,"exists":"","*":"Anti-realism"},{"ns":0,"exists":"","*":"Antonio Damasio"},{"ns":0,"exists":"","*":"Binding problem"},{"ns":0,"exists":"","*":"C. K. Ogden"},{"ns":0,"exists":"","*":"Categorical perception"},{"ns":0,"exists":"","*":"Causal theory of reference"},{"ns":0,"exists":"","*":"Charles Saunders Peirce"},{"ns":0,"exists":"","*":"Chinese room"},{"ns":0,"exists":"","*":"Communicative action"},{"ns":0,"exists":"","*":"Computational theory of mind"},{"ns":0,"exists":"","*":"Consciousness"},{"ns":0,"exists":"","*":"Constructivist epistemology"},{"ns":0,"exists":"","*":"Contextualism"},{"ns":0,"exists":"","*":"Dictionary"},{"ns":0,"exists":"","*":"Dualism (philosophy of mind)"},{"ns":0,"exists":"","*":"Dynamical system"},{"ns":0,"exists":"","*":"Formal language"},{"ns":0,"exists":"","*":"Formal system"},{"ns":0,"exists":"","*":"Frame problem"},{"ns":0,"exists":"","*":"Franz Brentano"},{"ns":0,"exists":"","*":"Functionalism (philosophy of mind)"},{"ns":0,"exists":"","*":"Gottlob Frege"},{"ns":0,"exists":"","*":"Hermeneutics"},{"ns":0,"exists":"","*":"Hilary Putnam"},{"ns":0,"exists":"","*":"Homeostasis"},{"ns":0,"exists":"","*":"I. A. Richards"},{"ns":0,"exists":"","*":"Intentionality"},{"ns":0,"exists":"","*":"Intuitionism"},{"ns":0,"exists":"","*":"Jerry Fodor"},{"ns":0,"exists":"","*":"John Searle"},{"ns":0,"exists":"","*":"Journal of Experimental and Theoretical Artificial Intelligence"},{"ns":0,"exists":"","*":"Luciano Floridi"},{"ns":0,"exists":"","*":"Meaning (psychology)"},{"ns":0,"exists":"","*":"Mental state"},{"ns":0,"exists":"","*":"Michael Dummett"},{"ns":0,"exists":"","*":"Natural language"},{"ns":0,"exists":"","*":"Philosophical Investigations"},{"ns":0,"exists":"","*":"Philosophical zombie"},{"ns":0,"exists":"","*":"Physical symbol system"},{"ns":0,"exists":"","*":"Pragmatics"},{"ns":0,"exists":"","*":"Psychokinesis"},{"ns":0,"exists":"","*":"Psychologism"},{"ns":0,"exists":"","*":"Semantic externalism"},{"ns":0,"exists":"","*":"Semantics"},{"ns":0,"exists":"","*":"Semiosis"},{"ns":0,"exists":"","*":"Semiotics"},{"ns":0,"exists":"","*":"Sentience"},{"ns":0,"exists":"","*":"Servomechanism"},{"ns":0,"exists":"","*":"Sign"},{"ns":0,"exists":"","*":"Sign relation"},{"ns":0,"exists":"","*":"Situated cognition"},{"ns":0,"exists":"","*":"Somatic marker hypothesis"},{"ns":0,"exists":"","*":"Symbol"},{"ns":0,"exists":"","*":"Syntax"},{"ns":0,"exists":"","*":"The Meaning of Meaning"},{"ns":0,"exists":"","*":"Triangle of reference"},{"ns":0,"exists":"","*":"Turing machine"},{"ns":0,"exists":"","*":"Turing test"},{"ns":0,"exists":"","*":"Word"},{"ns":0,"*":"Contextual theory of reference"},{"ns":1,"exists":"","*":"Talk:Symbol grounding problem"},{"ns":4,"exists":"","*":"Wikipedia:Citation needed"},{"ns":4,"exists":"","*":"Wikipedia:Citing sources"},{"ns":4,"exists":"","*":"Wikipedia:Cleanup"},{"ns":4,"exists":"","*":"Wikipedia:Conflict of interest"},{"ns":4,"exists":"","*":"Wikipedia:Neutral point of view"},{"ns":4,"exists":"","*":"Wikipedia:Please clarify"},{"ns":4,"exists":"","*":"Wikipedia:When to cite"},{"ns":4,"exists":"","*":"Wikipedia:WikiProject Fact and Reference Check"},{"ns":12,"exists":"","*":"Help:Maintenance template removal"}],"templates":[{"ns":10,"exists":"","*":"Template:Multiple issues"},{"ns":10,"exists":"","*":"Template:Ambox"},{"ns":10,"exists":"","*":"Template:COI"},{"ns":10,"exists":"","*":"Template:More footnotes"},{"ns":10,"exists":"","*":"Template:Yesno-no"},{"ns":10,"exists":"","*":"Template:Yesno"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":10,"exists":"","*":"Template:Unbalanced"},{"ns":10,"exists":"","*":"Template:Citation needed"},{"ns":10,"exists":"","*":"Template:Fix"},{"ns":10,"exists":"","*":"Template:Category handler"},{"ns":10,"exists":"","*":"Template:Fix/category"},{"ns":10,"exists":"","*":"Template:Delink"},{"ns":10,"exists":"","*":"Template:Why"},{"ns":10,"exists":"","*":"Template:Expand section"},{"ns":10,"exists":"","*":"Template:Main"},{"ns":10,"exists":"","*":"Template:Col-begin"},{"ns":10,"exists":"","*":"Template:Col-break"},{"ns":10,"exists":"","*":"Template:Col-end"},{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Use dmy dates"},{"ns":10,"exists":"","*":"Template:DMCA"},{"ns":10,"exists":"","*":"Template:Dated maintenance category"},{"ns":10,"exists":"","*":"Template:FULLROOTPAGENAME"},{"ns":10,"exists":"","*":"Template:Ns has subpages"},{"ns":828,"exists":"","*":"Module:Message box"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Message box/configuration"},{"ns":828,"exists":"","*":"Module:Unsubst"},{"ns":828,"exists":"","*":"Module:Category handler"},{"ns":828,"exists":"","*":"Module:Category handler/data"},{"ns":828,"exists":"","*":"Module:Category handler/config"},{"ns":828,"exists":"","*":"Module:Category handler/shared"},{"ns":828,"exists":"","*":"Module:Category handler/blacklist"},{"ns":828,"exists":"","*":"Module:Namespace detect/data"},{"ns":828,"exists":"","*":"Module:Namespace detect/config"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"},{"ns":828,"exists":"","*":"Module:String"},{"ns":828,"exists":"","*":"Module:Delink"},{"ns":828,"exists":"","*":"Module:Main"},{"ns":828,"exists":"","*":"Module:Hatnote"},{"ns":828,"exists":"","*":"Module:Hatnote list"},{"ns":828,"exists":"","*":"Module:Ns has subpages"}],"images":["Ambox_important.svg","Unbalanced_scales.svg","Text_document_with_red_question_mark.svg","Wiki_letter_w_cropped.svg"],"externallinks":["http://www.nyu.edu/gsas/dept/philo/courses/mindsandmachines/Papers/procedural.pdf","http://www.aaai.org/ojs/index.php/aimagazine/article/view/2069/2056","http://cogprints.org/3106/","http://plato.stanford.edu/entries/chinese-room/#4.1","http://cogprints.org/4023/","https://arxiv.org/abs/0806.3710","http://cogprints.org/2036/","http://cogprints.org/2132/","http://eprints.ecs.soton.ac.uk/6464/","http://cogprints.org/1592/","http://eprints.ecs.soton.ac.uk/3347/","http://cogprints.org/2615/","http://cogprints.org/1624/","http://eprints.ecs.soton.ac.uk/5943/","http://cogprints.org/2130/","http://eprints.ecs.soton.ac.uk/7718/","http://eprints.ecs.soton.ac.uk/11725/","http://eprints.ecs.soton.ac.uk/7741/","http://eprints.ecs.soton.ac.uk/12092/","http://www.macdorman.com","http://www.macdorman.com/kfm/writings/pubs/MacDorman1999GroundingSymbolsSMInteg.pdf","http://www.macdorman.com/kfm/writings/pubs/MacDorman2007SymbolSystemMetaphor.pdf","https://web.archive.org/web/20150923204614/http://www.class.uh.edu/phil/garson/MindsBrainsandPrograms.pdf","http://web.comlab.ox.ac.uk/oucl/research/areas/ieg/research_reports/ieg_rr210605.pdf#search=%22taddeo%20symbol%20grounding%20problem%2","http://cogprints.ecs.soton.ac.uk/archive/00000499/"],"sections":[{"toclevel":1,"level":"2","line":"Background","number":"1","index":"1","fromtitle":"Symbol_grounding_problem","byteoffset":449,"anchor":"Background"},{"toclevel":2,"level":"3","line":"Referents","number":"1.1","index":"2","fromtitle":"Symbol_grounding_problem","byteoffset":465,"anchor":"Referents"},{"toclevel":2,"level":"3","line":"Referential process","number":"1.2","index":"3","fromtitle":"Symbol_grounding_problem","byteoffset":2139,"anchor":"Referential_process"},{"toclevel":2,"level":"3","line":"Grounding process","number":"1.3","index":"4","fromtitle":"Symbol_grounding_problem","byteoffset":4576,"anchor":"Grounding_process"},{"toclevel":2,"level":"3","line":"Requirements for symbol grounding","number":"1.4","index":"5","fromtitle":"Symbol_grounding_problem","byteoffset":6212,"anchor":"Requirements_for_symbol_grounding"},{"toclevel":3,"level":"4","line":"Capacity to pick out referents","number":"1.4.1","index":"6","fromtitle":"Symbol_grounding_problem","byteoffset":6656,"anchor":"Capacity_to_pick_out_referents"},{"toclevel":3,"level":"4","line":"Consciousness","number":"1.4.2","index":"7","fromtitle":"Symbol_grounding_problem","byteoffset":9292,"anchor":"Consciousness"},{"toclevel":1,"level":"2","line":"Formulation","number":"2","index":"8","fromtitle":"Symbol_grounding_problem","byteoffset":12377,"anchor":"Formulation"},{"toclevel":1,"level":"2","line":"Functionalism","number":"3","index":"9","fromtitle":"Symbol_grounding_problem","byteoffset":12557,"anchor":"Functionalism"},{"toclevel":1,"level":"2","line":"Searle's Chinese room argument","number":"4","index":"10","fromtitle":"Symbol_grounding_problem","byteoffset":14231,"anchor":"Searle's_Chinese_room_argument"},{"toclevel":2,"level":"3","line":"Implications","number":"4.1","index":"11","fromtitle":"Symbol_grounding_problem","byteoffset":16292,"anchor":"Implications"},{"toclevel":1,"level":"2","line":"Brentano's notion of intentionality","number":"5","index":"12","fromtitle":"Symbol_grounding_problem","byteoffset":18820,"anchor":"Brentano's_notion_of_intentionality"},{"toclevel":1,"level":"2","line":"See also","number":"6","index":"13","fromtitle":"Symbol_grounding_problem","byteoffset":23896,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"Notes","number":"7","index":"14","fromtitle":"Symbol_grounding_problem","byteoffset":24371,"anchor":"Notes"},{"toclevel":1,"level":"2","line":"References","number":"8","index":"15","fromtitle":"Symbol_grounding_problem","byteoffset":24394,"anchor":"References"}],"parsewarnings":[],"displaytitle":"Symbol grounding problem","iwlinks":[],"properties":[{"name":"defaultsort","*":"Symbol Grounding"},{"name":"wikibase_item","*":"Q246989"}]}}