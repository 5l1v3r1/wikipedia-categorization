{"parse":{"title":"Semantic compression","pageid":34087348,"revid":850128404,"text":{"*":"<div class=\"mw-parser-output\"><p>In <a href=\"/wiki/Natural_language_processing\" title=\"Natural language processing\">natural language processing</a>, <b>semantic compression</b> is a process of compacting a lexicon used to build \na textual document (or a set of documents) by reducing language heterogeneity, while maintaining text <a href=\"/wiki/Semantics\" title=\"Semantics\">semantics</a>. \nAs a result, the same ideas can be represented using a smaller set of words.\n</p><p>In most applications, semantic compression is a lossy compression, that is, increased prolixity does not compensate for the lexical compression, and an original document cannot be reconstructed in a reverse process.\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#By_generalization\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">By generalization</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Implicit_semantic_compression\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Implicit semantic compression</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Applications_and_advantages\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Applications and advantages</span></a></li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#See_also\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#References\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">References</span></a></li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#External_links\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"By_generalization\">By generalization</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Semantic_compression&amp;action=edit&amp;section=1\" title=\"Edit section: By generalization\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Semantic compression is basically achieved in two steps, using <a href=\"/wiki/Frequency_list\" class=\"mw-redirect\" title=\"Frequency list\">frequency dictionaries</a> and <a href=\"/wiki/Semantic_network\" title=\"Semantic network\">semantic network</a>:\n</p>\n<ol><li>determining cumulated term frequencies to identify target lexicon,</li>\n<li>replacing less frequent terms with their hypernyms (<a href=\"/wiki/Generalization\" title=\"Generalization\">generalization</a>) from target lexicon.<sup id=\"cite_ref-1\" class=\"reference\"><a href=\"#cite_note-1\">&#91;1&#93;</a></sup></li></ol>\n<p>Step 1 requires assembling word frequencies and \ninformation on semantic relationships, specifically <a href=\"/wiki/Hyponymy\" class=\"mw-redirect\" title=\"Hyponymy\">hyponymy</a>. Moving upwards in word hierarchy, \na cumulative concept frequency is calculating by adding a sum of hyponyms' frequencies to frequency of their hypernym:\n<span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle cumf(k_{i})=f(k_{i})+\\sum _{j}cumf(k_{j})}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>c</mi>\n        <mi>u</mi>\n        <mi>m</mi>\n        <mi>f</mi>\n        <mo stretchy=\"false\">(</mo>\n        <msub>\n          <mi>k</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mo stretchy=\"false\">)</mo>\n        <mo>=</mo>\n        <mi>f</mi>\n        <mo stretchy=\"false\">(</mo>\n        <msub>\n          <mi>k</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n        <mo stretchy=\"false\">)</mo>\n        <mo>+</mo>\n        <munder>\n          <mo>&#x2211;<!-- \u2211 --></mo>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>j</mi>\n          </mrow>\n        </munder>\n        <mi>c</mi>\n        <mi>u</mi>\n        <mi>m</mi>\n        <mi>f</mi>\n        <mo stretchy=\"false\">(</mo>\n        <msub>\n          <mi>k</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle cumf(k_{i})=f(k_{i})+\\sum _{j}cumf(k_{j})}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e462f27df95a14a4fa31b77e4e2bb0be38fd9aaa\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -3.338ex; width:33.841ex; height:5.843ex;\" alt=\"cum f(k_{i}) = f(k_{i}) + \\sum_{j} cum f(k_{j})\"/></span> where <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle k_{i}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <msub>\n          <mi>k</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>i</mi>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle k_{i}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/f29138ed3ad54ffce527daccadc49c520459b0b0\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.671ex; width:2.011ex; height:2.509ex;\" alt=\"k_{{i}}\"/></span> is a hypernym of <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle k_{j}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <msub>\n          <mi>k</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>j</mi>\n          </mrow>\n        </msub>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle k_{j}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/05ddf2c6d7759ac955e001a7cfafb2abfca41b0b\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -1.005ex; width:2.121ex; height:2.843ex;\" alt=\"k_{j}\"/></span>.\nThen, a desired number of words with top cumulated frequencies are chosen to build a targed lexicon.\n</p><p>In the second step, compression mapping rules are defined for the remaining words, in order to handle every occurrence \nof a less frequent hyponym as its hypernym in output text.\n</p>\n<dl><dt>Example</dt></dl>\n<p>The below fragment of text has been processed by the semantic compression. Words in bold have been replaced by their hypernyms.\n</p>\n<blockquote><p>They are both <b>nest</b> building <b>social insects</b>, but <b>paper wasps</b> and honey <b>bees</b> <b>organize</b> their <b>colonies</b> \n</p><p>in very different <b>ways</b>. In a new study, researchers report that despite their <b>differences</b>, these insects \n<b>rely on</b> the same network of genes to guide their <b>social behavior</b>.The study appears in the Proceedings of the \n<b>Royal Society B</b>: Biological Sciences. Honey <b>bees</b> and <b>paper wasps</b> are separated by more than 100 million years of \n</p>\n<p><b>evolution</b>, and there are <b>striking differences</b> in how they divvy up the work of <b>maintaining</b> a <b>colony</b>.</p></blockquote>\n<p>The procedure outputs the following text:\n</p>\n<blockquote><p>They are both <b>facility</b> building <b>insect</b>, but <b>insect</b>s and honey <b>insects</b> <b>arrange</b> their <b>biological groups</b> \n</p><p>in very different <b>structure</b>. In a new study, researchers report that despite their <b>difference of opinions</b>, these insects \n<b>act</b> the same network of genes to <b>steer</b> their <b>party demeanor</b>. The study appears in the proceeding of the \n<b>institution bacteria</b> Biological Sciences. Honey <b>insects</b> and <b>insect</b> are separated by more than hundred million years of \n</p>\n<p><b>organic processes</b>, and there are <b>impinging differences of opinions</b> in how they divvy up the work of <b>affirming</b> a <b>biological group</b>.</p></blockquote>\n<h2><span class=\"mw-headline\" id=\"Implicit_semantic_compression\">Implicit semantic compression</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Semantic_compression&amp;action=edit&amp;section=2\" title=\"Edit section: Implicit semantic compression\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>A natural tendency to keep natural language expressions concise can be perceived as a form of implicit semantic compression, by omitting unmeaningful words or redundant meaningful words (especially to avoid <a href=\"/wiki/Pleonasm\" title=\"Pleonasm\">pleonasms</a>).<sup id=\"cite_ref-2\" class=\"reference\"><a href=\"#cite_note-2\">&#91;2&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Applications_and_advantages\">Applications and advantages</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Semantic_compression&amp;action=edit&amp;section=3\" title=\"Edit section: Applications and advantages\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>In the <a href=\"/wiki/Vector_space_model\" title=\"Vector space model\">vector space model</a>, compacting a lexicon leads to a reduction of <a href=\"/wiki/Curse_of_dimensionality\" title=\"Curse of dimensionality\">dimensionality</a>, which results in less \n<a href=\"/wiki/Computational_complexity_theory\" title=\"Computational complexity theory\">computational complexity</a> and a positive influence on efficiency. \n</p><p>Semantic compression is advantageous in <a href=\"/wiki/Information_retrieval\" title=\"Information retrieval\">information retrieval</a> tasks, improving their effectiveness (in terms of both precision and recall).<sup id=\"cite_ref-3\" class=\"reference\"><a href=\"#cite_note-3\">&#91;3&#93;</a></sup> This is due to more precise descriptors (reduced effect of language diversity \u2013 limited language redundancy, a step towards a controlled dictionary).\n</p><p>As in the example above, it is possible to display the output as natural text (re-applying inflexion, adding stop words).\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Semantic_compression&amp;action=edit&amp;section=4\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Information_theory\" title=\"Information theory\">Information theory</a></li>\n<li><a href=\"/wiki/Lexical_substitution\" title=\"Lexical substitution\">Lexical substitution</a></li>\n<li><a href=\"/wiki/Quantities_of_information\" title=\"Quantities of information\">Quantities of information</a></li>\n<li><a href=\"/wiki/Text_simplification\" title=\"Text simplification\">Text simplification</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Semantic_compression&amp;action=edit&amp;section=5\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"mw-references-wrap\"><ol class=\"references\">\n<li id=\"cite_note-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-1\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"https://dx.doi.org/10.1007/978-3-642-12090-9_10\">D. Ceglarek, K. Haniewicz, W. Rutkowski, Semantic Compression for Specialised Information Retrieval Systems</a>, Advances in Intelligent Information and Database Systems, vol. 283, p. 111-121, 2010</span>\n</li>\n<li id=\"cite_note-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-2\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"https://dx.doi.org/10.3115/990100.990155\">N. N. Percova, On the types of semantic compression of text</a>,\nCOLING '82 Proceedings of the 9th Conference on Computational Linguistics, vol. 2, p. 229-231, 1982</span>\n</li>\n<li id=\"cite_note-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-3\">^</a></b></span> <span class=\"reference-text\"><a rel=\"nofollow\" class=\"external text\" href=\"http://dl.acm.org/citation.cfm?id=1947662.1947683\">D. Ceglarek, K. Haniewicz, W. Rutkowski, Quality of semantic compression in classification</a> Proceedings of the 2nd International Conference on Computational Collective Intelligence: Technologies and Applications, vol. 1, p. 162-171, 2010</span>\n</li>\n</ol></div>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Semantic_compression&amp;action=edit&amp;section=6\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"http://semantic.net.pl/semantic_compression.php\">Semantic compression on Project SENECA (Semantic Networks and Categorization) website</a></li></ul>\n\n<!-- \nNewPP limit report\nParsed by mw2239\nCached time: 20180918183020\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.024 seconds\nReal time usage: 0.278 seconds\nPreprocessor visited node count: 84/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 0/2097152 bytes\nTemplate argument size: 0/2097152 bytes\nHighest expansion depth: 2/40\nExpensive parser function count: 0/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 1716/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%    0.000      1 -total\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:34087348-0!canonical!math=5 and timestamp 20180918183020 and revision id 850128404\n -->\n</div>"},"langlinks":[],"categories":[{"sortkey":"","*":"Information_retrieval_techniques"},{"sortkey":"","*":"Natural_language_processing"},{"sortkey":"","*":"Quantitative_linguistics"},{"sortkey":"","*":"Computational_linguistics"}],"links":[{"ns":0,"exists":"","*":"Computational complexity theory"},{"ns":0,"exists":"","*":"Curse of dimensionality"},{"ns":0,"exists":"","*":"Frequency list"},{"ns":0,"exists":"","*":"Generalization"},{"ns":0,"exists":"","*":"Hyponymy"},{"ns":0,"exists":"","*":"Information retrieval"},{"ns":0,"exists":"","*":"Information theory"},{"ns":0,"exists":"","*":"Lexical substitution"},{"ns":0,"exists":"","*":"Natural language processing"},{"ns":0,"exists":"","*":"Pleonasm"},{"ns":0,"exists":"","*":"Quantities of information"},{"ns":0,"exists":"","*":"Semantic network"},{"ns":0,"exists":"","*":"Semantics"},{"ns":0,"exists":"","*":"Text simplification"},{"ns":0,"exists":"","*":"Vector space model"}],"templates":[],"images":[],"externallinks":["https://dx.doi.org/10.1007/978-3-642-12090-9_10","https://dx.doi.org/10.3115/990100.990155","http://dl.acm.org/citation.cfm?id=1947662.1947683","http://semantic.net.pl/semantic_compression.php"],"sections":[{"toclevel":1,"level":"2","line":"By generalization","number":"1","index":"1","fromtitle":"Semantic_compression","byteoffset":528,"anchor":"By_generalization"},{"toclevel":1,"level":"2","line":"Implicit semantic compression","number":"2","index":"2","fromtitle":"Semantic_compression","byteoffset":3329,"anchor":"Implicit_semantic_compression"},{"toclevel":1,"level":"2","line":"Applications and advantages","number":"3","index":"3","fromtitle":"Semantic_compression","byteoffset":3801,"anchor":"Applications_and_advantages"},{"toclevel":1,"level":"2","line":"See also","number":"4","index":"4","fromtitle":"Semantic_compression","byteoffset":4796,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"References","number":"5","index":"5","fromtitle":"Semantic_compression","byteoffset":4920,"anchor":"References"},{"toclevel":1,"level":"2","line":"External links","number":"6","index":"6","fromtitle":"Semantic_compression","byteoffset":4950,"anchor":"External_links"}],"parsewarnings":[],"displaytitle":"Semantic compression","iwlinks":[],"properties":[{"name":"wikibase_item","*":"Q7449050"}]}}