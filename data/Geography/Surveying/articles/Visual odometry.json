{"parse":{"title":"Visual odometry","pageid":18315951,"revid":855663633,"text":{"*":"<div class=\"mw-parser-output\"><div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a href=\"/wiki/File:Optical_flow_example_v2.png\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/en/thumb/1/10/Optical_flow_example_v2.png/220px-Optical_flow_example_v2.png\" width=\"220\" height=\"102\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/en/thumb/1/10/Optical_flow_example_v2.png/330px-Optical_flow_example_v2.png 1.5x, //upload.wikimedia.org/wikipedia/en/thumb/1/10/Optical_flow_example_v2.png/440px-Optical_flow_example_v2.png 2x\" data-file-width=\"496\" data-file-height=\"230\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:Optical_flow_example_v2.png\" class=\"internal\" title=\"Enlarge\"></a></div>The <a href=\"/wiki/Optical_flow\" title=\"Optical flow\">optical flow</a> vector of a moving object in a video sequence.</div></div></div>\n<p>In <a href=\"/wiki/Robotics\" title=\"Robotics\">robotics</a> and <a href=\"/wiki/Computer_vision\" title=\"Computer vision\">computer vision</a>, <b>visual odometry</b> is the process of determining the position and orientation of a robot by analyzing the associated camera images. It has been used in a wide variety of robotic applications, such as on the <a href=\"/wiki/Mars_Exploration_Rover\" title=\"Mars Exploration Rover\">Mars Exploration Rovers</a>.<sup id=\"cite_ref-Maimone2007_1-0\" class=\"reference\"><a href=\"#cite_note-Maimone2007-1\">&#91;1&#93;</a></sup>\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Overview\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Overview</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Types\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Types</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-3\"><a href=\"#Monocular_and_Stereo\"><span class=\"tocnumber\">2.1</span> <span class=\"toctext\">Monocular and Stereo</span></a></li>\n<li class=\"toclevel-2 tocsection-4\"><a href=\"#Feature_Based_and_Direct_Method\"><span class=\"tocnumber\">2.2</span> <span class=\"toctext\">Feature Based and Direct Method</span></a></li>\n<li class=\"toclevel-2 tocsection-5\"><a href=\"#Visual_Inertial_Odometry\"><span class=\"tocnumber\">2.3</span> <span class=\"toctext\">Visual Inertial Odometry</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#Algorithm\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Algorithm</span></a></li>\n<li class=\"toclevel-1 tocsection-7\"><a href=\"#Egomotion\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Egomotion</span></a>\n<ul>\n<li class=\"toclevel-2 tocsection-8\"><a href=\"#Overview_2\"><span class=\"tocnumber\">4.1</span> <span class=\"toctext\">Overview</span></a></li>\n</ul>\n</li>\n<li class=\"toclevel-1 tocsection-9\"><a href=\"#See_also\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">See also</span></a></li>\n<li class=\"toclevel-1 tocsection-10\"><a href=\"#References\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">References</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Overview\">Overview</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=1\" title=\"Edit section: Overview\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>In <a href=\"/wiki/Navigation\" title=\"Navigation\">navigation</a>, <a href=\"/wiki/Odometry\" title=\"Odometry\">odometry</a> is the use of data from the movement of actuators to estimate change in position over time through devices such as <a href=\"/wiki/Rotary_encoder\" title=\"Rotary encoder\">rotary encoders</a> to measure wheel rotations. While useful for many wheeled or tracked vehicles, traditional odometry techniques cannot be applied to <a href=\"/wiki/Mobile_robot\" title=\"Mobile robot\">mobile robots</a> with non-standard locomotion methods, such as legged robots. In addition, odometry universally suffers from precision problems, since wheels tend to slip and slide on the floor creating a non-uniform distance traveled as compared to the wheel rotations. The error is compounded when the vehicle operates on non-smooth surfaces. Odometry readings become increasingly unreliable as these errors accumulate and compound over time.\n</p><p>Visual odometry is the process of determining equivalent odometry information using sequential camera images to estimate the distance traveled. Visual odometry allows for enhanced navigational accuracy in robots or vehicles using any type of locomotion on any surface.\n</p>\n<h2><span class=\"mw-headline\" id=\"Types\">Types</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=2\" title=\"Edit section: Types\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>There are various types of VO.\n</p>\n<h3><span class=\"mw-headline\" id=\"Monocular_and_Stereo\">Monocular and Stereo</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=3\" title=\"Edit section: Monocular and Stereo\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Depend on the camera setup, VO can be categorized as Monocular VO (single camera), Stereo VO (two camera in stereo setup).\n</p>\n<div class=\"thumb tright\"><div class=\"thumbinner\" style=\"width:222px;\"><a href=\"/wiki/File:VIO_sensor_in_various_commercial_quadcopters_.jpg\" class=\"image\"><img alt=\"\" src=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/VIO_sensor_in_various_commercial_quadcopters_.jpg/220px-VIO_sensor_in_various_commercial_quadcopters_.jpg\" width=\"220\" height=\"220\" class=\"thumbimage\" srcset=\"//upload.wikimedia.org/wikipedia/commons/thumb/c/cd/VIO_sensor_in_various_commercial_quadcopters_.jpg/330px-VIO_sensor_in_various_commercial_quadcopters_.jpg 1.5x, //upload.wikimedia.org/wikipedia/commons/thumb/c/cd/VIO_sensor_in_various_commercial_quadcopters_.jpg/440px-VIO_sensor_in_various_commercial_quadcopters_.jpg 2x\" data-file-width=\"3072\" data-file-height=\"3072\" /></a>  <div class=\"thumbcaption\"><div class=\"magnify\"><a href=\"/wiki/File:VIO_sensor_in_various_commercial_quadcopters_.jpg\" class=\"internal\" title=\"Enlarge\"></a></div>VIO is widely used in commercial quadcopters, which provide localization in GPS denied situations</div></div></div>\n<h3><span class=\"mw-headline\" id=\"Feature_Based_and_Direct_Method\">Feature Based and Direct Method</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=4\" title=\"Edit section: Feature Based and Direct Method\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>Traditional VO's visual information is obtained by Feature Based Method, which extract image feature points and tracking them in the image sequence. Recent development in VO research provided an alternative, called Direct Method, which uses pixel intensity in the image sequence directly as visual input. There are also hybrid methods.\n</p>\n<h3><span class=\"mw-headline\" id=\"Visual_Inertial_Odometry\">Visual Inertial Odometry</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=5\" title=\"Edit section: Visual Inertial Odometry\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>If an <a href=\"/wiki/Inertial_measurement_unit\" title=\"Inertial measurement unit\">inertial measurement unit</a> (IMU) is used within the VO system, it is commonly referred to as Visual Inertial Odometry (VIO).\n</p>\n<h2><span class=\"mw-headline\" id=\"Algorithm\">Algorithm</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=6\" title=\"Edit section: Algorithm\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Most existing approaches to visual odometry are based on the following stages.\n</p>\n<ol><li>Acquire input images: using either <a href=\"/wiki/Single_camera\" class=\"mw-redirect\" title=\"Single camera\">single cameras</a>.,<sup id=\"cite_ref-ChhaniyaraS_2-0\" class=\"reference\"><a href=\"#cite_note-ChhaniyaraS-2\">&#91;2&#93;</a></sup><sup id=\"cite_ref-Nister:2004p211_3-0\" class=\"reference\"><a href=\"#cite_note-Nister:2004p211-3\">&#91;3&#93;</a></sup> <a href=\"/wiki/Stereo_camera\" title=\"Stereo camera\">stereo cameras</a>,<sup id=\"cite_ref-Nister:2004p211_3-1\" class=\"reference\"><a href=\"#cite_note-Nister:2004p211-3\">&#91;3&#93;</a></sup><sup id=\"cite_ref-Comport10_4-0\" class=\"reference\"><a href=\"#cite_note-Comport10-4\">&#91;4&#93;</a></sup> or <a href=\"/wiki/Omnidirectional_camera\" title=\"Omnidirectional camera\">omnidirectional cameras</a>.<sup id=\"cite_ref-ScaramuzzaIEEE-TRO08_5-0\" class=\"reference\"><a href=\"#cite_note-ScaramuzzaIEEE-TRO08-5\">&#91;5&#93;</a></sup><sup id=\"cite_ref-Corke_6-0\" class=\"reference\"><a href=\"#cite_note-Corke-6\">&#91;6&#93;</a></sup></li>\n<li>Image correction: apply <a href=\"/wiki/Image_processing\" class=\"mw-redirect\" title=\"Image processing\">image processing</a> techniques for lens distortion removal, etc.</li>\n<li><a href=\"/wiki/Feature_detection_(computer_vision)\" title=\"Feature detection (computer vision)\">Feature detection</a>: define interest operators, and match features across frames and construct <a href=\"/wiki/Optical_flow\" title=\"Optical flow\">optical flow</a> field.\n<ol><li>Use correlation to establish correspondence of two images, and no long term feature tracking.</li>\n<li><a href=\"/wiki/Feature_extraction\" title=\"Feature extraction\">Feature extraction</a> and correlation.</li>\n<li>Construct optical flow field (<a href=\"/wiki/Lucas%E2%80%93Kanade_method\" title=\"Lucas\u2013Kanade method\">Lucas\u2013Kanade method</a>).</li></ol></li>\n<li>Check flow field vectors for potential tracking errors and remove outliers.<sup id=\"cite_ref-Campbell_7-0\" class=\"reference\"><a href=\"#cite_note-Campbell-7\">&#91;7&#93;</a></sup></li>\n<li>Estimation of the camera motion from the optical flow.<sup id=\"cite_ref-Sunderhauf2005_8-0\" class=\"reference\"><a href=\"#cite_note-Sunderhauf2005-8\">&#91;8&#93;</a></sup><sup id=\"cite_ref-Konolige2006_9-0\" class=\"reference\"><a href=\"#cite_note-Konolige2006-9\">&#91;9&#93;</a></sup><sup id=\"cite_ref-Olson2002_10-0\" class=\"reference\"><a href=\"#cite_note-Olson2002-10\">&#91;10&#93;</a></sup><sup id=\"cite_ref-Cheng2006_11-0\" class=\"reference\"><a href=\"#cite_note-Cheng2006-11\">&#91;11&#93;</a></sup>\n<ol><li>Choice 1: <a href=\"/wiki/Kalman_filter\" title=\"Kalman filter\">Kalman filter</a> for state estimate distribution maintenance.</li>\n<li>Choice 2: find the geometric and 3D properties of the features that minimize a <a href=\"/wiki/Loss_function\" title=\"Loss function\">cost function</a> based on the re-projection error between two adjacent images. This can be done by mathematical minimization or <a href=\"/wiki/Random_sampling\" class=\"mw-redirect\" title=\"Random sampling\">random sampling</a>.</li></ol></li>\n<li>Periodic repopulation of trackpoints to maintain coverage across the image.</li></ol>\n<p>An alternative to feature-based methods is the \"direct\" or appearance-based visual odometry technique which minimizes an error directly in sensor space and subsequently avoids feature matching and extraction.<sup id=\"cite_ref-Comport10_4-1\" class=\"reference\"><a href=\"#cite_note-Comport10-4\">&#91;4&#93;</a></sup><sup id=\"cite_ref-12\" class=\"reference\"><a href=\"#cite_note-12\">&#91;12&#93;</a></sup><sup id=\"cite_ref-13\" class=\"reference\"><a href=\"#cite_note-13\">&#91;13&#93;</a></sup>\n</p><p>Another method, coined 'visiodometry' estimates the planar roto-translations between images using <a href=\"/wiki/Phase_correlation\" title=\"Phase correlation\">Phase correlation</a> instead of extracting features.<sup id=\"cite_ref-ZamanICRA_14-0\" class=\"reference\"><a href=\"#cite_note-ZamanICRA-14\">&#91;14&#93;</a></sup><sup id=\"cite_ref-ZamanJRAS_15-0\" class=\"reference\"><a href=\"#cite_note-ZamanJRAS-15\">&#91;15&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Egomotion\">Egomotion</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=7\" title=\"Edit section: Egomotion\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p><b>Egomotion</b> is defined as the 3D motion of a camera within an environment.<sup id=\"cite_ref-irani_16-0\" class=\"reference\"><a href=\"#cite_note-irani-16\">&#91;16&#93;</a></sup> In the field of <a href=\"/wiki/Computer_vision\" title=\"Computer vision\">computer vision</a>, egomotion refers to estimating a camera's motion relative to a rigid scene.<sup id=\"cite_ref-17\" class=\"reference\"><a href=\"#cite_note-17\">&#91;17&#93;</a></sup> An example of egomotion estimation would be estimating a car's moving position relative to lines on the road or street signs being observed from the car itself. The estimation of egomotion is important in <a href=\"/wiki/Autonomous_robot#Autonomous_navigation\" title=\"Autonomous robot\">autonomous robot navigation</a> applications.<sup id=\"cite_ref-18\" class=\"reference\"><a href=\"#cite_note-18\">&#91;18&#93;</a></sup>\n</p>\n<h3><span class=\"mw-headline\" id=\"Overview_2\">Overview</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=8\" title=\"Edit section: Overview\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h3>\n<p>The goal of estimating the egomotion of a camera is to determine the 3D motion of that camera within the environment using a sequence of images taken by the camera.<sup id=\"cite_ref-19\" class=\"reference\"><a href=\"#cite_note-19\">&#91;19&#93;</a></sup> The process of estimating a camera's motion within an environment involves the use of visual odometry techniques on a sequence of images captured by the moving camera.<sup id=\"cite_ref-milella_20-0\" class=\"reference\"><a href=\"#cite_note-milella-20\">&#91;20&#93;</a></sup> This is typically done using <a href=\"/wiki/Feature_detection_(computer_vision)\" title=\"Feature detection (computer vision)\">feature detection</a> to construct an <a href=\"/wiki/Optical_flow\" title=\"Optical flow\">optical flow</a> from two image frames in a sequence<sup id=\"cite_ref-irani_16-1\" class=\"reference\"><a href=\"#cite_note-irani-16\">&#91;16&#93;</a></sup> generated from either single cameras or stereo cameras.<sup id=\"cite_ref-milella_20-1\" class=\"reference\"><a href=\"#cite_note-milella-20\">&#91;20&#93;</a></sup> Using stereo image pairs for each frame helps reduce error and provides additional depth and scale information.<sup id=\"cite_ref-olson_21-0\" class=\"reference\"><a href=\"#cite_note-olson-21\">&#91;21&#93;</a></sup><sup id=\"cite_ref-22\" class=\"reference\"><a href=\"#cite_note-22\">&#91;22&#93;</a></sup>\n</p><p>Features are detected in the first frame, and then matched in the second frame. This information is then used to make the optical flow field for the detected features in those two images. The optical flow field illustrates how features diverge from a single point, the <i>focus of expansion</i>. The focus of expansion can be detected from the optical flow field, indicating the direction of the motion of the camera, and thus providing an estimate of the camera motion.\n</p><p>There are other methods of extracting egomotion information from images as well, including a method that avoids feature detection and optical flow fields and directly uses the image intensities.<sup id=\"cite_ref-irani_16-2\" class=\"reference\"><a href=\"#cite_note-irani-16\">&#91;16&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"See_also\">See also</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=9\" title=\"Edit section: See also\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a href=\"/wiki/Dead_reckoning\" title=\"Dead reckoning\">Dead reckoning</a></li>\n<li><a href=\"/wiki/Odometry\" title=\"Odometry\">Odometry</a></li>\n<li><a href=\"/wiki/Optical_flow\" title=\"Optical flow\">Optical flow</a></li>\n<li><a href=\"/wiki/Optical_motion_capture\" class=\"mw-redirect\" title=\"Optical motion capture\">Optical motion capture</a></li></ul>\n<h2><span class=\"mw-headline\" id=\"References\">References</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=Visual_odometry&amp;action=edit&amp;section=10\" title=\"Edit section: References\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist columns references-column-width\" style=\"-moz-column-width: 30em; -webkit-column-width: 30em; column-width: 30em; list-style-type: decimal;\">\n<ol class=\"references\">\n<li id=\"cite_note-Maimone2007-1\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Maimone2007_1-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Maimone, M.; Cheng, Y.; Matthies, L. (2007). <a rel=\"nofollow\" class=\"external text\" href=\"http://www-robotics.jpl.nasa.gov/publications/Mark_Maimone/rob-06-0081.R4.pdf\">\"Two years of Visual Odometry on the Mars Exploration Rovers\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Journal of Field Robotics</i>. <b>24</b> (3): 169\u2013186. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1002/rob.20184\">10.1002/rob.20184</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2008-07-10</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Field+Robotics&amp;rft.atitle=Two+years+of+Visual+Odometry+on+the+Mars+Exploration+Rovers&amp;rft.volume=24&amp;rft.issue=3&amp;rft.pages=169-186&amp;rft.date=2007&amp;rft_id=info%3Adoi%2F10.1002%2Frob.20184&amp;rft.au=Maimone%2C+M.&amp;rft.au=Cheng%2C+Y.&amp;rft.au=Matthies%2C+L.&amp;rft_id=http%3A%2F%2Fwww-robotics.jpl.nasa.gov%2Fpublications%2FMark_Maimone%2Frob-06-0081.R4.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-ChhaniyaraS-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-ChhaniyaraS_2-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation conference\">Chhaniyara, Savan; KASPAR ALTHOEFER; LAKMAL D. SENEVIRATNE (2008). <a rel=\"nofollow\" class=\"external text\" href=\"http://eproceedings.worldscinet.com/9789812835772/9789812835772_0128.html\">\"Visual Odometry Technique Using Circular Marker Identification For Motion Parameter Estimation\"</a>. <i>Advances in Mobile Robotics: Proceedings of the Eleventh International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines, Coimbra, Portugal</i>. <a rel=\"nofollow\" class=\"external text\" href=\"https://books.google.com/books?id=8L7izBmmCuQC&amp;pg=PA1069&amp;dq=savan+chhaniyara&amp;cd=1#v=onepage&amp;q=savan%20chhaniyara&amp;f=false\">The Eleventh International Conference on Climbing and Walking Robots and the Support Technologies for Mobile Machines</a>. <b>11</b>. World Scientific, 2008.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Visual+Odometry+Technique+Using+Circular+Marker+Identification+For+Motion+Parameter+Estimation&amp;rft.btitle=Advances+in+Mobile+Robotics%3A+Proceedings+of+the+Eleventh+International+Conference+on+Climbing+and+Walking+Robots+and+the+Support+Technologies+for+Mobile+Machines%2C+Coimbra%2C+Portugal&amp;rft.pub=World+Scientific%2C+2008&amp;rft.date=2008&amp;rft.au=Chhaniyara%2C+Savan&amp;rft.au=KASPAR+ALTHOEFER&amp;rft.au=LAKMAL+D.+SENEVIRATNE&amp;rft_id=http%3A%2F%2Feproceedings.worldscinet.com%2F9789812835772%2F9789812835772_0128.html&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Nister:2004p211-3\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Nister:2004p211_3-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Nister:2004p211_3-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation conference\">Nister, D; Naroditsky, O.; Bergen, J (Jan 2004). <a rel=\"nofollow\" class=\"external text\" href=\"http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=1315094&amp;isnumber=29133&amp;punumber=9183&amp;k2dockey=1315094@ieeecnfs\"><i>Visual Odometry</i></a>. Computer Vision and Pattern Recognition, 2004. CVPR 2004. <b>1</b>. pp.&#160;I\u2013652 \u2013 I\u2013\u2013659 Vol.1. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1109/CVPR.2004.1315094\">10.1109/CVPR.2004.1315094</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.btitle=Visual+Odometry&amp;rft.pages=I-652+-+I--659+Vol.1&amp;rft.date=2004-01&amp;rft_id=info%3Adoi%2F10.1109%2FCVPR.2004.1315094&amp;rft.au=Nister%2C+D&amp;rft.au=Naroditsky%2C+O.&amp;rft.au=Bergen%2C+J&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fsearch%2Fsrchabstract.jsp%3Farnumber%3D1315094%26isnumber%3D29133%26punumber%3D9183%26k2dockey%3D1315094%40ieeecnfs&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Comport10-4\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-Comport10_4-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-Comport10_4-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Comport, A.I.; Malis, E.; Rives, P. (2010).  F. Chaumette; P. Corke; P. Newman, eds. <a rel=\"nofollow\" class=\"external text\" href=\"http://ijr.sagepub.com/content/29/2-3/245.abstract\">\"Real-time Quadrifocal Visual Odometry\"</a>. <i>International Journal of Robotics Research, Special issue on Robot Vision</i>. <b>29</b> (2-3): 245\u2013266. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1177/0278364909356601\">10.1177/0278364909356601</a>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=International+Journal+of+Robotics+Research%2C+Special+issue+on+Robot+Vision&amp;rft.atitle=Real-time+Quadrifocal+Visual+Odometry&amp;rft.volume=29&amp;rft.issue=2-3&amp;rft.pages=245-266&amp;rft.date=2010&amp;rft_id=info%3Adoi%2F10.1177%2F0278364909356601&amp;rft.au=Comport%2C+A.I.&amp;rft.au=Malis%2C+E.&amp;rft.au=Rives%2C+P.&amp;rft_id=http%3A%2F%2Fijr.sagepub.com%2Fcontent%2F29%2F2-3%2F245.abstract&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-ScaramuzzaIEEE-TRO08-5\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-ScaramuzzaIEEE-TRO08_5-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Scaramuzza, D.; Siegwart, R. (October 2008). <a rel=\"nofollow\" class=\"external text\" href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625958&amp;isnumber=4359257\">\"Appearance-Guided Monocular Omnidirectional Visual Odometry for Outdoor Ground Vehicles\"</a>. <i>IEEE Transactions on Robotics</i>: 1\u201312<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2008-10-20</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Robotics&amp;rft.atitle=Appearance-Guided+Monocular+Omnidirectional+Visual+Odometry+for+Outdoor+Ground+Vehicles&amp;rft.pages=1-12&amp;rft.date=2008-10&amp;rft.au=Scaramuzza%2C+D.&amp;rft.au=Siegwart%2C+R.&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Farnumber%3D4625958%26isnumber%3D4359257&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Corke-6\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Corke_6-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation conference\">Corke, P.; Strelow, D.; Singh, S. <a rel=\"nofollow\" class=\"external text\" href=\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1390041\">\"Omnidirectional visual odometry for a planetary rover\"</a>. <i>Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on</i>. <b>4</b>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Omnidirectional+visual+odometry+for+a+planetary+rover&amp;rft.btitle=Intelligent+Robots+and+Systems%2C+2004.%28IROS+2004%29.+Proceedings.+2004+IEEE%2FRSJ+International+Conference+on&amp;rft.au=Corke%2C+P.&amp;rft.au=Strelow%2C+D.&amp;rft.au=Singh%2C+S.&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1390041&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Campbell-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Campbell_7-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation conference\">Campbell, J.; Sukthankar, R.; Nourbakhsh, I.; Pittsburgh, I.R. <a rel=\"nofollow\" class=\"external text\" href=\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1389991\">\"Techniques for evaluating optical flow for visual odometry in extreme terrain\"</a>. <i>Intelligent Robots and Systems, 2004.(IROS 2004). Proceedings. 2004 IEEE/RSJ International Conference on</i>. <b>4</b>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=Techniques+for+evaluating+optical+flow+for+visual+odometry+in+extreme+terrain&amp;rft.btitle=Intelligent+Robots+and+Systems%2C+2004.%28IROS+2004%29.+Proceedings.+2004+IEEE%2FRSJ+International+Conference+on&amp;rft.au=Campbell%2C+J.&amp;rft.au=Sukthankar%2C+R.&amp;rft.au=Nourbakhsh%2C+I.&amp;rft.au=Pittsburgh%2C+I.R.&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fxpls%2Fabs_all.jsp%3Farnumber%3D1389991&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Sunderhauf2005-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Sunderhauf2005_8-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Sunderhauf, N.; Konolige, K.; Lacroix, S.; Protzel, P. (2005). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.tu-chemnitz.de/etit/proaut/index.download.df493a7bc2c27263f7d8ff467ea84879.pdf\">\"Visual odometry using sparse bundle adjustment on an autonomous outdoor vehicle\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Levi, Schanz, Lafrenz, and Avrutin, editors, Tagungsband Autonome Mobile Systeme</i>: 157\u2013163<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2008-07-10</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Levi%2C+Schanz%2C+Lafrenz%2C+and+Avrutin%2C+editors%2C+Tagungsband+Autonome+Mobile+Systeme&amp;rft.atitle=Visual+odometry+using+sparse+bundle+adjustment+on+an+autonomous+outdoor+vehicle&amp;rft.pages=157-163&amp;rft.date=2005&amp;rft.au=Sunderhauf%2C+N.&amp;rft.au=Konolige%2C+K.&amp;rft.au=Lacroix%2C+S.&amp;rft.au=Protzel%2C+P.&amp;rft_id=http%3A%2F%2Fwww.tu-chemnitz.de%2Fetit%2Fproaut%2Findex.download.df493a7bc2c27263f7d8ff467ea84879.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Konolige2006-9\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Konolige2006_9-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Konolige, K.; Agrawal, M.; Bolles, R.C.; Cowan, C.; Fischler, M.; Gerkey, B.P. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.springerlink.com/index/g442h0p7n313w1g2.pdf\">\"Outdoor mapping and navigation using stereo vision\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Proc. of the Intl. Symp. on Experimental Robotics (ISER)</i><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2008-07-10</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Proc.+of+the+Intl.+Symp.+on+Experimental+Robotics+%28ISER%29&amp;rft.atitle=Outdoor+mapping+and+navigation+using+stereo+vision&amp;rft.date=2006&amp;rft.au=Konolige%2C+K.&amp;rft.au=Agrawal%2C+M.&amp;rft.au=Bolles%2C+R.C.&amp;rft.au=Cowan%2C+C.&amp;rft.au=Fischler%2C+M.&amp;rft.au=Gerkey%2C+B.P.&amp;rft_id=http%3A%2F%2Fwww.springerlink.com%2Findex%2Fg442h0p7n313w1g2.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Olson2002-10\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Olson2002_10-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Olson, C.F.; Matthies, L.; Schoppers, M.; Maimone, M.W. (2002). <a rel=\"nofollow\" class=\"external text\" href=\"http://faculty.washington.edu/cfolson/papers/pdf/ras03.pdf\">\"Rover navigation using stereo ego-motion\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Robotics and Autonomous Systems</i>. <b>43</b>: 215\u2013229. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/s0921-8890%2803%2900004-6\">10.1016/s0921-8890(03)00004-6</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2010-06-06</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Robotics+and+Autonomous+Systems&amp;rft.atitle=Rover+navigation+using+stereo+ego-motion&amp;rft.volume=43&amp;rft.pages=215-229&amp;rft.date=2002&amp;rft_id=info%3Adoi%2F10.1016%2Fs0921-8890%2803%2900004-6&amp;rft.au=Olson%2C+C.F.&amp;rft.au=Matthies%2C+L.&amp;rft.au=Schoppers%2C+M.&amp;rft.au=Maimone%2C+M.W.&amp;rft_id=http%3A%2F%2Ffaculty.washington.edu%2Fcfolson%2Fpapers%2Fpdf%2Fras03.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-Cheng2006-11\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Cheng2006_11-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Cheng, Y.; Maimone, M.W.; Matthies, L. (2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://ieeexplore.ieee.org/iel5/100/31467/101109RA2006CHENG.pdf?arnumber=101109RA2006CHENG\">\"Visual Odometry on the Mars Exploration Rovers\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>IEEE Robotics and Automation Magazine</i>. <b>13</b> (2): 54\u201362. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1109/MRA.2006.1638016\">10.1109/MRA.2006.1638016</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">2008-07-10</span></span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Robotics+and+Automation+Magazine&amp;rft.atitle=Visual+Odometry+on+the+Mars+Exploration+Rovers&amp;rft.volume=13&amp;rft.issue=2&amp;rft.pages=54-62&amp;rft.date=2006&amp;rft_id=info%3Adoi%2F10.1109%2FMRA.2006.1638016&amp;rft.au=Cheng%2C+Y.&amp;rft.au=Maimone%2C+M.W.&amp;rft.au=Matthies%2C+L.&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F100%2F31467%2F101109RA2006CHENG.pdf%3Farnumber%3D101109RA2006CHENG&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-12\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-12\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Engel, Jakob; Sch\u00f6ps, Thomas; Cremers, Daniel (2014). <a rel=\"nofollow\" class=\"external text\" href=\"https://vision.in.tum.de/_media/spezial/bib/engel14eccv.pdf\">\"European Conference on Computer Vision (ECCV) 2014\"</a> <span style=\"font-size:85%;\">(PDF)</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=European+Conference+on+Computer+Vision+%28ECCV%29+2014&amp;rft.date=2014&amp;rft.aulast=Engel&amp;rft.aufirst=Jakob&amp;rft.au=Sch%C3%B6ps%2C+Thomas&amp;rft.au=Cremers%2C+Daniel&amp;rft_id=https%3A%2F%2Fvision.in.tum.de%2F_media%2Fspezial%2Fbib%2Fengel14eccv.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> <span style=\"font-size:100%\" class=\"error citation-comment\"><code style=\"color:inherit; border:inherit; padding:inherit;\">&#124;contribution=</code> ignored (<a href=\"/wiki/Help:CS1_errors#chapter_ignored\" title=\"Help:CS1 errors\">help</a>)</span></span>\n</li>\n<li id=\"cite_note-13\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-13\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Engel, Jakob; Sturm, J\u00fcrgen; Cremers, Daniel (2013). <a rel=\"nofollow\" class=\"external text\" href=\"https://vision.in.tum.de/_media/spezial/bib/engel2013iccv.pdf\">\"IEEE International Conference on Computer Vision (ICCV)\"</a> <span style=\"font-size:85%;\">(PDF)</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.atitle=IEEE+International+Conference+on+Computer+Vision+%28ICCV%29&amp;rft.date=2013&amp;rft.aulast=Engel&amp;rft.aufirst=Jakob&amp;rft.au=Sturm%2C+J%C3%BCrgen&amp;rft.au=Cremers%2C+Daniel&amp;rft_id=https%3A%2F%2Fvision.in.tum.de%2F_media%2Fspezial%2Fbib%2Fengel2013iccv.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span> <span style=\"font-size:100%\" class=\"error citation-comment\"><code style=\"color:inherit; border:inherit; padding:inherit;\">&#124;contribution=</code> ignored (<a href=\"/wiki/Help:CS1_errors#chapter_ignored\" title=\"Help:CS1 errors\">help</a>)</span></span>\n</li>\n<li id=\"cite_note-ZamanICRA-14\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-ZamanICRA_14-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation conference\">Zaman, M. (2007). <a rel=\"nofollow\" class=\"external text\" href=\"http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&amp;arnumber=4209696&amp;userType=inst\">\"High Precision Relative Localization Using a Single Camera\"</a>. <i>Robotics and Automation, 2007.(ICRA 2007). Proceedings. 2007 IEEE International Conference on</i>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Abook&amp;rft.genre=conference&amp;rft.atitle=High+Precision+Relative+Localization+Using+a+Single+Camera&amp;rft.btitle=Robotics+and+Automation%2C+2007.%28ICRA+2007%29.+Proceedings.+2007+IEEE+International+Conference+on&amp;rft.date=2007&amp;rft.au=Zaman%2C+M.&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fstamp%2Fstamp.jsp%3Ftp%3D%26arnumber%3D4209696%26userType%3Dinst&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-ZamanJRAS-15\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-ZamanJRAS_15-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Zaman, M. (2007). \"High resolution relative localisation using two cameras\". <i>Journal of Robotics and Autonomous Systems (JRAS)</i>. Elsevier: 685\u2013692.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Journal+of+Robotics+and+Autonomous+Systems+%28JRAS%29&amp;rft.atitle=High+resolution+relative+localisation+using+two+cameras&amp;rft.pages=685-692&amp;rft.date=2007&amp;rft.au=Zaman%2C+M.&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-irani-16\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-irani_16-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-irani_16-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-irani_16-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Irani, M.; Rousso, B.; Peleg S. (June 1994). <a rel=\"nofollow\" class=\"external text\" href=\"http://www.vision.huji.ac.il/papers/ego-mtn-cvpr94.pdf\">\"Recovery of Ego-Motion Using Image Stabilization\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i>: 21\u201323<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">7 June</span> 2010</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Computer+Society+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;rft.atitle=Recovery+of+Ego-Motion+Using+Image+Stabilization&amp;rft.pages=21-23&amp;rft.date=1994-06&amp;rft.au=Irani%2C+M.&amp;rft.au=Rousso%2C+B.&amp;rft.au=Peleg+S.&amp;rft_id=http%3A%2F%2Fwww.vision.huji.ac.il%2Fpapers%2Fego-mtn-cvpr94.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-17\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-17\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Burger, W.; Bhanu, B. (Nov 1990). <a rel=\"nofollow\" class=\"external text\" href=\"http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=61704\">\"Estimating 3D egomotion from perspective image sequence\"</a>. <i>IEEE Transactions on Pattern Analysis and Machine Intelligence</i>. <b>12</b> (11): 1040\u20131058. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1109/34.61704\">10.1109/34.61704</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">7 June</span> 2010</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Transactions+on+Pattern+Analysis+and+Machine+Intelligence&amp;rft.atitle=Estimating+3D+egomotion+from+perspective+image+sequence&amp;rft.volume=12&amp;rft.issue=11&amp;rft.pages=1040-1058&amp;rft.date=1990-11&amp;rft_id=info%3Adoi%2F10.1109%2F34.61704&amp;rft.au=Burger%2C+W.&amp;rft.au=Bhanu%2C+B.&amp;rft_id=http%3A%2F%2Fieeexplore.ieee.org%2Fxpl%2Ffreeabs_all.jsp%3Farnumber%3D61704&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-18\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-18\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Shakernia, O.; Vidal, R.; Shankar, S. (2003). <a rel=\"nofollow\" class=\"external text\" href=\"http://cis.jhu.edu/~rvidal/publications/OMNIVIS03-backflow.pdf\">\"Omnidirectional Egomotion Estimation From Back-projection Flow\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Conference on Computer Vision and Pattern Recognition Workshop</i>. <b>7</b>: 82<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">7 June</span> 2010</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Conference+on+Computer+Vision+and+Pattern+Recognition+Workshop&amp;rft.atitle=Omnidirectional+Egomotion+Estimation+From+Back-projection+Flow&amp;rft.volume=7&amp;rft.pages=82&amp;rft.date=2003&amp;rft.au=Shakernia%2C+O.&amp;rft.au=Vidal%2C+R.&amp;rft.au=Shankar%2C+S.&amp;rft_id=http%3A%2F%2Fcis.jhu.edu%2F~rvidal%2Fpublications%2FOMNIVIS03-backflow.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-19\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-19\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Tian, T.; Tomasi, C.; Heeger, D. (1996). <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20080808123021/http://www.cs.duke.edu/%7Etomasi/papers/tian/tianCvpr96.pdf\">\"Comparison of Approaches to Egomotion Computation\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>IEEE Computer Society Conference on Computer Vision and Pattern Recognition</i>: 315. Archived from <a rel=\"nofollow\" class=\"external text\" href=\"http://www.cs.duke.edu/~tomasi/papers/tian/tianCvpr96.pdf\">the original</a> <span style=\"font-size:85%;\">(PDF)</span> on August 8, 2008<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">7 June</span> 2010</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+Computer+Society+Conference+on+Computer+Vision+and+Pattern+Recognition&amp;rft.atitle=Comparison+of+Approaches+to+Egomotion+Computation&amp;rft.pages=315&amp;rft.date=1996&amp;rft.au=Tian%2C+T.&amp;rft.au=Tomasi%2C+C.&amp;rft.au=Heeger%2C+D.&amp;rft_id=http%3A%2F%2Fwww.cs.duke.edu%2F~tomasi%2Fpapers%2Ftian%2FtianCvpr96.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-milella-20\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-milella_20-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-milella_20-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\"><cite class=\"citation journal\">Milella, A.; Siegwart, R. (January 2006). <a rel=\"nofollow\" class=\"external text\" href=\"http://asl.epfl.ch/aslInternalWeb/ASL/publications/uploadedFiles/21_amilella_EgoMotion_rev_publication.pdf\">\"Stereo-Based Ego-Motion Estimation Using Pixel Tracking and Iterative Closest Point\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>IEEE International Conference on Computer Vision Systems</i>: 21<span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">7 June</span> 2010</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=IEEE+International+Conference+on+Computer+Vision+Systems&amp;rft.atitle=Stereo-Based+Ego-Motion+Estimation+Using+Pixel+Tracking+and+Iterative+Closest+Point&amp;rft.pages=21&amp;rft.date=2006-01&amp;rft.au=Milella%2C+A.&amp;rft.au=Siegwart%2C+R.&amp;rft_id=http%3A%2F%2Fasl.epfl.ch%2FaslInternalWeb%2FASL%2Fpublications%2FuploadedFiles%2F21_amilella_EgoMotion_rev_publication.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-olson-21\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-olson_21-0\">^</a></b></span> <span class=\"reference-text\"><cite class=\"citation journal\">Olson, C. F.; Matthies, L.; Schoppers, M.; Maimoneb M. W. (June 2003). <a rel=\"nofollow\" class=\"external text\" href=\"http://faculty.washington.edu/cfolson/papers/pdf/ras03.pdf\">\"Rover navigation using stereo ego-motion\"</a> <span style=\"font-size:85%;\">(PDF)</span>. <i>Robotics and Autonomous Systems</i>. <b>43</b> (9): 215\u2013229. <a href=\"/wiki/Digital_object_identifier\" title=\"Digital object identifier\">doi</a>:<a rel=\"nofollow\" class=\"external text\" href=\"//doi.org/10.1016/s0921-8890%2803%2900004-6\">10.1016/s0921-8890(03)00004-6</a><span class=\"reference-accessdate\">. Retrieved <span class=\"nowrap\">7 June</span> 2010</span>.</cite><span title=\"ctx_ver=Z39.88-2004&amp;rft_val_fmt=info%3Aofi%2Ffmt%3Akev%3Amtx%3Ajournal&amp;rft.genre=article&amp;rft.jtitle=Robotics+and+Autonomous+Systems&amp;rft.atitle=Rover+navigation+using+stereo+ego-motion&amp;rft.volume=43&amp;rft.issue=9&amp;rft.pages=215-229&amp;rft.date=2003-06&amp;rft_id=info%3Adoi%2F10.1016%2Fs0921-8890%2803%2900004-6&amp;rft.au=Olson%2C+C.+F.&amp;rft.au=Matthies%2C+L.&amp;rft.au=Schoppers%2C+M.&amp;rft.au=Maimoneb+M.+W.&amp;rft_id=http%3A%2F%2Ffaculty.washington.edu%2Fcfolson%2Fpapers%2Fpdf%2Fras03.pdf&amp;rfr_id=info%3Asid%2Fen.wikipedia.org%3AVisual+odometry\" class=\"Z3988\"><span style=\"display:none;\">&#160;</span></span></span>\n</li>\n<li id=\"cite_note-22\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-22\">^</a></b></span> <span class=\"reference-text\">Sudin Dinesh, Koteswara Rao, K.&#160;; Unnikrishnan, M.&#160;; Brinda, V.&#160;; Lalithambika, V.R.&#160;; Dhekane, M.V. \"<a rel=\"nofollow\" class=\"external text\" href=\"http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6749359\">Improvements in Visual Odometry Algorithm for Planetary Exploration Rovers</a>\". IEEE International Conference on Emerging Trends in Communication, Control, Signal Processing &amp; Computing Applications (C2SPCA), 2013</span>\n</li>\n</ol></div>\n\n<!-- \nNewPP limit report\nParsed by mw1331\nCached time: 20180911000752\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.208 seconds\nReal time usage: 0.234 seconds\nPreprocessor visited node count: 1051/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 43595/2097152 bytes\nTemplate argument size: 111/2097152 bytes\nHighest expansion depth: 7/40\nExpensive parser function count: 0/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 30711/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.106/10.000 seconds\nLua memory usage: 3.15 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  175.438      1 Template:Reflist\n100.00%  175.438      1 -total\n 66.05%  115.880     16 Template:Cite_journal\n 12.29%   21.565      5 Template:Cite_conference\n  1.68%    2.950      1 Template:Column-width\n  1.47%    2.583      1 Template:Main_other\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:18315951-0!canonical and timestamp 20180911000752 and revision id 855663633\n -->\n</div>"},"langlinks":[{"lang":"ar","url":"https://ar.wikipedia.org/wiki/%D8%A7%D9%88%D8%AF%D9%88%D9%85%D8%AA%D8%B1%D9%8A_%D8%A8%D8%B5%D8%B1%D9%8A","langname":"Arabic","autonym":"\u0627\u0644\u0639\u0631\u0628\u064a\u0629","*":"\u0627\u0648\u062f\u0648\u0645\u062a\u0631\u064a \u0628\u0635\u0631\u064a"},{"lang":"ca","url":"https://ca.wikipedia.org/wiki/Odometria_visual","langname":"Catalan","autonym":"catal\u00e0","*":"Odometria visual"},{"lang":"ru","url":"https://ru.wikipedia.org/wiki/%D0%92%D0%B8%D0%B7%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0%D1%8F_%D0%BE%D0%B4%D0%BE%D0%BC%D0%B5%D1%82%D1%80%D0%B8%D1%8F","langname":"Russian","autonym":"\u0440\u0443\u0441\u0441\u043a\u0438\u0439","*":"\u0412\u0438\u0437\u0443\u0430\u043b\u044c\u043d\u0430\u044f \u043e\u0434\u043e\u043c\u0435\u0442\u0440\u0438\u044f"},{"lang":"uk","url":"https://uk.wikipedia.org/wiki/%D0%92%D1%96%D0%B7%D1%83%D0%B0%D0%BB%D1%8C%D0%BD%D0%B0_%D0%BE%D0%B4%D0%BE%D0%BC%D0%B5%D1%82%D1%80%D1%96%D1%8F","langname":"Ukrainian","autonym":"\u0443\u043a\u0440\u0430\u0457\u043d\u0441\u044c\u043a\u0430","*":"\u0412\u0456\u0437\u0443\u0430\u043b\u044c\u043d\u0430 \u043e\u0434\u043e\u043c\u0435\u0442\u0440\u0456\u044f"}],"categories":[{"sortkey":"","hidden":"","*":"CS1_errors:_chapter_ignored"},{"sortkey":"","*":"Robotic_sensing"},{"sortkey":"","*":"Motion_in_computer_vision"},{"sortkey":"","*":"Surveying"}],"links":[{"ns":0,"exists":"","*":"Autonomous robot"},{"ns":0,"exists":"","*":"Computer vision"},{"ns":0,"exists":"","*":"Dead reckoning"},{"ns":0,"exists":"","*":"Digital object identifier"},{"ns":0,"exists":"","*":"Feature detection (computer vision)"},{"ns":0,"exists":"","*":"Feature extraction"},{"ns":0,"exists":"","*":"Image processing"},{"ns":0,"exists":"","*":"Inertial measurement unit"},{"ns":0,"exists":"","*":"Kalman filter"},{"ns":0,"exists":"","*":"Loss function"},{"ns":0,"exists":"","*":"Lucas\u2013Kanade method"},{"ns":0,"exists":"","*":"Mars Exploration Rover"},{"ns":0,"exists":"","*":"Mobile robot"},{"ns":0,"exists":"","*":"Navigation"},{"ns":0,"exists":"","*":"Odometry"},{"ns":0,"exists":"","*":"Omnidirectional camera"},{"ns":0,"exists":"","*":"Optical flow"},{"ns":0,"exists":"","*":"Optical motion capture"},{"ns":0,"exists":"","*":"Phase correlation"},{"ns":0,"exists":"","*":"Random sampling"},{"ns":0,"exists":"","*":"Robotics"},{"ns":0,"exists":"","*":"Rotary encoder"},{"ns":0,"exists":"","*":"Single camera"},{"ns":0,"exists":"","*":"Stereo camera"},{"ns":12,"exists":"","*":"Help:CS1 errors"}],"templates":[{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Column-width"},{"ns":10,"exists":"","*":"Template:Cite journal"},{"ns":10,"exists":"","*":"Template:Cite conference"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":828,"exists":"","*":"Module:Citation/CS1"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Configuration"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Whitelist"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Utilities"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Date validation"},{"ns":828,"exists":"","*":"Module:Citation/CS1/Identifiers"},{"ns":828,"exists":"","*":"Module:Citation/CS1/COinS"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"}],"images":["Optical_flow_example_v2.png","VIO_sensor_in_various_commercial_quadcopters_.jpg"],"externallinks":["http://www-robotics.jpl.nasa.gov/publications/Mark_Maimone/rob-06-0081.R4.pdf","//doi.org/10.1002/rob.20184","http://eproceedings.worldscinet.com/9789812835772/9789812835772_0128.html","https://books.google.com/books?id=8L7izBmmCuQC&pg=PA1069&dq=savan+chhaniyara&cd=1#v=onepage&q=savan%20chhaniyara&f=false","http://ieeexplore.ieee.org/search/srchabstract.jsp?arnumber=1315094&isnumber=29133&punumber=9183&k2dockey=1315094@ieeecnfs","//doi.org/10.1109/CVPR.2004.1315094","http://ijr.sagepub.com/content/29/2-3/245.abstract","//doi.org/10.1177/0278364909356601","http://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4625958&isnumber=4359257","http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1390041","http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=1389991","http://www.tu-chemnitz.de/etit/proaut/index.download.df493a7bc2c27263f7d8ff467ea84879.pdf","http://www.springerlink.com/index/g442h0p7n313w1g2.pdf","http://faculty.washington.edu/cfolson/papers/pdf/ras03.pdf","//doi.org/10.1016/s0921-8890(03)00004-6","http://ieeexplore.ieee.org/iel5/100/31467/101109RA2006CHENG.pdf?arnumber=101109RA2006CHENG","//doi.org/10.1109/MRA.2006.1638016","https://vision.in.tum.de/_media/spezial/bib/engel14eccv.pdf","https://vision.in.tum.de/_media/spezial/bib/engel2013iccv.pdf","http://ieeexplore.ieee.org/stamp/stamp.jsp?tp=&arnumber=4209696&userType=inst","http://www.vision.huji.ac.il/papers/ego-mtn-cvpr94.pdf","http://ieeexplore.ieee.org/xpl/freeabs_all.jsp?arnumber=61704","//doi.org/10.1109/34.61704","http://cis.jhu.edu/~rvidal/publications/OMNIVIS03-backflow.pdf","https://web.archive.org/web/20080808123021/http://www.cs.duke.edu/~tomasi/papers/tian/tianCvpr96.pdf","http://www.cs.duke.edu/~tomasi/papers/tian/tianCvpr96.pdf","http://asl.epfl.ch/aslInternalWeb/ASL/publications/uploadedFiles/21_amilella_EgoMotion_rev_publication.pdf","http://ieeexplore.ieee.org/xpls/abs_all.jsp?arnumber=6749359"],"sections":[{"toclevel":1,"level":"2","line":"Overview","number":"1","index":"1","fromtitle":"Visual_odometry","byteoffset":824,"anchor":"Overview"},{"toclevel":1,"level":"2","line":"Types","number":"2","index":"2","fromtitle":"Visual_odometry","byteoffset":1853,"anchor":"Types"},{"toclevel":2,"level":"3","line":"Monocular and Stereo","number":"2.1","index":"3","fromtitle":"Visual_odometry","byteoffset":1895,"anchor":"Monocular_and_Stereo"},{"toclevel":2,"level":"3","line":"Feature Based and Direct Method","number":"2.2","index":"4","fromtitle":"Visual_odometry","byteoffset":2210,"anchor":"Feature_Based_and_Direct_Method"},{"toclevel":2,"level":"3","line":"Visual Inertial Odometry","number":"2.3","index":"5","fromtitle":"Visual_odometry","byteoffset":2585,"anchor":"Visual_Inertial_Odometry"},{"toclevel":1,"level":"2","line":"Algorithm","number":"3","index":"6","fromtitle":"Visual_odometry","byteoffset":2751,"anchor":"Algorithm"},{"toclevel":1,"level":"2","line":"Egomotion","number":"4","index":"7","fromtitle":"Visual_odometry","byteoffset":10621,"anchor":"Egomotion"},{"toclevel":2,"level":"3","line":"Overview","number":"4.1","index":"8","fromtitle":"Visual_odometry","byteoffset":12259,"anchor":"Overview_2"},{"toclevel":1,"level":"2","line":"See also","number":"5","index":"9","fromtitle":"Visual_odometry","byteoffset":15382,"anchor":"See_also"},{"toclevel":1,"level":"2","line":"References","number":"6","index":"10","fromtitle":"Visual_odometry","byteoffset":15480,"anchor":"References"}],"parsewarnings":[],"displaytitle":"Visual odometry","iwlinks":[],"properties":[{"name":"wikibase_item","*":"Q4110915"}]}}