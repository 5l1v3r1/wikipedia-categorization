Web archiving is the process of collecting portions of the World Wide Web to ensure the information is preserved in an archive for future researchers, historians, and the public.  Web archivists typically employ web crawlers for automated capture due to the massive size and amount of information on the Web.  The largest web archiving organization based on a bulk crawling approach is the Internet Archive which strives to maintain an archive of the entire Web.  The International Web Archiving Workshop (IWAW), begun in 2001, has provided a platform to share experiences and exchange ideas. The later founding of the International Internet Preservation Consortium (IIPC), in 2003, has greatly facilitated international collaboration in developing standards and open source tools for the creation of web archives. These developments, and the growing portion of human culture created and recorded on the web, combine to make it inevitable that more and more libraries and archives will have to face the challenges of web archiving. National libraries, national archives and various consortia of organizations are also involved in archiving culturally important Web content.  Commercial web archiving software and services are also available to organizations who need to archive their own web content for corporate heritage, regulatory, or legal purposes. Early practice of web archiving involved the highlighting of a “site of the week award” as a record for the contest. Besides that, another early practice was the professional link list (for example Amnesty International’s list of human rights groups) and (Yahoo!) directory and Open Directory Project. In the mid-1990s, one of the more important listing sites of its kind continually updated an index of worthwhile website destinations organized by content category. In 1998, Yahoo directory was considered to have made a significant contribution to emerging online library science, not only by its classification scheme but also by the means of the content “navigation” it developed. Soliciting, evaluating, and categorizing websites – the large-scale collecting, hand-sorting, and display of websites – could be considered an original form of website analysis. The rise of the algorithmic search engine has largely led to the disappearance of such manual methods. Web archivists generally archive various types of web content including HTML web pages, style sheets, JavaScript, images, and video. They also archive metadata about the collected resources such as access time, MIME type, and content length.  This metadata is useful in establishing authenticity and provenance of the archived collection. The most common web archiving technique uses web crawlers to automate the process of collecting web pages.  Web crawlers typically access web pages in the same manner that users with a browser see the Web, and therefore provide a comparatively simple method of remote harvesting web content. Examples of web crawlers used for web archiving include: There exist various free services which may be used to archive web resources "on-demand", using web crawling techniques. These services include the Wayback Machine and WebCite. Database archiving refers to methods for archiving the underlying content of database-driven websites. It typically requires the extraction of the database content into a standard schema, often using XML. Once stored in that standard format, the archived content of multiple databases can then be made available using a single access system. This approach is exemplified by the DeepArc and Xinq tools developed by the Bibliothèque nationale de France and the National Library of Australia respectively. DeepArc enables the structure of a relational database to be mapped to an XML schema, and the content exported into an XML document. Xinq then allows that content to be delivered online. Although the original layout and behavior of the website cannot be preserved exactly, Xinq does allow the basic querying and retrieval functionality to be replicated. Transactional archiving is an event-driven approach, which collects the actual transactions which take place between a web server and a web browser. It is primarily used as a means of preserving evidence of the content which was actually viewed on a particular website, on a given date. This may be particularly important for organizations which need to comply with legal or regulatory requirements for disclosing and retaining information. A transactional archiving system typically operates by intercepting every HTTP request to, and response from, the web server, filtering each response to eliminate duplicate content, and permanently storing the responses as bitstreams. Web archives which rely on web crawling as their primary means of collecting the Web are influenced by the difficulties of web crawling: However, it is important to note that a native format web archive, i.e., a fully browsable web archive, with working links, media, etc., is only really possible using crawler technology. The Web is so large that crawling a significant portion of it takes a large amount of technical resources.  The Web is changing so fast that portions of a website may change before a crawler has even finished crawling it. Some web servers are configured to return different pages to web archiver requests than they would in response to regular browser requests.   This is typically done to fool search engines into directing more user traffic to a website, and is often done to avoid accountability, or to provide enhanced content only to those browsers that can display it. Not only must web archivists deal with the technical challenges of web archiving, they must also contend with intellectual property laws.  Peter Lyman  states that "although the Web is popularly regarded as a public domain resource, it is copyrighted; thus, archivists have no legal right to copy the Web". However national libraries in some countries   have a legal right to copy portions of the web under an extension of a legal deposit. Some private non-profit web archives that are made publicly accessible like WebCite, the Internet Archive or the Internet Memory Foundation allow content owners to hide or remove archived content that they do not want the public to have access to. Other web archives are only accessible from certain locations or have regulated usage. WebCite cites  a recent lawsuit against Google's caching, which Google won.  Web curation, like any digital curation, entails: Thus, besides the discussion on methods of collecting the Web, those of providing access, certification, and organizing must be included. There are a set of popular tools that addresses these curation steps: A suite of tools for Web Curation by International Internet Preservation Consortium: Other open source tools for manipulating web archives: Free but not open source tools also exist: 