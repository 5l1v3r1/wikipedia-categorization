Stylometry is the application of the study of linguistic style, usually to written language, but it has successfully been applied to music  and to fine-art paintings  as well.  Stylometry is often used to attribute authorship to anonymous or disputed documents. It has legal as well as academic and literary applications, ranging from the question of the authorship of Shakespeare's works to forensic linguistics. Stylometry grew out of earlier techniques of analyzing texts for evidence of authenticity, author identity, and other questions. The modern practice of the discipline received major impetus from the study of authorship problems in English Renaissance drama. Researchers and readers observed that some playwrights of the era had distinctive patterns of language preferences, and attempted to use those patterns to identify authors in uncertain or collaborative works. Early efforts were not always successful: in 1901, one researcher attempted to use John Fletcher's preference for "'em", the contractional form of "them", as a marker to distinguish between Fletcher and Philip Massinger in their collaborations—but he mistakenly employed an edition of Massinger's works in which the editor had expanded all instances of "'em" to "them".  The basics of stylometry were set out by Polish philosopher Wincenty Lutosławski in Principes de stylométrie (1890). Lutosławski used this method to build a chronology of Plato's Dialogues. The development of computers and their capacities for analyzing large quantities of data enhanced this type of effort by orders of magnitude. The great capacity of computers for data analysis, however, did not guarantee quality output. In the early 1960s, Rev. A. Q. Morton produced a computer analysis of the fourteen Epistles of the New Testament attributed to St. Paul, which showed that six different authors had written that body of work. A check of his method, applied to the works of James Joyce, gave the result that Ulysses, Joyce's multi-perspective, multi-style masterpiece, was written by five separate individuals, none of whom apparently had any part in the crafting of Joyce's first novel, A Portrait of the Artist as a Young Man.  In time, however, and with practice, researchers and scholars have refined their approaches and methods, to yield better results. One notable early success was the resolution of disputed authorship in twelve of The Federalist Papers by Frederick Mosteller and David Wallace.   While questions of initial assumptions and methodology still arise (and, perhaps, always will), few now dispute the basic premise that linguistic analysis of written texts can produce valuable information and insight. (Indeed, this was apparent even before the advent of computers: the successful application of a textual/linguistic approach to the Fletcher canon by Cyrus Hoy and others yielded clear results in the late 1950s and early '60s.) Applications of stylometry include literary studies, historical studies, social studies, gender studies, and many forensic cases and studies.   It can also be applied to computer code.  Modern stylometry draws heavily on the aid of computers for statistical analysis, artificial intelligence and access to the growing corpus of texts available via the Internet.  Software systems such as Signature  (freeware produced by Dr Peter Millican of Oxford University), JGAAP  (the Java Graphical Authorship Attribution Program—freeware produced by Dr Patrick Juola of Duquesne University), stylo   (an open-source R package for a variety of stylometric analyses, including authorship attribution) and Stylene  for Dutch (online freeware by Prof Walter Daelemans of University of Antwerp and Dr Véronique Hoste of University of Ghent) make its use increasingly practicable, even for the non-expert. Stylometry has been used in a number of high-profile cases. Matthew Jockers used stylometric techniques to analyse the Book of Mormon, concluding that Sidney Rigdon, and not Joseph Smith, was its author.  More recently, a critique by Schaalje et al. of the above paper by Jockers et al. was published in Literary and Linguistic Computing (now Digital Scholarship in the Humanities), showing the Jockers et al. application (using a closed set of candidate authors, rather than an open one) of nearest shrunken centroid (NSC) classification to be misguided. The closed-set method leads to nonsensical results when applied to other authorship attribution problems, as the Schaalje et al. paper shows. Schaalje and colleagues produced results from an open-set NSC method showing that Sidney Rigdon was a very unlikely author—a result that comports with documented history that Rigdon and Smith did not even know each other in 1829 or early 1830, when the Book of Mormon was dictated and published, making it manifestly impossible for Rigdon to have been the author. Schaalje and colleagues also demonstrated that both Joseph Smith and the men who acted as scribes were unlikely authors, while also revealing multiple distinct voices aligning with the main purported authors of the text.   Simon Fuller and James O'Sullivan published a study claiming that bestselling author James Patterson does not do any writing in his co-authored novels.    According to O'Sullivan, his collaboration with former U.S. president Bill Clinton, The President is Missing, is an exception to this rule.  Stylometric methods are discussed in several academic fields, mostly as a tangential field of application as with machine learning, natural language processing, and lexicography. The International Association of Forensic Linguists (IAFL) organises the Biennial Conference of the International Association of Forensic Linguists (13th edition in 2016 in Porto) and publishes The International Journal of Speech, Language and the Law with forensic stylistics as one of its central topics. The Association for the Advancement of Artificial Intelligence (AAAI) has hosted several events on subjective and stylistic analysis of text.    PAN workshops (originally, plagiarism analysis, authorship identification, and near-duplicate detection, later more generally workshop on uncovering plagiarism, authorship, and social software misuse)  organised since 2007 mainly in conjunction with information access conferences such as ACM SIGIR, FIRE, and CLEF. PAN formulates shared challenge tasks for plagiarism detection,  authorship identification,  author gender identification,  author profiling,  vandalism detection,  and other related text analysis tasks, many of which hinge on stylometry. Since stylometry has both descriptive use cases, used to characterise the content of a collection, and identificatory use cases, e.g. identifying authors or categories of texts, the methods used to analyse the data and features above range from those built to classify items into sets or to distribute items in a space of feature variation. Most methods are statistical in nature, such as cluster analysis and discriminant analysis, are typically based on philological data and features, and are fruitful application domains for modern machine learning approaches.  Whereas in the past, stylometry emphasized the rarest or most striking elements of a text, contemporary techniques can isolate identifying patterns even in common parts of speech. Most systems are based on lexical statistics, i.e. using the frequencies of words and terms in the text to characterise the text (or its author). In this context, unlike in information retrieval, the observed occurrence patterns of the most common words are more interesting than the topical terms which are less frequent.   The primary stylometric method is the writer invariant: a property held in common by all texts, or at least all texts long enough to admit of analysis yielding statistically significant results, written by a given author. An example of a writer invariant is frequency of function words used by the writer. In one such method, the text is analyzed to find the 50 most common words. The text is then broken into 5,000 word chunks and each of the chunks is analyzed to find the frequency of those 50 words in that chunk. This generates a unique 50-number identifier for each chunk. These numbers place each chunk of text into a point in a 50-dimensional space. This 50-dimensional space is flattened into a plane using principal components analysis (PCA). This results in a display of points that correspond to an author's style. If two literary works are placed on the same plane, the resulting pattern may show if both works were by the same author or different authors. Neural networks, a special case of statistical machine learning methods, have been used to analyze authorship of texts. Text of undisputed authorship are used to train the neural network through processes such as backpropagation, where training error is calculated and used to update the process to increase accuracy. Through a process akin to non-linear regression, the network gains the ability to generalize its recognition ability to new texts to which it has not yet been exposed, classifying them to a stated degree of confidence. Such techniques were applied to the long-standing claims of collaboration of Shakespeare with his contemporaries Fletcher and Christopher Marlowe,   and confirmed the view, based on more conventional scholarship, that such collaboration had indeed taken place.  A 1999 study showed that a neural network program reached 70% accuracy in determining authorship of poems it had not yet analyzed. This study from Vrije Universiteit examined identification of poems by three Dutch authors using only letter sequences such as "den".  A study used deep belief networks (DBN) for authorship verification model applicable for continuous authentication (CA).  One problem with this method of analysis is that the network can become biased based on its training set, possibly selecting authors the network has more often analyzed.  The genetic algorithm is another machine learning technique used in stylometry. This involves a method that starts out with a set of rules. An example rule might be, "If but appears more than 1.7 times in every thousand words, then the text is author X". The program is presented with text and uses the rules to determine authorship. The rules are tested against a set of known texts and each rule is given a fitness score. The 50 rules with the lowest scores are thrown out. The remaining 50 rules are given small changes and 50 new rules are introduced. This is repeated until the evolved rules correctly attribute the texts. One method for identifying style is called "rare pairs", and relies upon individual habits of collocation. The use of certain words may, for a particular author, idiosyncratically entail the use of other, predictable words. The diffusion of Internet has shifted the authorship attribution attention towards online texts (web pages, blogs, etc.) electronic messages (e-mails, tweets, posts, etc.), and other types of written information that are far shorter than an average book, much less formal and more diverse in terms of expressive elements such as colors, layout, fonts, graphics, emoticons, etc. Efforts to take into account such aspects at the level of both structure and syntax were reported in.  In addition, content-specific and idiosyncratic cues (e.g., topic models and grammar checking tools) were introduced to unveil deliberate stylistic choices.  Standard stylometric features have been employed to categorize the content of a chat over instant messaging,  or the behavior of the participants,  but attempts of identifying chat participants are still few and early. Furthermore, the similarity between spoken conversations and chat interactions has been neglected while being a key difference between chat data and any other type of written information. See also the academic journal Literary and Linguistic Computing (published by the University of Oxford) and the Language Resources and Evaluation journal. 