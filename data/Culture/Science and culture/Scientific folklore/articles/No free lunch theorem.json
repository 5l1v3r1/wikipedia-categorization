{"parse":{"title":"No free lunch theorem","pageid":1297317,"revid":854920592,"text":{"*":"<div class=\"mw-parser-output\"><div role=\"note\" class=\"hatnote navigation-not-searchable\">This article is about mathematical folklore. For treatment of the mathematics, see <a href=\"/wiki/No_free_lunch_in_search_and_optimization\" title=\"No free lunch in search and optimization\">No free lunch in search and optimization</a>.</div>\n<p>In <a href=\"/wiki/Mathematical_folklore\" title=\"Mathematical folklore\">mathematical folklore</a>, the <b>\"no free lunch\" (NFL) theorem</b> (sometimes pluralized) of <a href=\"/wiki/David_Wolpert\" title=\"David Wolpert\">David Wolpert</a> and <a href=\"/w/index.php?title=William_G._Macready&amp;action=edit&amp;redlink=1\" class=\"new\" title=\"William G. Macready (page does not exist)\">William Macready</a> appears in the 1997 \"No Free Lunch Theorems for Optimization\".<sup id=\"cite_ref-WM97_1-0\" class=\"reference\"><a href=\"#cite_note-WM97-1\">&#91;1&#93;</a></sup> Wolpert had previously derived no free lunch theorems for <a href=\"/wiki/Machine_learning\" title=\"Machine learning\">machine learning</a> (statistical inference).<sup id=\"cite_ref-Wolpert96_2-0\" class=\"reference\"><a href=\"#cite_note-Wolpert96-2\">&#91;2&#93;</a></sup>\n</p><p>In 2005, Wolpert and Macready themselves indicated that the first theorem in their paper \"state[s] that any two <a href=\"/wiki/Optimization_(mathematics)\" class=\"mw-redirect\" title=\"Optimization (mathematics)\">optimization</a> algorithms are equivalent when their performance is averaged across all possible problems\".<sup id=\"cite_ref-WM-coev_3-0\" class=\"reference\"><a href=\"#cite_note-WM-coev-3\">&#91;3&#93;</a></sup> The 1997 theorems of Wolpert and Macready are mathematically technical.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"This claim needs references to reliable sources. (April 2018)\">citation needed</span></a></i>&#93;</sup>\n</p><p>The folkloric<sup class=\"noprint Inline-Template\" style=\"margin-left:0.1em; white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Please_clarify\" title=\"Wikipedia:Please clarify\"><span title=\"article doesn&#39;t specifically say what the folkloric version is (May 2018)\">clarification needed</span></a></i>&#93;</sup> \"no free lunch\" (NFL) theorem is an easily stated and easily understood consequence of theorems Wolpert and Macready actually prove. It is weaker than the proven theorems, and thus does not encapsulate them. Various investigators have extended the work of Wolpert and Macready substantively. See <a href=\"/wiki/No_free_lunch_in_search_and_optimization\" title=\"No free lunch in search and optimization\">No free lunch in search and optimization</a> for treatment of the research area.\n</p><p>While some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research.<sup id=\"cite_ref-whitley_4-0\" class=\"reference\"><a href=\"#cite_note-whitley-4\">&#91;4&#93;</a></sup><sup id=\"cite_ref-carrier_5-0\" class=\"reference\"><a href=\"#cite_note-carrier-5\">&#91;5&#93;</a></sup>\n</p>\n<div id=\"toc\" class=\"toc\"><input type=\"checkbox\" role=\"button\" id=\"toctogglecheckbox\" class=\"toctogglecheckbox\" style=\"display:none\" /><div class=\"toctitle\" lang=\"en\" dir=\"ltr\"><h2>Contents</h2><span class=\"toctogglespan\"><label class=\"toctogglelabel\" for=\"toctogglecheckbox\"></label></span></div>\n<ul>\n<li class=\"toclevel-1 tocsection-1\"><a href=\"#Example\"><span class=\"tocnumber\">1</span> <span class=\"toctext\">Example</span></a></li>\n<li class=\"toclevel-1 tocsection-2\"><a href=\"#Original_NFL_theorems\"><span class=\"tocnumber\">2</span> <span class=\"toctext\">Original NFL theorems</span></a></li>\n<li class=\"toclevel-1 tocsection-3\"><a href=\"#Motivation\"><span class=\"tocnumber\">3</span> <span class=\"toctext\">Motivation</span></a></li>\n<li class=\"toclevel-1 tocsection-4\"><a href=\"#Implications_for_computing_and_for_the_scientific_method\"><span class=\"tocnumber\">4</span> <span class=\"toctext\">Implications for computing and for the scientific method</span></a></li>\n<li class=\"toclevel-1 tocsection-5\"><a href=\"#Notes\"><span class=\"tocnumber\">5</span> <span class=\"toctext\">Notes</span></a></li>\n<li class=\"toclevel-1 tocsection-6\"><a href=\"#External_links\"><span class=\"tocnumber\">6</span> <span class=\"toctext\">External links</span></a></li>\n</ul>\n</div>\n\n<h2><span class=\"mw-headline\" id=\"Example\">Example</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=No_free_lunch_theorem&amp;action=edit&amp;section=1\" title=\"Edit section: Example\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>To find the highest point on Earth, Alice uses a steepest-ascent local search which restarts when a local peak is fully climbed. Bob uses a steepest-descent local search which restarts when it bottoms out at a local trough. In our Earth, Alice will find the highest point much faster than Bob will. However, in some ensembles, each world such as ours can be paired 1-to-1 with a hypothetical \"Bobworld\" which is identical to ours, except that the elevation of peak of Mount Everest and of the lowest point in the Marianas Trench are swapped, as if a tall pole has been stuck in the Trench. In Bobworld, Bob's strategy of descending outperforms Alice's strategy of climbing; in fact, given further 1-to-1 pairing assumptions that are reasonable in a universe of bounded size, neither Alice's strategy nor Bob's strategy performs better on average, in the absence of some systematic natural bias towards worlds like Earth and away from worlds like Bobworld.<sup id=\"cite_ref-whitley_4-1\" class=\"reference\"><a href=\"#cite_note-whitley-4\">&#91;4&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Original_NFL_theorems\">Original NFL theorems</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=No_free_lunch_theorem&amp;action=edit&amp;section=2\" title=\"Edit section: Original NFL theorems\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>Wolpert and Macready give two NFL theorems that are closely related to the folkloric theorem. In their paper, they state:\n</p>\n<style data-mw-deduplicate=\"TemplateStyles:r856303468\">.mw-parser-output .templatequote{overflow:hidden;margin:1em 0;padding:0 40px}.mw-parser-output .templatequote .templatequotecite{line-height:1.5em;text-align:left;padding-left:1.6em;margin-top:0}</style><blockquote class=\"templatequote\"><p>We have dubbed the associated results NFL theorems because they demonstrate that if an algorithm performs well on a certain class of problems then it necessarily pays for that with degraded performance on the set of all remaining problems.<sup id=\"cite_ref-WM97_1-1\" class=\"reference\"><a href=\"#cite_note-WM97-1\">&#91;1&#93;</a></sup> \n</p></blockquote>\n<p>The first theorem first hypothesizes that <a href=\"/wiki/Objective_function\" class=\"mw-redirect\" title=\"Objective function\">objective functions</a> do not change while optimization is in progress, and then hypothesizes that objective functions may change.<sup id=\"cite_ref-WM97_1-2\" class=\"reference\"><a href=\"#cite_note-WM97-1\">&#91;1&#93;</a></sup>\n</p>\n<div style=\"padding-left: 3em; padding-right:0em; overflow: hidden;\"><i>Theorem 1</i>: For any algorithms <i>a</i><sub>1</sub> and <i>a</i><sub>2</sub>, at iteration step <i>m</i>\n<div style=\"padding-left: 3em; padding-right:0em; overflow: hidden;\"><span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle \\sum _{f}P(d_{m}^{y}|f,m,a_{1})=\\sum _{f}P(d_{m}^{y}|f,m,a_{2}),}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <munder>\n          <mo>&#x2211;<!-- \u2211 --></mo>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>f</mi>\n          </mrow>\n        </munder>\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <msubsup>\n          <mi>d</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>m</mi>\n          </mrow>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>y</mi>\n          </mrow>\n        </msubsup>\n        <mrow class=\"MJX-TeXAtom-ORD\">\n          <mo stretchy=\"false\">|</mo>\n        </mrow>\n        <mi>f</mi>\n        <mo>,</mo>\n        <mi>m</mi>\n        <mo>,</mo>\n        <msub>\n          <mi>a</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mn>1</mn>\n          </mrow>\n        </msub>\n        <mo stretchy=\"false\">)</mo>\n        <mo>=</mo>\n        <munder>\n          <mo>&#x2211;<!-- \u2211 --></mo>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>f</mi>\n          </mrow>\n        </munder>\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <msubsup>\n          <mi>d</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>m</mi>\n          </mrow>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>y</mi>\n          </mrow>\n        </msubsup>\n        <mrow class=\"MJX-TeXAtom-ORD\">\n          <mo stretchy=\"false\">|</mo>\n        </mrow>\n        <mi>f</mi>\n        <mo>,</mo>\n        <mi>m</mi>\n        <mo>,</mo>\n        <msub>\n          <mi>a</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mn>2</mn>\n          </mrow>\n        </msub>\n        <mo stretchy=\"false\">)</mo>\n        <mo>,</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle \\sum _{f}P(d_{m}^{y}|f,m,a_{1})=\\sum _{f}P(d_{m}^{y}|f,m,a_{2}),}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4d8189d428653c79c9dc36de662934fca74dcf99\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -3.338ex; width:40.742ex; height:5.843ex;\" alt=\"\\sum _{f}P(d_{m}^{y}|f,m,a_{1})=\\sum _{f}P(d_{m}^{y}|f,m,a_{2}),\"/></span></div></div>\n<p>where <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle d_{m}^{y}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <msubsup>\n          <mi>d</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>m</mi>\n          </mrow>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>y</mi>\n          </mrow>\n        </msubsup>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle d_{m}^{y}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/3a7801071798b4be686a4e144a2cdd1e893097b5\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.671ex; width:2.884ex; height:2.843ex;\" alt=\"d_{m}^{y}\"/></span> denotes the ordered set of size <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle m}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>m</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle m}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0a07d98bb302f3856cbabc47b2b9016692e3f7bc\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:2.04ex; height:1.676ex;\" alt=\"m\"/></span> of the cost values <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle y}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>y</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle y}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/b8a6208ec717213d4317e666f1ae872e00620a0d\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.671ex; width:1.155ex; height:2.009ex;\" alt=\"y\"/></span> associated to input values <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle x\\in X}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>x</mi>\n        <mo>&#x2208;<!-- \u2208 --></mo>\n        <mi>X</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle x\\in X}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/3e580967f68f36743e894aa7944f032dda6ea01d\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:6.15ex; height:2.176ex;\" alt=\"x\\in X\"/></span>, <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle f:X\\rightarrow Y}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>f</mi>\n        <mo>:</mo>\n        <mi>X</mi>\n        <mo stretchy=\"false\">&#x2192;<!-- \u2192 --></mo>\n        <mi>Y</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle f:X\\rightarrow Y}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/b215af1e965d0595a97ad2b21f7d0cbcf6281303\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.671ex; width:10.583ex; height:2.509ex;\" alt=\"f:X\\rightarrow Y\"/></span> is the function being optimized and <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle P(d_{m}^{y}|f,m,a)}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>P</mi>\n        <mo stretchy=\"false\">(</mo>\n        <msubsup>\n          <mi>d</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>m</mi>\n          </mrow>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>y</mi>\n          </mrow>\n        </msubsup>\n        <mrow class=\"MJX-TeXAtom-ORD\">\n          <mo stretchy=\"false\">|</mo>\n        </mrow>\n        <mi>f</mi>\n        <mo>,</mo>\n        <mi>m</mi>\n        <mo>,</mo>\n        <mi>a</mi>\n        <mo stretchy=\"false\">)</mo>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle P(d_{m}^{y}|f,m,a)}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/554388a3dcb8d2e77f07caf32ec0fe26931fe66b\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.838ex; width:13.702ex; height:3.009ex;\" alt=\"P(d_{m}^{y}|f,m,a)\"/></span> is the conditional probability of obtaining a given sequence of cost values from algorithm <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle a}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>a</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle a}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/ffd2487510aa438433a2579450ab2b3d557e5edc\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.23ex; height:1.676ex;\" alt=\"a\"/></span> run <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle m}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>m</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle m}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0a07d98bb302f3856cbabc47b2b9016692e3f7bc\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:2.04ex; height:1.676ex;\" alt=\"m\"/></span> times on function <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle f}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>f</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle f}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.671ex; width:1.279ex; height:2.509ex;\" alt=\"f\"/></span>.\n</p><p>The theorem can be equivalently formulated as follows:\n</p>\n<div style=\"padding-left: 3em; padding-right:0em; overflow: hidden;\"><i>Theorem 1</i>: Given a finite set <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle V}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>V</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle V}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.787ex; height:2.176ex;\" alt=\"V\"/></span> and a finite set <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle S}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>S</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle S}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.499ex; height:2.176ex;\" alt=\"S\"/></span> of real numbers, assume that <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle f:V\\to S}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>f</mi>\n        <mo>:</mo>\n        <mi>V</mi>\n        <mo stretchy=\"false\">&#x2192;<!-- \u2192 --></mo>\n        <mi>S</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle f:V\\to S}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/0f670393a38729a5fbc9b55833858d089224306a\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.671ex; width:10.116ex; height:2.509ex;\" alt=\"f : V \\to S\"/></span> is chosen at random according to uniform distribution on the set <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle S^{V}}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <msup>\n          <mi>S</mi>\n          <mrow class=\"MJX-TeXAtom-ORD\">\n            <mi>V</mi>\n          </mrow>\n        </msup>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle S^{V}}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/e1fd3c409381ace8d6a06877f41e86f462c1f5f4\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:3.018ex; height:2.676ex;\" alt=\"{\\displaystyle S^{V}}\"/></span> of all possible functions from <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle V}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>V</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle V}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.787ex; height:2.176ex;\" alt=\"V\"/></span> to <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle S}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>S</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle S}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/4611d85173cd3b508e67077d4a1252c9c05abca2\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.499ex; height:2.176ex;\" alt=\"S\"/></span>. For the problem of optimizing <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle f}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>f</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle f}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/132e57acb643253e7810ee9702d9581f159a1c61\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.671ex; width:1.279ex; height:2.509ex;\" alt=\"f\"/></span> over the set <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle V}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>V</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle V}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.787ex; height:2.176ex;\" alt=\"V\"/></span>, then no algorithm performs better than blind search.</div>\n<p>Here, <i>blind search</i> means that at each step of the algorithm, the element <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle v\\in V}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>v</mi>\n        <mo>&#x2208;<!-- \u2208 --></mo>\n        <mi>V</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle v\\in V}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/99886ebbde63daa0224fb9bf56fa11b3c8a6f4fb\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:5.756ex; height:2.176ex;\" alt=\"v\\in V\"/></span> is chosen at random with uniform probability distribution from the elements of <span class=\"mwe-math-element\"><span class=\"mwe-math-mathml-inline mwe-math-mathml-a11y\" style=\"display: none;\"><math xmlns=\"http://www.w3.org/1998/Math/MathML\"  alttext=\"{\\displaystyle V}\">\n  <semantics>\n    <mrow class=\"MJX-TeXAtom-ORD\">\n      <mstyle displaystyle=\"true\" scriptlevel=\"0\">\n        <mi>V</mi>\n      </mstyle>\n    </mrow>\n    <annotation encoding=\"application/x-tex\">{\\displaystyle V}</annotation>\n  </semantics>\n</math></span><img src=\"https://wikimedia.org/api/rest_v1/media/math/render/svg/af0f6064540e84211d0ffe4dac72098adfa52845\" class=\"mwe-math-fallback-image-inline\" aria-hidden=\"true\" style=\"vertical-align: -0.338ex; width:1.787ex; height:2.176ex;\" alt=\"V\"/></span> that have not been chosen previously.\n</p><p>In essence, this says that when all functions <i>f</i> are equally likely, the probability of observing an arbitrary sequence of <i>m</i> values in the course of optimization does not depend upon the algorithm. In the analytic framework of Wolpert and Macready, performance is a function of the sequence of observed values (and not e.g. of wall-clock time), so it follows easily that all algorithms have identically distributed performance when objective functions are drawn uniformly at random, and also that all algorithms have identical mean performance. But identical mean performance of all algorithms does not imply Theorem 1, and thus the folkloric theorem is not equivalent to the original theorem.\n</p><p>Theorem 2 establishes a similar, but \"more subtle\", NFL result for time-varying objective functions.<sup id=\"cite_ref-WM97_1-3\" class=\"reference\"><a href=\"#cite_note-WM97-1\">&#91;1&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Motivation\">Motivation</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=No_free_lunch_theorem&amp;action=edit&amp;section=3\" title=\"Edit section: Motivation\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<p>The NFL theorems were explicitly <i>not</i> motivated by the question of what can be inferred (in the case of NFL for machine learning) or found (in the case of NFL for search) when the \"environment is uniform random\". Rather uniform randomness was used as a tool, to compare the number of environments for which algorithm A outperforms algorithm B to the number of environments for which B outperforms A. NFL tells us that (appropriately weighted)<sup class=\"noprint Inline-Template\" style=\"margin-left:0.1em; white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Please_clarify\" title=\"Wikipedia:Please clarify\"><span title=\"The text near this tag may need clarification or removal of jargon. (May 2018)\">clarification needed</span></a></i>&#93;</sup> there are just as many environments in both of those sets.\n</p><p>This is true for many definitions of what precisely an \"environment\" is. In particular, there are just as many prior distributions (appropriately weighted) in which learning algorithm A beats B (on average) as vice versa.<sup class=\"noprint Inline-Template Template-Fact\" style=\"white-space:nowrap;\">&#91;<i><a href=\"/wiki/Wikipedia:Citation_needed\" title=\"Wikipedia:Citation needed\"><span title=\"dubious; the set of possible prior distributions is infinite (May 2018)\">citation needed</span></a></i>&#93;</sup> This statement about <i>sets of priors</i> is what is most important about NFL, not the fact that any two algorithms perform equally for the single, specific prior distribution that assigns equal probability to all environments.\n</p><p>While the NFL is important to understand the fundamental limitation for a set of problems, it does not state anything about each particular instance of a problem that can arise in practice. That is, the NFL states what the NFL states in the mathematical statements and it is nothing more than that. For example, it applies to the situations where the algorithm is fixed first and a nature can choose a worst problem instance to each fixed algorithm. Therefore, if we have a \"good\" problem in practice or if we can choose a \"good\" learning algorithm for a given particular problem instance, then the NFL does not mention any limitation about this particular problem instance. See for example.<sup id=\"cite_ref-KA-deep_6-0\" class=\"reference\"><a href=\"#cite_note-KA-deep-6\">&#91;6&#93;</a></sup> To understand the results of the NFL along with \"seemingly\" contradicting results from other papers, it is important to actually understand the mathematical logic of the NFL instead of intuitive notation of the NFL. All results including the NFL and<sup id=\"cite_ref-KA-deep_6-1\" class=\"reference\"><a href=\"#cite_note-KA-deep-6\">&#91;6&#93;</a></sup> are indeed consistent.\n</p>\n<h2><span class=\"mw-headline\" id=\"Implications_for_computing_and_for_the_scientific_method\">Implications for computing and for the scientific method</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=No_free_lunch_theorem&amp;action=edit&amp;section=4\" title=\"Edit section: Implications for computing and for the scientific method\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div role=\"note\" class=\"hatnote navigation-not-searchable\">Further information: <a href=\"/wiki/Problem_of_induction\" title=\"Problem of induction\">Problem of induction</a></div>\n<p>To illustrate one of the counter-intuitive implications of NFL, suppose we fix two supervised learning algorithms, C and D. We then sample a target function f to produce a set of input-output pairs, <i>d</i>. How should we choose whether to train C or D on <i>d</i>, in order to make predictions for what output would be associated with a point lying outside of <i>d?</i>\n</p><p>It is common in almost of all science and statistics to answer this question  - to choose between C and D -  by running cross-validation on <i>d</i> with those two algorithms. In other words, to decide whether to generalize from <i>d</i> with either C or D<i>,</i> we see which of them has better out-of-sample performance when tested within <i>d</i>.\n</p><p>Note that since C and D are fixed, this use of cross-validation to choose between them is itself an algorithm, i.e., a way of generalizing from an arbitrary dataset. Call this algorithm A. (Arguably, A is a simplified model of the scientific method itself.)\n</p><p>Note as well though that we could also use <i>anti</i>-cross-validation to make our choice. In other words, we could choose between C and D based on which has <i>worse</i> out-of-sample performance within <i>d</i>. Again, since C and D are fixed, this use of anti-cross-validation is itself an algorithm. Call that algorithm B.\n</p><p>NFL tells us (loosely speaking) that B must beat A on just as many target functions (and associated datasets <i>d</i>) as A beats B. In this very specific sense, the scientific method will lose to the \"anti\" scientific method just as readily as it wins.<sup id=\"cite_ref-7\" class=\"reference\"><a href=\"#cite_note-7\">&#91;7&#93;</a></sup>\n</p><p>However, note that NFL only applies if the target function is chosen from a uniform distribution of all possible functions. If this is not the case, and certain target functions are more likely to be chosen than others, then A may perform better than B overall. The contribution of NFL is that it tells us choosing an appropriate algorithm requires making assumptions about the kinds of target functions the algorithm is being used for. With no assumptions, no \"meta-algorithm\", such as the scientific method, performs better than random choice.\n</p><p>While some scholars argue that NFL conveys important insight, others argue that NFL is of little relevance to machine learning research.<sup id=\"cite_ref-whitley_4-2\" class=\"reference\"><a href=\"#cite_note-whitley-4\">&#91;4&#93;</a></sup><sup id=\"cite_ref-carrier_5-1\" class=\"reference\"><a href=\"#cite_note-carrier-5\">&#91;5&#93;</a></sup> If <a href=\"/wiki/Occam%27s_razor\" title=\"Occam&#39;s razor\">Occam's razor</a> is correct, for example if sequences of lower <a href=\"/wiki/Kolmogorov_complexity\" title=\"Kolmogorov complexity\">Kolmogorov complexity</a> are more probable than sequences of higher complexity, then (as is observed in real life) some algorithms, such as cross-validation, perform better on average on practical problems (when compared with random choice or with anti-cross-validation).<sup id=\"cite_ref-8\" class=\"reference\"><a href=\"#cite_note-8\">&#91;8&#93;</a></sup>\n</p>\n<h2><span class=\"mw-headline\" id=\"Notes\">Notes</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=No_free_lunch_theorem&amp;action=edit&amp;section=5\" title=\"Edit section: Notes\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<div class=\"reflist\" style=\"list-style-type: decimal;\">\n<div class=\"mw-references-wrap\"><ol class=\"references\">\n<li id=\"cite_note-WM97-1\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-WM97_1-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-WM97_1-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-WM97_1-2\"><sup><i><b>c</b></i></sup></a> <a href=\"#cite_ref-WM97_1-3\"><sup><i><b>d</b></i></sup></a></span> <span class=\"reference-text\">Wolpert, D.H., Macready, W.G. (1997), \"<a rel=\"nofollow\" class=\"external text\" href=\"http://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf\">No Free Lunch Theorems for Optimization</a>\", <i>IEEE Transactions on Evolutionary Computation</i> <b>1</b>, 67.</span>\n</li>\n<li id=\"cite_note-Wolpert96-2\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-Wolpert96_2-0\">^</a></b></span> <span class=\"reference-text\">Wolpert, David (1996), \"<a rel=\"nofollow\" class=\"external text\" href=\"http://www.zabaras.com/Courses/BayesianComputing/Papers/lack_of_a_priori_distinctions_wolpert.pdf\">The Lack of <i>A Priori</i> Distinctions between Learning Algorithms</a>\", <i>Neural Computation</i>, pp. 1341-1390.  <a rel=\"nofollow\" class=\"external text\" href=\"https://web.archive.org/web/20161220125415/http://www.zabaras.com/Courses/BayesianComputing/Papers/lack_of_a_priori_distinctions_wolpert.pdf\">Archived</a> 2016-12-20 at the <a href=\"/wiki/Wayback_Machine\" title=\"Wayback Machine\">Wayback Machine</a>.</span>\n</li>\n<li id=\"cite_note-WM-coev-3\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-WM-coev_3-0\">^</a></b></span> <span class=\"reference-text\">Wolpert, D.H., and Macready, W.G. (2005) \"Coevolutionary free lunches\", <i>IEEE Transactions on Evolutionary Computation</i>, 9(6): 721-735</span>\n</li>\n<li id=\"cite_note-whitley-4\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-whitley_4-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-whitley_4-1\"><sup><i><b>b</b></i></sup></a> <a href=\"#cite_ref-whitley_4-2\"><sup><i><b>c</b></i></sup></a></span> <span class=\"reference-text\">Whitley, Darrell, and Jean Paul Watson. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/profile/Darrell_Whitley2/publication/226085645_Complexity_Theory_and_the_No_Free_Lunch_Theorem/links/5632148608ae0530378e94b9.pdf\">Complexity theory and the no free lunch theorem</a>.\" In Search Methodologies, pp. 317-339. Springer, Boston, MA, 2005.</span>\n</li>\n<li id=\"cite_note-carrier-5\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-carrier_5-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-carrier_5-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">Giraud-Carrier, Christophe, and Foster Provost. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://www.researchgate.net/profile/Christophe_Giraud-Carrier/publication/228671734_Toward_a_justification_of_meta-learning_Is_the_no_free_lunch_theorem_a_show-stopper/links/0fcfd510c5d5b83ec8000000/Toward-a-justification-of-meta-learning-Is-the-no-free-lunch-theorem-a-show-stopper.pdf\">Toward a justification of meta-learning: Is the no free lunch theorem a show-stopper</a>.\" In Proceedings of the ICML-2005 Workshop on Meta-learning, pp. 12-19. 2005.</span>\n</li>\n<li id=\"cite_note-KA-deep-6\"><span class=\"mw-cite-backlink\">^ <a href=\"#cite_ref-KA-deep_6-0\"><sup><i><b>a</b></i></sup></a> <a href=\"#cite_ref-KA-deep_6-1\"><sup><i><b>b</b></i></sup></a></span> <span class=\"reference-text\">Kawaguchi, K., Kaelbling, L.P, and Bengio, Y.(2017) \"Generalization in deep learning\", <a rel=\"nofollow\" class=\"external free\" href=\"https://arxiv.org/abs/1710.05468\">https://arxiv.org/abs/1710.05468</a></span>\n</li>\n<li id=\"cite_note-7\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-7\">^</a></b></span> <span class=\"reference-text\">Wolpert, D.H. (2013) \"What the no free lunch theorems really mean\", Ubiquity, Volume 2013, December 2013, DOI: 10.1145/2555235.2555237</span>\n</li>\n<li id=\"cite_note-8\"><span class=\"mw-cite-backlink\"><b><a href=\"#cite_ref-8\">^</a></b></span> <span class=\"reference-text\">Lattimore, Tor, and Marcus Hutter. \"<a rel=\"nofollow\" class=\"external text\" href=\"https://arxiv.org/pdf/1111.3846\">No free lunch versus Occam\u2019s razor in supervised learning</a>.\" In Algorithmic Probability and Friends. Bayesian Prediction and Artificial Intelligence, pp. 223-235. Springer, Berlin, Heidelberg, 2013.</span>\n</li>\n</ol></div></div>\n<h2><span class=\"mw-headline\" id=\"External_links\">External links</span><span class=\"mw-editsection\"><span class=\"mw-editsection-bracket\">[</span><a href=\"/w/index.php?title=No_free_lunch_theorem&amp;action=edit&amp;section=6\" title=\"Edit section: External links\">edit</a><span class=\"mw-editsection-bracket\">]</span></span></h2>\n<ul><li><a rel=\"nofollow\" class=\"external text\" href=\"http://www.no-free-lunch.org/\">No Free Lunch Theorems</a></li>\n<li><a class=\"external autonumber\" href=\"https://commons.wikimedia.org/wiki/File:No_free_lunch_theorem.svg\">[1]</a> - graphics illustrating the theorem</li></ul>\n\n<!-- \nNewPP limit report\nParsed by mw1253\nCached time: 20180909112252\nCache expiry: 1900800\nDynamic content: false\nCPU time usage: 0.176 seconds\nReal time usage: 0.300 seconds\nPreprocessor visited node count: 946/1000000\nPreprocessor generated node count: 0/1500000\nPost\u2010expand include size: 10412/2097152 bytes\nTemplate argument size: 3074/2097152 bytes\nHighest expansion depth: 11/40\nExpensive parser function count: 3/500\nUnstrip recursion depth: 0/20\nUnstrip post\u2010expand size: 6743/5000000 bytes\nNumber of Wikibase entities loaded: 0/400\nLua time usage: 0.060/10.000 seconds\nLua memory usage: 2.09 MB/50 MB\n-->\n<!--\nTransclusion expansion time report (%,ms,calls,template)\n100.00%  181.543      1 -total\n 27.86%   50.582      2 Template:Citation_needed\n 24.54%   44.553      2 Template:Fix\n 22.90%   41.580      1 Template:About\n 17.09%   31.019      2 Template:Clarify\n 16.11%   29.243      8 Template:Category_handler\n 15.57%   28.266      1 Template:Reflist\n 15.12%   27.441      4 Template:Delink\n 14.79%   26.847      2 Template:Fix-span\n  9.18%   16.665      2 Template:Replace\n-->\n\n<!-- Saved in parser cache with key enwiki:pcache:idhash:1297317-0!canonical!math=5 and timestamp 20180909112251 and revision id 854920592\n -->\n</div>"},"langlinks":[],"categories":[{"sortkey":"","hidden":"","*":"Webarchive_template_wayback_links"},{"sortkey":"","hidden":"","*":"All_articles_with_unsourced_statements"},{"sortkey":"","hidden":"","*":"Articles_with_unsourced_statements_from_April_2018"},{"sortkey":"","hidden":"","*":"Wikipedia_articles_needing_clarification_from_May_2018"},{"sortkey":"","hidden":"","*":"Articles_with_unsourced_statements_from_May_2018"},{"sortkey":"","*":"Scientific_folklore"},{"sortkey":"","*":"Philosophy_of_mathematics"},{"sortkey":"","*":"Mathematical_theorems"}],"links":[{"ns":14,"exists":"","*":"Category:Articles with unsourced statements from April 2018"},{"ns":14,"exists":"","*":"Category:Wikipedia articles needing clarification from May 2018"},{"ns":14,"exists":"","*":"Category:Articles with unsourced statements from May 2018"},{"ns":0,"exists":"","*":"David Wolpert"},{"ns":0,"exists":"","*":"Kolmogorov complexity"},{"ns":0,"exists":"","*":"Machine learning"},{"ns":0,"exists":"","*":"Mathematical folklore"},{"ns":0,"exists":"","*":"No free lunch in search and optimization"},{"ns":0,"exists":"","*":"Objective function"},{"ns":0,"exists":"","*":"Occam's razor"},{"ns":0,"exists":"","*":"Optimization (mathematics)"},{"ns":0,"exists":"","*":"Problem of induction"},{"ns":0,"exists":"","*":"Wayback Machine"},{"ns":0,"*":"William G. Macready"},{"ns":4,"exists":"","*":"Wikipedia:Citation needed"},{"ns":4,"exists":"","*":"Wikipedia:Please clarify"}],"templates":[{"ns":10,"exists":"","*":"Template:About"},{"ns":10,"exists":"","*":"Template:Citation needed"},{"ns":10,"exists":"","*":"Template:Fix"},{"ns":10,"exists":"","*":"Template:Category handler"},{"ns":10,"exists":"","*":"Template:Fix/category"},{"ns":10,"exists":"","*":"Template:Delink"},{"ns":10,"exists":"","*":"Template:Clarify"},{"ns":10,"exists":"","*":"Template:Fix-span"},{"ns":10,"exists":"","*":"Template:Replace"},{"ns":10,"exists":"","*":"Template:Quote"},{"ns":10,"exists":"","*":"Template:Quote/styles.css"},{"ns":10,"exists":"","*":"Template:Block indent"},{"ns":10,"exists":"","*":"Template:Further"},{"ns":10,"exists":"","*":"Template:Reflist"},{"ns":10,"exists":"","*":"Template:Webarchive"},{"ns":10,"exists":"","*":"Template:Main other"},{"ns":828,"exists":"","*":"Module:About"},{"ns":828,"exists":"","*":"Module:Hatnote"},{"ns":828,"exists":"","*":"Module:Hatnote list"},{"ns":828,"exists":"","*":"Module:Arguments"},{"ns":828,"exists":"","*":"Module:Pagetype"},{"ns":828,"exists":"","*":"Module:Pagetype/config"},{"ns":828,"exists":"","*":"Module:Yesno"},{"ns":828,"exists":"","*":"Module:Namespace detect"},{"ns":828,"exists":"","*":"Module:Namespace detect/data"},{"ns":828,"exists":"","*":"Module:Namespace detect/config"},{"ns":828,"exists":"","*":"Module:Unsubst"},{"ns":828,"exists":"","*":"Module:Category handler"},{"ns":828,"exists":"","*":"Module:Category handler/data"},{"ns":828,"exists":"","*":"Module:Category handler/config"},{"ns":828,"exists":"","*":"Module:Category handler/shared"},{"ns":828,"exists":"","*":"Module:Category handler/blacklist"},{"ns":828,"exists":"","*":"Module:Delink"},{"ns":828,"exists":"","*":"Module:No globals"},{"ns":828,"exists":"","*":"Module:String"},{"ns":828,"exists":"","*":"Module:Labelled list hatnote"},{"ns":828,"exists":"","*":"Module:Webarchive"},{"ns":828,"exists":"","*":"Module:Webarchive/data"},{"ns":828,"exists":"","*":"Module:Check for unknown parameters"}],"images":[],"externallinks":["http://ti.arc.nasa.gov/m/profile/dhw/papers/78.pdf","http://www.zabaras.com/Courses/BayesianComputing/Papers/lack_of_a_priori_distinctions_wolpert.pdf","https://web.archive.org/web/20161220125415/http://www.zabaras.com/Courses/BayesianComputing/Papers/lack_of_a_priori_distinctions_wolpert.pdf","https://www.researchgate.net/profile/Darrell_Whitley2/publication/226085645_Complexity_Theory_and_the_No_Free_Lunch_Theorem/links/5632148608ae0530378e94b9.pdf","https://www.researchgate.net/profile/Christophe_Giraud-Carrier/publication/228671734_Toward_a_justification_of_meta-learning_Is_the_no_free_lunch_theorem_a_show-stopper/links/0fcfd510c5d5b83ec8000000/Toward-a-justification-of-meta-learning-Is-the-no-free-lunch-theorem-a-show-stopper.pdf","https://arxiv.org/pdf/1111.3846","https://arxiv.org/abs/1710.05468","http://www.no-free-lunch.org/","https://commons.wikimedia.org/wiki/File:No_free_lunch_theorem.svg"],"sections":[{"toclevel":1,"level":"2","line":"Example","number":"1","index":"1","fromtitle":"No_free_lunch_theorem","byteoffset":3089,"anchor":"Example"},{"toclevel":1,"level":"2","line":"Original NFL theorems","number":"2","index":"2","fromtitle":"No_free_lunch_theorem","byteoffset":4077,"anchor":"Original_NFL_theorems"},{"toclevel":1,"level":"2","line":"Motivation","number":"3","index":"3","fromtitle":"No_free_lunch_theorem","byteoffset":6846,"anchor":"Motivation"},{"toclevel":1,"level":"2","line":"Implications for computing and for the scientific method","number":"4","index":"4","fromtitle":"No_free_lunch_theorem","byteoffset":9072,"anchor":"Implications_for_computing_and_for_the_scientific_method"},{"toclevel":1,"level":"2","line":"Notes","number":"5","index":"5","fromtitle":"No_free_lunch_theorem","byteoffset":12188,"anchor":"Notes"},{"toclevel":1,"level":"2","line":"External links","number":"6","index":"6","fromtitle":"No_free_lunch_theorem","byteoffset":12211,"anchor":"External_links"}],"parsewarnings":[],"displaytitle":"No free lunch theorem","iwlinks":[],"properties":[{"name":"wikibase_item","*":"Q7045226"}]}}